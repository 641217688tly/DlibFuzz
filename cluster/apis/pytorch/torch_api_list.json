{
  "1": {
    "name": "align_tensors",
    "module": "torch",
    "fullName": "torch.align_tensors",
    "signature": "(*tensors)",
    "description": "No description available."
  },
  "2": {
    "name": "are_deterministic_algorithms_enabled",
    "module": "torch",
    "fullName": "torch.are_deterministic_algorithms_enabled",
    "signature": "()",
    "description": "Returns True if the global deterministic flag is turned on. Refer to"
  },
  "3": {
    "name": "atleast_1d",
    "module": "torch",
    "fullName": "torch.atleast_1d",
    "signature": "(*tensors)",
    "description": "Returns a 1-dimensional view of each input tensor with zero dimensions."
  },
  "4": {
    "name": "atleast_2d",
    "module": "torch",
    "fullName": "torch.atleast_2d",
    "signature": "(*tensors)",
    "description": "Returns a 2-dimensional view of each input tensor with zero dimensions."
  },
  "5": {
    "name": "atleast_3d",
    "module": "torch",
    "fullName": "torch.atleast_3d",
    "signature": "(*tensors)",
    "description": "Returns a 3-dimensional view of each input tensor with zero dimensions."
  },
  "6": {
    "name": "block_diag",
    "module": "torch",
    "fullName": "torch.block_diag",
    "signature": "(*tensors)",
    "description": "Create a block diagonal matrix from provided tensors."
  },
  "7": {
    "name": "broadcast_shapes",
    "module": "torch",
    "fullName": "torch.broadcast_shapes",
    "signature": "(*shapes)",
    "description": "broadcast_shapes(*shapes) -> Size"
  },
  "8": {
    "name": "broadcast_tensors",
    "module": "torch",
    "fullName": "torch.broadcast_tensors",
    "signature": "(*tensors)",
    "description": "broadcast_tensors(*tensors) -> List of Tensors"
  },
  "9": {
    "name": "cartesian_prod",
    "module": "torch",
    "fullName": "torch.cartesian_prod",
    "signature": "(*tensors)",
    "description": "Do cartesian product of the given sequence of tensors. The behavior is similar to"
  },
  "10": {
    "name": "cdist",
    "module": "torch",
    "fullName": "torch.cdist",
    "signature": "(x1, x2, p=2.0, compute_mode='use_mm_for_euclid_dist_if_necessary')",
    "description": "Computes batched the p-norm distance between each pair of the two collections of row vectors."
  },
  "11": {
    "name": "classproperty",
    "module": "torch",
    "fullName": "torch.classproperty",
    "signature": "(func)",
    "description": "No description available."
  },
  "12": {
    "name": "compiled_with_cxx11_abi",
    "module": "torch",
    "fullName": "torch.compiled_with_cxx11_abi",
    "signature": "()",
    "description": "Returns whether PyTorch was built with _GLIBCXX_USE_CXX11_ABI=1"
  },
  "13": {
    "name": "einsum",
    "module": "torch",
    "fullName": "torch.einsum",
    "signature": "(*args: Any) -> torch.Tensor",
    "description": "einsum(equation, *operands) -> Tensor"
  },
  "14": {
    "name": "from_dlpack",
    "module": "torch",
    "fullName": "torch.from_dlpack",
    "signature": "(ext_tensor: Any) -> torch.Tensor",
    "description": "from_dlpack(ext_tensor) -> Tensor"
  },
  "15": {
    "name": "get_deterministic_debug_mode",
    "module": "torch",
    "fullName": "torch.get_deterministic_debug_mode",
    "signature": "() -> int",
    "description": "Returns the current value of the debug mode for deterministic"
  },
  "16": {
    "name": "get_file_path",
    "module": "torch",
    "fullName": "torch.get_file_path",
    "signature": "(*path_components: str) -> str",
    "description": "No description available."
  },
  "17": {
    "name": "get_float32_matmul_precision",
    "module": "torch",
    "fullName": "torch.get_float32_matmul_precision",
    "signature": "() -> str",
    "description": "Returns the current value of float32 matrix multiplication precision. Refer to"
  },
  "18": {
    "name": "get_rng_state",
    "module": "torch",
    "fullName": "torch.get_rng_state",
    "signature": "() -> torch.Tensor",
    "description": "Returns the random number generator state as a `torch.ByteTensor`."
  },
  "19": {
    "name": "initial_seed",
    "module": "torch",
    "fullName": "torch.initial_seed",
    "signature": "() -> int",
    "description": "Returns the initial seed for generating random numbers as a"
  },
  "20": {
    "name": "is_deterministic_algorithms_warn_only_enabled",
    "module": "torch",
    "fullName": "torch.is_deterministic_algorithms_warn_only_enabled",
    "signature": "()",
    "description": "Returns True if the global deterministic flag is set to warn only."
  },
  "21": {
    "name": "is_storage",
    "module": "torch",
    "fullName": "torch.is_storage",
    "signature": "(obj)",
    "description": "Returns True if `obj` is a PyTorch storage object."
  },
  "22": {
    "name": "is_tensor",
    "module": "torch",
    "fullName": "torch.is_tensor",
    "signature": "(obj)",
    "description": "Returns True if `obj` is a PyTorch tensor."
  },
  "23": {
    "name": "is_warn_always_enabled",
    "module": "torch",
    "fullName": "torch.is_warn_always_enabled",
    "signature": "()",
    "description": "Returns True if the global warn_always flag is turned on. Refer to"
  },
  "24": {
    "name": "load",
    "module": "torch",
    "fullName": "torch.load",
    "signature": "(f, map_location=None, pickle_module=<module 'pickle' from '/root/miniconda3/envs/DlibFuzz/lib/python3.9/pickle.py'>, **pickle_load_args)",
    "description": "load(f, map_location=None, pickle_module=pickle, **pickle_load_args)"
  },
  "25": {
    "name": "lobpcg",
    "module": "torch",
    "fullName": "torch.lobpcg",
    "signature": "(A: torch.Tensor, k: Optional[int] = None, B: Optional[torch.Tensor] = None, X: Optional[torch.Tensor] = None, n: Optional[int] = None, iK: Optional[torch.Tensor] = None, niter: Optional[int] = None, tol: Optional[float] = None, largest: Optional[bool] = None, method: Optional[str] = None, tracker: None = None, ortho_iparams: Optional[Dict[str, int]] = None, ortho_fparams: Optional[Dict[str, float]] = None, ortho_bparams: Optional[Dict[str, bool]] = None) -> Tuple[torch.Tensor, torch.Tensor]",
    "description": "Find the k largest (or smallest) eigenvalues and the corresponding"
  },
  "26": {
    "name": "lu",
    "module": "torch",
    "fullName": "torch.lu",
    "signature": "(*args, **kwargs)",
    "description": "Computes the LU factorization of a matrix or batches of matrices"
  },
  "27": {
    "name": "manual_seed",
    "module": "torch",
    "fullName": "torch.manual_seed",
    "signature": "(seed) -> torch._C.Generator",
    "description": "Sets the seed for generating random numbers. Returns a"
  },
  "28": {
    "name": "meshgrid",
    "module": "torch",
    "fullName": "torch.meshgrid",
    "signature": "(*tensors, indexing: Optional[str] = None) -> Tuple[torch.Tensor, ...]",
    "description": "Creates grids of coordinates specified by the 1D inputs in `attr`:tensors."
  },
  "29": {
    "name": "pca_lowrank",
    "module": "torch",
    "fullName": "torch.pca_lowrank",
    "signature": "(A: torch.Tensor, q: Optional[int] = None, center: bool = True, niter: int = 2) -> Tuple[torch.Tensor, torch.Tensor, torch.Tensor]",
    "description": "Performs linear Principal Component Analysis (PCA) on a low-rank"
  },
  "30": {
    "name": "prepare_multiprocessing_environment",
    "module": "torch",
    "fullName": "torch.prepare_multiprocessing_environment",
    "signature": "(path: str) -> None",
    "description": "No description available."
  },
  "31": {
    "name": "save",
    "module": "torch",
    "fullName": "torch.save",
    "signature": "(obj, f: Union[str, os.PathLike, BinaryIO, IO[bytes]], pickle_module=<module 'pickle' from '/root/miniconda3/envs/DlibFuzz/lib/python3.9/pickle.py'>, pickle_protocol=2, _use_new_zipfile_serialization=True) -> None",
    "description": "save(obj, f, pickle_module=pickle, pickle_protocol=DEFAULT_PROTOCOL, _use_new_zipfile_serialization=True)"
  },
  "32": {
    "name": "seed",
    "module": "torch",
    "fullName": "torch.seed",
    "signature": "() -> int",
    "description": "Sets the seed for generating random numbers to a non-deterministic"
  },
  "33": {
    "name": "set_default_dtype",
    "module": "torch",
    "fullName": "torch.set_default_dtype",
    "signature": "(d)",
    "description": "Sets the default floating point dtype to :attr:`d`. Supports torch.float32"
  },
  "34": {
    "name": "set_default_tensor_type",
    "module": "torch",
    "fullName": "torch.set_default_tensor_type",
    "signature": "(t)",
    "description": "Sets the default ``torch.Tensor`` type to floating point tensor type"
  },
  "35": {
    "name": "set_deterministic_debug_mode",
    "module": "torch",
    "fullName": "torch.set_deterministic_debug_mode",
    "signature": "(debug_mode: Union[int, str]) -> None",
    "description": "Sets the debug mode for deterministic operations."
  },
  "36": {
    "name": "set_float32_matmul_precision",
    "module": "torch",
    "fullName": "torch.set_float32_matmul_precision",
    "signature": "(precision)",
    "description": "Sets the internal precision of float32 matrix multiplications."
  },
  "37": {
    "name": "set_printoptions",
    "module": "torch",
    "fullName": "torch.set_printoptions",
    "signature": "(precision=None, threshold=None, edgeitems=None, linewidth=None, profile=None, sci_mode=None)",
    "description": "Set options for printing. Items shamelessly taken from NumPy"
  },
  "38": {
    "name": "set_rng_state",
    "module": "torch",
    "fullName": "torch.set_rng_state",
    "signature": "(new_state: torch.Tensor) -> None",
    "description": "Sets the random number generator state."
  },
  "39": {
    "name": "set_warn_always",
    "module": "torch",
    "fullName": "torch.set_warn_always",
    "signature": "(b)",
    "description": "When this flag is False (default) then some PyTorch warnings may only"
  },
  "40": {
    "name": "solve",
    "module": "torch",
    "fullName": "torch.solve",
    "signature": "(input: torch.Tensor, A: torch.Tensor, *, out=None) -> Tuple[torch.Tensor, torch.Tensor]",
    "description": "No description available."
  },
  "41": {
    "name": "split",
    "module": "torch",
    "fullName": "torch.split",
    "signature": "(tensor: torch.Tensor, split_size_or_sections: Union[int, List[int]], dim: int = 0) -> List[torch.Tensor]",
    "description": "Splits the tensor into chunks. Each chunk is a view of the original tensor."
  },
  "42": {
    "name": "svd_lowrank",
    "module": "torch",
    "fullName": "torch.svd_lowrank",
    "signature": "(A: torch.Tensor, q: Optional[int] = 6, niter: Optional[int] = 2, M: Optional[torch.Tensor] = None) -> Tuple[torch.Tensor, torch.Tensor, torch.Tensor]",
    "description": "Return the singular value decomposition ``(U, S, V)`` of a matrix,"
  },
  "43": {
    "name": "tensordot",
    "module": "torch",
    "fullName": "torch.tensordot",
    "signature": "(a, b, dims=2, out: Optional[torch.Tensor] = None)",
    "description": "Returns a contraction of a and b over multiple dimensions."
  },
  "44": {
    "name": "typename",
    "module": "torch",
    "fullName": "torch.typename",
    "signature": "(o)",
    "description": "No description available."
  },
  "45": {
    "name": "unique",
    "module": "torch",
    "fullName": "torch.unique",
    "signature": "(*args, **kwargs)",
    "description": "unique(input, sorted=True, return_inverse=False, return_counts=False, dim=None) -> Tuple[Tensor, Tensor, Tensor]"
  },
  "46": {
    "name": "unique_consecutive",
    "module": "torch",
    "fullName": "torch.unique_consecutive",
    "signature": "(*args, **kwargs)",
    "description": "Eliminates all but the first element from every consecutive group of equivalent elements."
  },
  "47": {
    "name": "use_deterministic_algorithms",
    "module": "torch",
    "fullName": "torch.use_deterministic_algorithms",
    "signature": "(mode, *, warn_only=False)",
    "description": "Sets whether PyTorch operations must use \"deterministic\""
  },
  "48": {
    "name": "filterwarnings",
    "module": "torch.warnings",
    "fullName": "torch.warnings.filterwarnings",
    "signature": "(action, message='', category=<class 'Warning'>, module='', lineno=0, append=False)",
    "description": "Insert an entry into the list of warnings filters (at the front)."
  },
  "49": {
    "name": "formatwarning",
    "module": "torch.warnings",
    "fullName": "torch.warnings.formatwarning",
    "signature": "(message, category, filename, lineno, line=None)",
    "description": "Function to format a warning the standard way."
  },
  "50": {
    "name": "resetwarnings",
    "module": "torch.warnings",
    "fullName": "torch.warnings.resetwarnings",
    "signature": "()",
    "description": "Clear the list of warning filters, so that no filters are active."
  },
  "51": {
    "name": "showwarning",
    "module": "torch.warnings",
    "fullName": "torch.warnings.showwarning",
    "signature": "(message, category, filename, lineno, file=None, line=None)",
    "description": "Hook to write a warning to a file; replace if you like."
  },
  "52": {
    "name": "simplefilter",
    "module": "torch.warnings",
    "fullName": "torch.warnings.simplefilter",
    "signature": "(action, category=<class 'Warning'>, lineno=0, append=False)",
    "description": "Insert a simple entry into the list of warnings filters (at the front)."
  },
  "53": {
    "name": "disable_minidumps",
    "module": "torch.utils",
    "fullName": "torch.utils.disable_minidumps",
    "signature": "()",
    "description": "No description available."
  },
  "54": {
    "name": "enable_minidumps",
    "module": "torch.utils",
    "fullName": "torch.utils.enable_minidumps",
    "signature": "(directory='/tmp/pytorch_crashes')",
    "description": "No description available."
  },
  "55": {
    "name": "enable_minidumps_on_exceptions",
    "module": "torch.utils",
    "fullName": "torch.utils.enable_minidumps_on_exceptions",
    "signature": "()",
    "description": "No description available."
  },
  "56": {
    "name": "set_module",
    "module": "torch.utils",
    "fullName": "torch.utils.set_module",
    "signature": "(obj, mod)",
    "description": "No description available."
  },
  "57": {
    "name": "format_time",
    "module": "torch.utils.throughput_benchmark",
    "fullName": "torch.utils.throughput_benchmark.format_time",
    "signature": "(time_us=None, time_ms=None, time_s=None)",
    "description": "Defines how to format time"
  },
  "58": {
    "name": "unserializable_hook",
    "module": "torch.utils.hooks",
    "fullName": "torch.utils.hooks.unserializable_hook",
    "signature": "(f)",
    "description": "Decorator which marks a function as an unserializable hook."
  },
  "59": {
    "name": "warn_if_has_hooks",
    "module": "torch.utils.hooks",
    "fullName": "torch.utils.hooks.warn_if_has_hooks",
    "signature": "(tensor)",
    "description": "No description available."
  },
  "60": {
    "name": "cache",
    "module": "torch.utils.hooks.functools",
    "fullName": "torch.utils.hooks.functools.cache",
    "signature": "(user_function, /)",
    "description": "Simple lightweight unbounded cache.  Sometimes called \"memoize\"."
  },
  "61": {
    "name": "lru_cache",
    "module": "torch.utils.hooks.functools",
    "fullName": "torch.utils.hooks.functools.lru_cache",
    "signature": "(maxsize=128, typed=False)",
    "description": "Least-recently-used cache decorator."
  },
  "62": {
    "name": "namedtuple",
    "module": "torch.utils.hooks.functools",
    "fullName": "torch.utils.hooks.functools.namedtuple",
    "signature": "(typename, field_names, *, rename=False, defaults=None, module=None)",
    "description": "Returns a new subclass of tuple with named fields."
  },
  "63": {
    "name": "recursive_repr",
    "module": "torch.utils.hooks.functools",
    "fullName": "torch.utils.hooks.functools.recursive_repr",
    "signature": "(fillvalue='...')",
    "description": "Decorator to make a repr function return fillvalue for a recursive call"
  },
  "64": {
    "name": "singledispatch",
    "module": "torch.utils.hooks.functools",
    "fullName": "torch.utils.hooks.functools.singledispatch",
    "signature": "(func)",
    "description": "Single-dispatch generic function decorator."
  },
  "65": {
    "name": "total_ordering",
    "module": "torch.utils.hooks.functools",
    "fullName": "torch.utils.hooks.functools.total_ordering",
    "signature": "(cls)",
    "description": "Class decorator that fills in missing ordering methods"
  },
  "66": {
    "name": "update_wrapper",
    "module": "torch.utils.hooks.functools",
    "fullName": "torch.utils.hooks.functools.update_wrapper",
    "signature": "(wrapper, wrapped, assigned=('__module__', '__name__', '__qualname__', '__doc__', '__annotations__'), updated=('__dict__',))",
    "description": "Update a wrapper function to look like the wrapped function"
  },
  "67": {
    "name": "wraps",
    "module": "torch.utils.hooks.functools",
    "fullName": "torch.utils.hooks.functools.wraps",
    "signature": "(wrapped, assigned=('__module__', '__name__', '__qualname__', '__doc__', '__annotations__'), updated=('__dict__',))",
    "description": "Decorator factory to apply update_wrapper() to a wrapper function"
  },
  "68": {
    "name": "from_dlpack",
    "module": "torch.utils.dlpack",
    "fullName": "torch.utils.dlpack.from_dlpack",
    "signature": "(ext_tensor: Any) -> torch.Tensor",
    "description": "from_dlpack(ext_tensor) -> Tensor"
  },
  "69": {
    "name": "unique",
    "module": "torch.utils.dlpack.enum",
    "fullName": "torch.utils.dlpack.enum.unique",
    "signature": "(enumeration)",
    "description": "Class decorator for enumerations ensuring unique member values."
  },
  "70": {
    "name": "argument_validation",
    "module": "torch.utils.data",
    "fullName": "torch.utils.data.argument_validation",
    "signature": "(f)",
    "description": "No description available."
  },
  "71": {
    "name": "default_collate",
    "module": "torch.utils.data",
    "fullName": "torch.utils.data.default_collate",
    "signature": "(batch)",
    "description": "Function that takes in a batch of data and puts the elements within the batch"
  },
  "72": {
    "name": "default_convert",
    "module": "torch.utils.data",
    "fullName": "torch.utils.data.default_convert",
    "signature": "(data)",
    "description": "Function that converts each NumPy array element into a :class:`torch.Tensor`. If the input is a `Sequence`,"
  },
  "73": {
    "name": "get_worker_info",
    "module": "torch.utils.data",
    "fullName": "torch.utils.data.get_worker_info",
    "signature": "()",
    "description": "Returns the information about the current"
  },
  "74": {
    "name": "random_split",
    "module": "torch.utils.data",
    "fullName": "torch.utils.data.random_split",
    "signature": "(dataset: torch.utils.data.dataset.Dataset[~T], lengths: Sequence[int], generator: Optional[torch._C.Generator] = <torch._C.Generator object at 0x7fce236379b0>) -> List[torch.utils.data.dataset.Subset[~T]]",
    "description": "Randomly split a dataset into non-overlapping new datasets of given lengths."
  },
  "75": {
    "name": "runtime_validation",
    "module": "torch.utils.data",
    "fullName": "torch.utils.data.runtime_validation",
    "signature": "(f)",
    "description": "No description available."
  },
  "76": {
    "name": "apply_sharding",
    "module": "torch.utils.data.graph_settings",
    "fullName": "torch.utils.data.graph_settings.apply_sharding",
    "signature": "(datapipe, num_of_instances, instance_id)",
    "description": "No description available."
  },
  "77": {
    "name": "apply_shuffle_seed",
    "module": "torch.utils.data.graph_settings",
    "fullName": "torch.utils.data.graph_settings.apply_shuffle_seed",
    "signature": "(datapipe, rng)",
    "description": "No description available."
  },
  "78": {
    "name": "apply_shuffle_settings",
    "module": "torch.utils.data.graph_settings",
    "fullName": "torch.utils.data.graph_settings.apply_shuffle_settings",
    "signature": "(datapipe, shuffle)",
    "description": "No description available."
  },
  "79": {
    "name": "get_all_graph_pipes",
    "module": "torch.utils.data.graph_settings",
    "fullName": "torch.utils.data.graph_settings.get_all_graph_pipes",
    "signature": "(graph)",
    "description": "No description available."
  },
  "80": {
    "name": "DataPipe",
    "module": "torch.utils.data.graph",
    "fullName": "torch.utils.data.graph.DataPipe",
    "signature": "(*args, **kwargs)",
    "description": "No description available."
  },
  "81": {
    "name": "traverse",
    "module": "torch.utils.data.graph",
    "fullName": "torch.utils.data.graph.traverse",
    "signature": "(datapipe, only_datapipe=False)",
    "description": "No description available."
  },
  "82": {
    "name": "decode_long",
    "module": "torch.utils.data.graph.pickle",
    "fullName": "torch.utils.data.graph.pickle.decode_long",
    "signature": "(data)",
    "description": "Decode a long from a two's complement little-endian binary string."
  },
  "83": {
    "name": "encode_long",
    "module": "torch.utils.data.graph.pickle",
    "fullName": "torch.utils.data.graph.pickle.encode_long",
    "signature": "(x)",
    "description": "Encode a long to a two's complement little-endian binary string."
  },
  "84": {
    "name": "whichmodule",
    "module": "torch.utils.data.graph.pickle",
    "fullName": "torch.utils.data.graph.pickle.whichmodule",
    "signature": "(obj, name)",
    "description": "Find the module an object belong to."
  },
  "85": {
    "name": "compile",
    "module": "torch.utils.data.graph.pickle.re",
    "fullName": "torch.utils.data.graph.pickle.re.compile",
    "signature": "(pattern, flags=0)",
    "description": "Compile a regular expression pattern, returning a Pattern object."
  },
  "86": {
    "name": "escape",
    "module": "torch.utils.data.graph.pickle.re",
    "fullName": "torch.utils.data.graph.pickle.re.escape",
    "signature": "(pattern)",
    "description": "Escape special characters in a string."
  },
  "87": {
    "name": "findall",
    "module": "torch.utils.data.graph.pickle.re",
    "fullName": "torch.utils.data.graph.pickle.re.findall",
    "signature": "(pattern, string, flags=0)",
    "description": "Return a list of all non-overlapping matches in the string."
  },
  "88": {
    "name": "finditer",
    "module": "torch.utils.data.graph.pickle.re",
    "fullName": "torch.utils.data.graph.pickle.re.finditer",
    "signature": "(pattern, string, flags=0)",
    "description": "Return an iterator over all non-overlapping matches in the"
  },
  "89": {
    "name": "fullmatch",
    "module": "torch.utils.data.graph.pickle.re",
    "fullName": "torch.utils.data.graph.pickle.re.fullmatch",
    "signature": "(pattern, string, flags=0)",
    "description": "Try to apply the pattern to all of the string, returning"
  },
  "90": {
    "name": "match",
    "module": "torch.utils.data.graph.pickle.re",
    "fullName": "torch.utils.data.graph.pickle.re.match",
    "signature": "(pattern, string, flags=0)",
    "description": "Try to apply the pattern at the start of the string, returning"
  },
  "91": {
    "name": "purge",
    "module": "torch.utils.data.graph.pickle.re",
    "fullName": "torch.utils.data.graph.pickle.re.purge",
    "signature": "()",
    "description": "Clear the regular expression caches"
  },
  "92": {
    "name": "search",
    "module": "torch.utils.data.graph.pickle.re",
    "fullName": "torch.utils.data.graph.pickle.re.search",
    "signature": "(pattern, string, flags=0)",
    "description": "Scan through string looking for a match to the pattern, returning"
  },
  "93": {
    "name": "split",
    "module": "torch.utils.data.graph.pickle.re",
    "fullName": "torch.utils.data.graph.pickle.re.split",
    "signature": "(pattern, string, maxsplit=0, flags=0)",
    "description": "Split the source string by the occurrences of the pattern,"
  },
  "94": {
    "name": "sub",
    "module": "torch.utils.data.graph.pickle.re",
    "fullName": "torch.utils.data.graph.pickle.re.sub",
    "signature": "(pattern, repl, string, count=0, flags=0)",
    "description": "Return the string obtained by replacing the leftmost"
  },
  "95": {
    "name": "subn",
    "module": "torch.utils.data.graph.pickle.re",
    "fullName": "torch.utils.data.graph.pickle.re.subn",
    "signature": "(pattern, repl, string, count=0, flags=0)",
    "description": "Return a 2-tuple containing (new_string, number)."
  },
  "96": {
    "name": "template",
    "module": "torch.utils.data.graph.pickle.re",
    "fullName": "torch.utils.data.graph.pickle.re.template",
    "signature": "(pattern, flags=0)",
    "description": "Compile a template pattern, returning a Pattern object"
  },
  "97": {
    "name": "expand_template",
    "module": "torch.utils.data.graph.pickle.re.sre_parse",
    "fullName": "torch.utils.data.graph.pickle.re.sre_parse.expand_template",
    "signature": "(template, match)",
    "description": "No description available."
  },
  "98": {
    "name": "fix_flags",
    "module": "torch.utils.data.graph.pickle.re.sre_parse",
    "fullName": "torch.utils.data.graph.pickle.re.sre_parse.fix_flags",
    "signature": "(src, flags)",
    "description": "No description available."
  },
  "99": {
    "name": "parse",
    "module": "torch.utils.data.graph.pickle.re.sre_parse",
    "fullName": "torch.utils.data.graph.pickle.re.sre_parse.parse",
    "signature": "(str, flags=0, state=None)",
    "description": "No description available."
  },
  "100": {
    "name": "parse_template",
    "module": "torch.utils.data.graph.pickle.re.sre_parse",
    "fullName": "torch.utils.data.graph.pickle.re.sre_parse.parse_template",
    "signature": "(source, state)",
    "description": "No description available."
  },
  "101": {
    "name": "compile",
    "module": "torch.utils.data.graph.pickle.re.sre_compile",
    "fullName": "torch.utils.data.graph.pickle.re.sre_compile.compile",
    "signature": "(p, flags=0)",
    "description": "No description available."
  },
  "102": {
    "name": "dis",
    "module": "torch.utils.data.graph.pickle.re.sre_compile",
    "fullName": "torch.utils.data.graph.pickle.re.sre_compile.dis",
    "signature": "(code)",
    "description": "No description available."
  },
  "103": {
    "name": "isstring",
    "module": "torch.utils.data.graph.pickle.re.sre_compile",
    "fullName": "torch.utils.data.graph.pickle.re.sre_compile.isstring",
    "signature": "(obj)",
    "description": "No description available."
  },
  "104": {
    "name": "add_extension",
    "module": "torch.utils.data.graph.pickle.re.copyreg",
    "fullName": "torch.utils.data.graph.pickle.re.copyreg.add_extension",
    "signature": "(module, name, code)",
    "description": "Register an extension code."
  },
  "105": {
    "name": "clear_extension_cache",
    "module": "torch.utils.data.graph.pickle.re.copyreg",
    "fullName": "torch.utils.data.graph.pickle.re.copyreg.clear_extension_cache",
    "signature": "()",
    "description": "No description available."
  },
  "106": {
    "name": "constructor",
    "module": "torch.utils.data.graph.pickle.re.copyreg",
    "fullName": "torch.utils.data.graph.pickle.re.copyreg.constructor",
    "signature": "(object)",
    "description": "No description available."
  },
  "107": {
    "name": "pickle",
    "module": "torch.utils.data.graph.pickle.re.copyreg",
    "fullName": "torch.utils.data.graph.pickle.re.copyreg.pickle",
    "signature": "(ob_type, pickle_function, constructor_ob=None)",
    "description": "No description available."
  },
  "108": {
    "name": "pickle_complex",
    "module": "torch.utils.data.graph.pickle.re.copyreg",
    "fullName": "torch.utils.data.graph.pickle.re.copyreg.pickle_complex",
    "signature": "(c)",
    "description": "No description available."
  },
  "109": {
    "name": "remove_extension",
    "module": "torch.utils.data.graph.pickle.re.copyreg",
    "fullName": "torch.utils.data.graph.pickle.re.copyreg.remove_extension",
    "signature": "(module, name, code)",
    "description": "Unregister an extension code.  For testing only."
  },
  "110": {
    "name": "abstractmethod",
    "module": "torch.utils.data.graph.pickle.io.abc",
    "fullName": "torch.utils.data.graph.pickle.io.abc.abstractmethod",
    "signature": "(funcobj)",
    "description": "A decorator indicating abstract methods."
  },
  "111": {
    "name": "EncodedFile",
    "module": "torch.utils.data.graph.pickle.codecs",
    "fullName": "torch.utils.data.graph.pickle.codecs.EncodedFile",
    "signature": "(file, data_encoding, file_encoding=None, errors='strict')",
    "description": "Return a wrapped version of file which provides transparent"
  },
  "112": {
    "name": "getdecoder",
    "module": "torch.utils.data.graph.pickle.codecs",
    "fullName": "torch.utils.data.graph.pickle.codecs.getdecoder",
    "signature": "(encoding)",
    "description": "Lookup up the codec for the given encoding and return"
  },
  "113": {
    "name": "getencoder",
    "module": "torch.utils.data.graph.pickle.codecs",
    "fullName": "torch.utils.data.graph.pickle.codecs.getencoder",
    "signature": "(encoding)",
    "description": "Lookup up the codec for the given encoding and return"
  },
  "114": {
    "name": "getincrementaldecoder",
    "module": "torch.utils.data.graph.pickle.codecs",
    "fullName": "torch.utils.data.graph.pickle.codecs.getincrementaldecoder",
    "signature": "(encoding)",
    "description": "Lookup up the codec for the given encoding and return"
  },
  "115": {
    "name": "getincrementalencoder",
    "module": "torch.utils.data.graph.pickle.codecs",
    "fullName": "torch.utils.data.graph.pickle.codecs.getincrementalencoder",
    "signature": "(encoding)",
    "description": "Lookup up the codec for the given encoding and return"
  },
  "116": {
    "name": "getreader",
    "module": "torch.utils.data.graph.pickle.codecs",
    "fullName": "torch.utils.data.graph.pickle.codecs.getreader",
    "signature": "(encoding)",
    "description": "Lookup up the codec for the given encoding and return"
  },
  "117": {
    "name": "getwriter",
    "module": "torch.utils.data.graph.pickle.codecs",
    "fullName": "torch.utils.data.graph.pickle.codecs.getwriter",
    "signature": "(encoding)",
    "description": "Lookup up the codec for the given encoding and return"
  },
  "118": {
    "name": "iterdecode",
    "module": "torch.utils.data.graph.pickle.codecs",
    "fullName": "torch.utils.data.graph.pickle.codecs.iterdecode",
    "signature": "(iterator, encoding, errors='strict', **kwargs)",
    "description": "Decoding iterator."
  },
  "119": {
    "name": "iterencode",
    "module": "torch.utils.data.graph.pickle.codecs",
    "fullName": "torch.utils.data.graph.pickle.codecs.iterencode",
    "signature": "(iterator, encoding, errors='strict', **kwargs)",
    "description": "Encoding iterator."
  },
  "120": {
    "name": "make_encoding_map",
    "module": "torch.utils.data.graph.pickle.codecs",
    "fullName": "torch.utils.data.graph.pickle.codecs.make_encoding_map",
    "signature": "(decoding_map)",
    "description": "Creates an encoding map from a decoding map."
  },
  "121": {
    "name": "make_identity_dict",
    "module": "torch.utils.data.graph.pickle.codecs",
    "fullName": "torch.utils.data.graph.pickle.codecs.make_identity_dict",
    "signature": "(rng)",
    "description": "make_identity_dict(rng) -> dict"
  },
  "122": {
    "name": "open",
    "module": "torch.utils.data.graph.pickle.codecs",
    "fullName": "torch.utils.data.graph.pickle.codecs.open",
    "signature": "(filename, mode='r', encoding=None, errors='strict', buffering=-1)",
    "description": "Open an encoded file using the given mode and return"
  },
  "123": {
    "name": "all_gather",
    "module": "torch.utils.data.distributed.dist",
    "fullName": "torch.utils.data.distributed.dist.all_gather",
    "signature": "(tensor_list, tensor, group=None, async_op=False)",
    "description": "Gathers tensors from the whole group in a list."
  },
  "124": {
    "name": "all_gather_coalesced",
    "module": "torch.utils.data.distributed.dist",
    "fullName": "torch.utils.data.distributed.dist.all_gather_coalesced",
    "signature": "(output_tensor_lists, input_tensor_list, group=None, async_op=False)",
    "description": "Gathers input tensors from the whole group in a list in a coalesced manner."
  },
  "125": {
    "name": "all_gather_multigpu",
    "module": "torch.utils.data.distributed.dist",
    "fullName": "torch.utils.data.distributed.dist.all_gather_multigpu",
    "signature": "(output_tensor_lists, input_tensor_list, group=None, async_op=False)",
    "description": "Gathers tensors from the whole group in a list."
  },
  "126": {
    "name": "all_gather_object",
    "module": "torch.utils.data.distributed.dist",
    "fullName": "torch.utils.data.distributed.dist.all_gather_object",
    "signature": "(object_list, obj, group=None)",
    "description": "Gathers picklable objects from the whole group into a list. Similar to"
  },
  "127": {
    "name": "all_reduce",
    "module": "torch.utils.data.distributed.dist",
    "fullName": "torch.utils.data.distributed.dist.all_reduce",
    "signature": "(tensor, op=<ReduceOp.SUM: 0>, group=None, async_op=False)",
    "description": "Reduces the tensor data across all machines in such a way that all get"
  },
  "128": {
    "name": "all_reduce_coalesced",
    "module": "torch.utils.data.distributed.dist",
    "fullName": "torch.utils.data.distributed.dist.all_reduce_coalesced",
    "signature": "(tensors, op=<ReduceOp.SUM: 0>, group=None, async_op=False)",
    "description": "WARNING: at this time individual shape checking is not implemented across nodes."
  },
  "129": {
    "name": "all_reduce_multigpu",
    "module": "torch.utils.data.distributed.dist",
    "fullName": "torch.utils.data.distributed.dist.all_reduce_multigpu",
    "signature": "(tensor_list, op=<ReduceOp.SUM: 0>, group=None, async_op=False)",
    "description": "Reduces the tensor data across all machines in such a way that all get"
  },
  "130": {
    "name": "barrier",
    "module": "torch.utils.data.distributed.dist",
    "fullName": "torch.utils.data.distributed.dist.barrier",
    "signature": "(group=None, async_op=False, device_ids=None)",
    "description": "Synchronizes all processes."
  },
  "131": {
    "name": "batch_isend_irecv",
    "module": "torch.utils.data.distributed.dist",
    "fullName": "torch.utils.data.distributed.dist.batch_isend_irecv",
    "signature": "(p2p_op_list)",
    "description": "Send or Receive a batch of tensors asynchronously and return a list of requests."
  },
  "132": {
    "name": "broadcast",
    "module": "torch.utils.data.distributed.dist",
    "fullName": "torch.utils.data.distributed.dist.broadcast",
    "signature": "(tensor, src, group=None, async_op=False)",
    "description": "Broadcasts the tensor to the whole group."
  },
  "133": {
    "name": "broadcast_multigpu",
    "module": "torch.utils.data.distributed.dist",
    "fullName": "torch.utils.data.distributed.dist.broadcast_multigpu",
    "signature": "(tensor_list, src, group=None, async_op=False, src_tensor=0)",
    "description": "Broadcasts the tensor to the whole group with multiple GPU tensors"
  },
  "134": {
    "name": "broadcast_object_list",
    "module": "torch.utils.data.distributed.dist",
    "fullName": "torch.utils.data.distributed.dist.broadcast_object_list",
    "signature": "(object_list, src=0, group=None, device=None)",
    "description": "Broadcasts picklable objects in ``object_list`` to the whole group. Similar"
  },
  "135": {
    "name": "destroy_process_group",
    "module": "torch.utils.data.distributed.dist",
    "fullName": "torch.utils.data.distributed.dist.destroy_process_group",
    "signature": "(group=None)",
    "description": "Destroy a given process group, and deinitialize the distributed package"
  },
  "136": {
    "name": "gather",
    "module": "torch.utils.data.distributed.dist",
    "fullName": "torch.utils.data.distributed.dist.gather",
    "signature": "(tensor, gather_list=None, dst=0, group=None, async_op=False)",
    "description": "Gathers a list of tensors in a single process."
  },
  "137": {
    "name": "gather_object",
    "module": "torch.utils.data.distributed.dist",
    "fullName": "torch.utils.data.distributed.dist.gather_object",
    "signature": "(obj, object_gather_list=None, dst=0, group=None)",
    "description": "Gathers picklable objects from the whole group in a single process."
  },
  "138": {
    "name": "get_backend",
    "module": "torch.utils.data.distributed.dist",
    "fullName": "torch.utils.data.distributed.dist.get_backend",
    "signature": "(group=None)",
    "description": "Returns the backend of the given process group."
  },
  "139": {
    "name": "get_rank",
    "module": "torch.utils.data.distributed.dist",
    "fullName": "torch.utils.data.distributed.dist.get_rank",
    "signature": "(group=None)",
    "description": "Returns the rank of the current process in the provided ``group`` or the"
  },
  "140": {
    "name": "get_world_size",
    "module": "torch.utils.data.distributed.dist",
    "fullName": "torch.utils.data.distributed.dist.get_world_size",
    "signature": "(group=None)",
    "description": "Returns the number of processes in the current process group"
  },
  "141": {
    "name": "irecv",
    "module": "torch.utils.data.distributed.dist",
    "fullName": "torch.utils.data.distributed.dist.irecv",
    "signature": "(tensor, src=None, group=None, tag=0)",
    "description": "Receives a tensor asynchronously."
  },
  "142": {
    "name": "is_available",
    "module": "torch.utils.data.distributed.dist",
    "fullName": "torch.utils.data.distributed.dist.is_available",
    "signature": "() -> bool",
    "description": "Returns ``True`` if the distributed package is available. Otherwise,"
  },
  "143": {
    "name": "is_gloo_available",
    "module": "torch.utils.data.distributed.dist",
    "fullName": "torch.utils.data.distributed.dist.is_gloo_available",
    "signature": "()",
    "description": "Checks if the Gloo backend is available."
  },
  "144": {
    "name": "is_initialized",
    "module": "torch.utils.data.distributed.dist",
    "fullName": "torch.utils.data.distributed.dist.is_initialized",
    "signature": "()",
    "description": "Checking if the default process group has been initialized"
  },
  "145": {
    "name": "is_mpi_available",
    "module": "torch.utils.data.distributed.dist",
    "fullName": "torch.utils.data.distributed.dist.is_mpi_available",
    "signature": "()",
    "description": "Checks if the MPI backend is available."
  },
  "146": {
    "name": "is_nccl_available",
    "module": "torch.utils.data.distributed.dist",
    "fullName": "torch.utils.data.distributed.dist.is_nccl_available",
    "signature": "()",
    "description": "Checks if the NCCL backend is available."
  },
  "147": {
    "name": "is_torchelastic_launched",
    "module": "torch.utils.data.distributed.dist",
    "fullName": "torch.utils.data.distributed.dist.is_torchelastic_launched",
    "signature": "()",
    "description": "Checks whether this process was launched with ``torch.distributed.elastic``"
  },
  "148": {
    "name": "isend",
    "module": "torch.utils.data.distributed.dist",
    "fullName": "torch.utils.data.distributed.dist.isend",
    "signature": "(tensor, dst, group=None, tag=0)",
    "description": "Sends a tensor asynchronously."
  },
  "149": {
    "name": "monitored_barrier",
    "module": "torch.utils.data.distributed.dist",
    "fullName": "torch.utils.data.distributed.dist.monitored_barrier",
    "signature": "(group=None, timeout=None, wait_all_ranks=False)",
    "description": "Synchronizes all processes similar to ``torch.distributed.barrier``, but takes"
  },
  "150": {
    "name": "new_group",
    "module": "torch.utils.data.distributed.dist",
    "fullName": "torch.utils.data.distributed.dist.new_group",
    "signature": "(ranks=None, timeout=datetime.timedelta(seconds=1800), backend=None, pg_options=None)",
    "description": "Creates a new distributed group."
  },
  "151": {
    "name": "new_subgroups",
    "module": "torch.utils.data.distributed.dist",
    "fullName": "torch.utils.data.distributed.dist.new_subgroups",
    "signature": "(group_size=None, group=None, timeout=datetime.timedelta(seconds=1800), backend=None, pg_options=None)",
    "description": "Creates GPU subgroups of equal size. By default, it creates intra-machine subgroups,"
  },
  "152": {
    "name": "new_subgroups_by_enumeration",
    "module": "torch.utils.data.distributed.dist",
    "fullName": "torch.utils.data.distributed.dist.new_subgroups_by_enumeration",
    "signature": "(ranks_per_subgroup_list, timeout=datetime.timedelta(seconds=1800), backend=None, pg_options=None)",
    "description": "Creates GPU subgroups by dividing the global world, where the division is specified by"
  },
  "153": {
    "name": "recv",
    "module": "torch.utils.data.distributed.dist",
    "fullName": "torch.utils.data.distributed.dist.recv",
    "signature": "(tensor, src=None, group=None, tag=0)",
    "description": "Receives a tensor synchronously."
  },
  "154": {
    "name": "reduce",
    "module": "torch.utils.data.distributed.dist",
    "fullName": "torch.utils.data.distributed.dist.reduce",
    "signature": "(tensor, dst, op=<ReduceOp.SUM: 0>, group=None, async_op=False)",
    "description": "Reduces the tensor data across all machines."
  },
  "155": {
    "name": "reduce_multigpu",
    "module": "torch.utils.data.distributed.dist",
    "fullName": "torch.utils.data.distributed.dist.reduce_multigpu",
    "signature": "(tensor_list, dst, op=<ReduceOp.SUM: 0>, group=None, async_op=False, dst_tensor=0)",
    "description": "Reduces the tensor data on multiple GPUs across all machines. Each tensor"
  },
  "156": {
    "name": "reduce_scatter",
    "module": "torch.utils.data.distributed.dist",
    "fullName": "torch.utils.data.distributed.dist.reduce_scatter",
    "signature": "(output, input_list, op=<ReduceOp.SUM: 0>, group=None, async_op=False)",
    "description": "Reduces, then scatters a list of tensors to all processes in a group."
  },
  "157": {
    "name": "reduce_scatter_multigpu",
    "module": "torch.utils.data.distributed.dist",
    "fullName": "torch.utils.data.distributed.dist.reduce_scatter_multigpu",
    "signature": "(output_tensor_list, input_tensor_lists, op=<ReduceOp.SUM: 0>, group=None, async_op=False)",
    "description": "Reduce and scatter a list of tensors to the whole group.  Only nccl backend"
  },
  "158": {
    "name": "register_rendezvous_handler",
    "module": "torch.utils.data.distributed.dist",
    "fullName": "torch.utils.data.distributed.dist.register_rendezvous_handler",
    "signature": "(scheme, handler)",
    "description": "Registers a new rendezvous handler."
  },
  "159": {
    "name": "rendezvous",
    "module": "torch.utils.data.distributed.dist",
    "fullName": "torch.utils.data.distributed.dist.rendezvous",
    "signature": "(url: str, rank: int = -1, world_size: int = -1, **kwargs)",
    "description": "No description available."
  },
  "160": {
    "name": "scatter",
    "module": "torch.utils.data.distributed.dist",
    "fullName": "torch.utils.data.distributed.dist.scatter",
    "signature": "(tensor, scatter_list=None, src=0, group=None, async_op=False)",
    "description": "Scatters a list of tensors to all processes in a group."
  },
  "161": {
    "name": "scatter_object_list",
    "module": "torch.utils.data.distributed.dist",
    "fullName": "torch.utils.data.distributed.dist.scatter_object_list",
    "signature": "(scatter_object_output_list, scatter_object_input_list, src=0, group=None)",
    "description": "Scatters picklable objects in ``scatter_object_input_list`` to the whole"
  },
  "162": {
    "name": "send",
    "module": "torch.utils.data.distributed.dist",
    "fullName": "torch.utils.data.distributed.dist.send",
    "signature": "(tensor, dst, group=None, tag=0)",
    "description": "Sends a tensor synchronously."
  },
  "163": {
    "name": "supports_complex",
    "module": "torch.utils.data.distributed.dist",
    "fullName": "torch.utils.data.distributed.dist.supports_complex",
    "signature": "(reduceOp: torch._C._distributed_c10d.ReduceOp) -> bool",
    "description": "No description available."
  },
  "164": {
    "name": "namedtuple",
    "module": "torch.utils.data.distributed.dist.utils.collections",
    "fullName": "torch.utils.data.distributed.dist.utils.collections.namedtuple",
    "signature": "(typename, field_names, *, rename=False, defaults=None, module=None)",
    "description": "Returns a new subclass of tuple with named fields."
  },
  "165": {
    "name": "get_worker_info",
    "module": "torch.utils.data.distributed.dist.rpc",
    "fullName": "torch.utils.data.distributed.dist.rpc.get_worker_info",
    "signature": "(worker_name=None)",
    "description": "Get :class:`~torch.distributed.rpc.WorkerInfo` of a given worker name."
  },
  "166": {
    "name": "init_rpc",
    "module": "torch.utils.data.distributed.dist.rpc",
    "fullName": "torch.utils.data.distributed.dist.rpc.init_rpc",
    "signature": "(name, backend=None, rank=-1, world_size=None, rpc_backend_options=None)",
    "description": "Initializes RPC primitives such as the local RPC agent"
  },
  "167": {
    "name": "is_available",
    "module": "torch.utils.data.distributed.dist.rpc",
    "fullName": "torch.utils.data.distributed.dist.rpc.is_available",
    "signature": "()",
    "description": "No description available."
  },
  "168": {
    "name": "method_factory",
    "module": "torch.utils.data.distributed.dist.rpc",
    "fullName": "torch.utils.data.distributed.dist.rpc.method_factory",
    "signature": "(method_name, docstring)",
    "description": "No description available."
  },
  "169": {
    "name": "new_method",
    "module": "torch.utils.data.distributed.dist.rpc",
    "fullName": "torch.utils.data.distributed.dist.rpc.new_method",
    "signature": "(self, *args, **kwargs)",
    "description": "No description available."
  },
  "170": {
    "name": "remote",
    "module": "torch.utils.data.distributed.dist.rpc",
    "fullName": "torch.utils.data.distributed.dist.rpc.remote",
    "signature": "(to, func, args=None, kwargs=None, timeout=-1.0)",
    "description": "Make a remote call to run ``func`` on worker ``to`` and return an"
  },
  "171": {
    "name": "rpc_async",
    "module": "torch.utils.data.distributed.dist.rpc",
    "fullName": "torch.utils.data.distributed.dist.rpc.rpc_async",
    "signature": "(to, func, args=None, kwargs=None, timeout=-1.0)",
    "description": "Make a non-blocking RPC call to run function ``func`` on worker ``to``. RPC"
  },
  "172": {
    "name": "rpc_sync",
    "module": "torch.utils.data.distributed.dist.rpc",
    "fullName": "torch.utils.data.distributed.dist.rpc.rpc_sync",
    "signature": "(to, func, args=None, kwargs=None, timeout=-1.0)",
    "description": "Make a blocking RPC call to run function ``func`` on worker ``to``. RPC"
  },
  "173": {
    "name": "shutdown",
    "module": "torch.utils.data.distributed.dist.rpc",
    "fullName": "torch.utils.data.distributed.dist.rpc.shutdown",
    "signature": "(graceful=True, timeout=0)",
    "description": "Perform a shutdown of the RPC agent, and then destroy the RPC agent. This"
  },
  "174": {
    "name": "urlparse",
    "module": "torch.utils.data.distributed.dist.rpc",
    "fullName": "torch.utils.data.distributed.dist.rpc.urlparse",
    "signature": "(url, scheme='', allow_fragments=True)",
    "description": "Parse a URL into 6 components:"
  },
  "175": {
    "name": "RLock",
    "module": "torch.utils.data.distributed.dist.rpc.threading",
    "fullName": "torch.utils.data.distributed.dist.rpc.threading.RLock",
    "signature": "(*args, **kwargs)",
    "description": "Factory function that returns a new reentrant lock."
  },
  "176": {
    "name": "activeCount",
    "module": "torch.utils.data.distributed.dist.rpc.threading",
    "fullName": "torch.utils.data.distributed.dist.rpc.threading.activeCount",
    "signature": "()",
    "description": "Return the number of Thread objects currently alive."
  },
  "177": {
    "name": "active_count",
    "module": "torch.utils.data.distributed.dist.rpc.threading",
    "fullName": "torch.utils.data.distributed.dist.rpc.threading.active_count",
    "signature": "()",
    "description": "Return the number of Thread objects currently alive."
  },
  "178": {
    "name": "currentThread",
    "module": "torch.utils.data.distributed.dist.rpc.threading",
    "fullName": "torch.utils.data.distributed.dist.rpc.threading.currentThread",
    "signature": "()",
    "description": "Return the current Thread object, corresponding to the caller's thread of control."
  },
  "179": {
    "name": "current_thread",
    "module": "torch.utils.data.distributed.dist.rpc.threading",
    "fullName": "torch.utils.data.distributed.dist.rpc.threading.current_thread",
    "signature": "()",
    "description": "Return the current Thread object, corresponding to the caller's thread of control."
  },
  "180": {
    "name": "enumerate",
    "module": "torch.utils.data.distributed.dist.rpc.threading",
    "fullName": "torch.utils.data.distributed.dist.rpc.threading.enumerate",
    "signature": "()",
    "description": "Return a list of all Thread objects currently alive."
  },
  "181": {
    "name": "main_thread",
    "module": "torch.utils.data.distributed.dist.rpc.threading",
    "fullName": "torch.utils.data.distributed.dist.rpc.threading.main_thread",
    "signature": "()",
    "description": "Return the main thread object."
  },
  "182": {
    "name": "setprofile",
    "module": "torch.utils.data.distributed.dist.rpc.threading",
    "fullName": "torch.utils.data.distributed.dist.rpc.threading.setprofile",
    "signature": "(func)",
    "description": "Set a profile function for all threads started from the threading module."
  },
  "183": {
    "name": "settrace",
    "module": "torch.utils.data.distributed.dist.rpc.threading",
    "fullName": "torch.utils.data.distributed.dist.rpc.threading.settrace",
    "signature": "(func)",
    "description": "Set a trace function for all threads started from the threading module."
  },
  "184": {
    "name": "execl",
    "module": "torch.utils.data.distributed.dist.rpc.os",
    "fullName": "torch.utils.data.distributed.dist.rpc.os.execl",
    "signature": "(file, *args)",
    "description": "execl(file, *args)"
  },
  "185": {
    "name": "execle",
    "module": "torch.utils.data.distributed.dist.rpc.os",
    "fullName": "torch.utils.data.distributed.dist.rpc.os.execle",
    "signature": "(file, *args)",
    "description": "execle(file, *args, env)"
  },
  "186": {
    "name": "execlp",
    "module": "torch.utils.data.distributed.dist.rpc.os",
    "fullName": "torch.utils.data.distributed.dist.rpc.os.execlp",
    "signature": "(file, *args)",
    "description": "execlp(file, *args)"
  },
  "187": {
    "name": "execlpe",
    "module": "torch.utils.data.distributed.dist.rpc.os",
    "fullName": "torch.utils.data.distributed.dist.rpc.os.execlpe",
    "signature": "(file, *args)",
    "description": "execlpe(file, *args, env)"
  },
  "188": {
    "name": "execvp",
    "module": "torch.utils.data.distributed.dist.rpc.os",
    "fullName": "torch.utils.data.distributed.dist.rpc.os.execvp",
    "signature": "(file, args)",
    "description": "execvp(file, args)"
  },
  "189": {
    "name": "execvpe",
    "module": "torch.utils.data.distributed.dist.rpc.os",
    "fullName": "torch.utils.data.distributed.dist.rpc.os.execvpe",
    "signature": "(file, args, env)",
    "description": "execvpe(file, args, env)"
  },
  "190": {
    "name": "fdopen",
    "module": "torch.utils.data.distributed.dist.rpc.os",
    "fullName": "torch.utils.data.distributed.dist.rpc.os.fdopen",
    "signature": "(fd, *args, **kwargs)",
    "description": "No description available."
  },
  "191": {
    "name": "fsdecode",
    "module": "torch.utils.data.distributed.dist.rpc.os",
    "fullName": "torch.utils.data.distributed.dist.rpc.os.fsdecode",
    "signature": "(filename)",
    "description": "Decode filename (an os.PathLike, bytes, or str) from the filesystem"
  },
  "192": {
    "name": "fsencode",
    "module": "torch.utils.data.distributed.dist.rpc.os",
    "fullName": "torch.utils.data.distributed.dist.rpc.os.fsencode",
    "signature": "(filename)",
    "description": "Encode filename (an os.PathLike, bytes, or str) to the filesystem"
  },
  "193": {
    "name": "fwalk",
    "module": "torch.utils.data.distributed.dist.rpc.os",
    "fullName": "torch.utils.data.distributed.dist.rpc.os.fwalk",
    "signature": "(top='.', topdown=True, onerror=None, *, follow_symlinks=False, dir_fd=None)",
    "description": "Directory tree generator."
  },
  "194": {
    "name": "get_exec_path",
    "module": "torch.utils.data.distributed.dist.rpc.os",
    "fullName": "torch.utils.data.distributed.dist.rpc.os.get_exec_path",
    "signature": "(env=None)",
    "description": "Returns the sequence of directories that will be searched for the"
  },
  "195": {
    "name": "getenv",
    "module": "torch.utils.data.distributed.dist.rpc.os",
    "fullName": "torch.utils.data.distributed.dist.rpc.os.getenv",
    "signature": "(key, default=None)",
    "description": "Get an environment variable, return None if it doesn't exist."
  },
  "196": {
    "name": "getenvb",
    "module": "torch.utils.data.distributed.dist.rpc.os",
    "fullName": "torch.utils.data.distributed.dist.rpc.os.getenvb",
    "signature": "(key, default=None)",
    "description": "Get an environment variable, return None if it doesn't exist."
  },
  "197": {
    "name": "makedirs",
    "module": "torch.utils.data.distributed.dist.rpc.os",
    "fullName": "torch.utils.data.distributed.dist.rpc.os.makedirs",
    "signature": "(name, mode=511, exist_ok=False)",
    "description": "makedirs(name [, mode=0o777][, exist_ok=False])"
  },
  "198": {
    "name": "popen",
    "module": "torch.utils.data.distributed.dist.rpc.os",
    "fullName": "torch.utils.data.distributed.dist.rpc.os.popen",
    "signature": "(cmd, mode='r', buffering=-1)",
    "description": "No description available."
  },
  "199": {
    "name": "removedirs",
    "module": "torch.utils.data.distributed.dist.rpc.os",
    "fullName": "torch.utils.data.distributed.dist.rpc.os.removedirs",
    "signature": "(name)",
    "description": "removedirs(name)"
  },
  "200": {
    "name": "renames",
    "module": "torch.utils.data.distributed.dist.rpc.os",
    "fullName": "torch.utils.data.distributed.dist.rpc.os.renames",
    "signature": "(old, new)",
    "description": "renames(old, new)"
  },
  "201": {
    "name": "spawnl",
    "module": "torch.utils.data.distributed.dist.rpc.os",
    "fullName": "torch.utils.data.distributed.dist.rpc.os.spawnl",
    "signature": "(mode, file, *args)",
    "description": "spawnl(mode, file, *args) -> integer"
  },
  "202": {
    "name": "spawnle",
    "module": "torch.utils.data.distributed.dist.rpc.os",
    "fullName": "torch.utils.data.distributed.dist.rpc.os.spawnle",
    "signature": "(mode, file, *args)",
    "description": "spawnle(mode, file, *args, env) -> integer"
  },
  "203": {
    "name": "spawnlp",
    "module": "torch.utils.data.distributed.dist.rpc.os",
    "fullName": "torch.utils.data.distributed.dist.rpc.os.spawnlp",
    "signature": "(mode, file, *args)",
    "description": "spawnlp(mode, file, *args) -> integer"
  },
  "204": {
    "name": "spawnlpe",
    "module": "torch.utils.data.distributed.dist.rpc.os",
    "fullName": "torch.utils.data.distributed.dist.rpc.os.spawnlpe",
    "signature": "(mode, file, *args)",
    "description": "spawnlpe(mode, file, *args, env) -> integer"
  },
  "205": {
    "name": "spawnv",
    "module": "torch.utils.data.distributed.dist.rpc.os",
    "fullName": "torch.utils.data.distributed.dist.rpc.os.spawnv",
    "signature": "(mode, file, args)",
    "description": "spawnv(mode, file, args) -> integer"
  },
  "206": {
    "name": "spawnve",
    "module": "torch.utils.data.distributed.dist.rpc.os",
    "fullName": "torch.utils.data.distributed.dist.rpc.os.spawnve",
    "signature": "(mode, file, args, env)",
    "description": "spawnve(mode, file, args, env) -> integer"
  },
  "207": {
    "name": "spawnvp",
    "module": "torch.utils.data.distributed.dist.rpc.os",
    "fullName": "torch.utils.data.distributed.dist.rpc.os.spawnvp",
    "signature": "(mode, file, args)",
    "description": "spawnvp(mode, file, args) -> integer"
  },
  "208": {
    "name": "spawnvpe",
    "module": "torch.utils.data.distributed.dist.rpc.os",
    "fullName": "torch.utils.data.distributed.dist.rpc.os.spawnvpe",
    "signature": "(mode, file, args, env)",
    "description": "spawnvpe(mode, file, args, env) -> integer"
  },
  "209": {
    "name": "walk",
    "module": "torch.utils.data.distributed.dist.rpc.os",
    "fullName": "torch.utils.data.distributed.dist.rpc.os.walk",
    "signature": "(top, topdown=True, onerror=None, followlinks=False)",
    "description": "Directory tree generator."
  },
  "210": {
    "name": "abspath",
    "module": "torch.utils.data.distributed.dist.rpc.os.path",
    "fullName": "torch.utils.data.distributed.dist.rpc.os.path.abspath",
    "signature": "(path)",
    "description": "Return an absolute path."
  },
  "211": {
    "name": "basename",
    "module": "torch.utils.data.distributed.dist.rpc.os.path",
    "fullName": "torch.utils.data.distributed.dist.rpc.os.path.basename",
    "signature": "(p)",
    "description": "Returns the final component of a pathname"
  },
  "212": {
    "name": "commonpath",
    "module": "torch.utils.data.distributed.dist.rpc.os.path",
    "fullName": "torch.utils.data.distributed.dist.rpc.os.path.commonpath",
    "signature": "(paths)",
    "description": "Given a sequence of path names, returns the longest common sub-path."
  },
  "213": {
    "name": "commonprefix",
    "module": "torch.utils.data.distributed.dist.rpc.os.path",
    "fullName": "torch.utils.data.distributed.dist.rpc.os.path.commonprefix",
    "signature": "(m)",
    "description": "Given a list of pathnames, returns the longest common leading component"
  },
  "214": {
    "name": "dirname",
    "module": "torch.utils.data.distributed.dist.rpc.os.path",
    "fullName": "torch.utils.data.distributed.dist.rpc.os.path.dirname",
    "signature": "(p)",
    "description": "Returns the directory component of a pathname"
  },
  "215": {
    "name": "exists",
    "module": "torch.utils.data.distributed.dist.rpc.os.path",
    "fullName": "torch.utils.data.distributed.dist.rpc.os.path.exists",
    "signature": "(path)",
    "description": "Test whether a path exists.  Returns False for broken symbolic links"
  },
  "216": {
    "name": "expanduser",
    "module": "torch.utils.data.distributed.dist.rpc.os.path",
    "fullName": "torch.utils.data.distributed.dist.rpc.os.path.expanduser",
    "signature": "(path)",
    "description": "Expand ~ and ~user constructions.  If user or $HOME is unknown,"
  },
  "217": {
    "name": "expandvars",
    "module": "torch.utils.data.distributed.dist.rpc.os.path",
    "fullName": "torch.utils.data.distributed.dist.rpc.os.path.expandvars",
    "signature": "(path)",
    "description": "Expand shell variables of form $var and ${var}.  Unknown variables"
  },
  "218": {
    "name": "getatime",
    "module": "torch.utils.data.distributed.dist.rpc.os.path",
    "fullName": "torch.utils.data.distributed.dist.rpc.os.path.getatime",
    "signature": "(filename)",
    "description": "Return the last access time of a file, reported by os.stat()."
  },
  "219": {
    "name": "getctime",
    "module": "torch.utils.data.distributed.dist.rpc.os.path",
    "fullName": "torch.utils.data.distributed.dist.rpc.os.path.getctime",
    "signature": "(filename)",
    "description": "Return the metadata change time of a file, reported by os.stat()."
  },
  "220": {
    "name": "getmtime",
    "module": "torch.utils.data.distributed.dist.rpc.os.path",
    "fullName": "torch.utils.data.distributed.dist.rpc.os.path.getmtime",
    "signature": "(filename)",
    "description": "Return the last modification time of a file, reported by os.stat()."
  },
  "221": {
    "name": "getsize",
    "module": "torch.utils.data.distributed.dist.rpc.os.path",
    "fullName": "torch.utils.data.distributed.dist.rpc.os.path.getsize",
    "signature": "(filename)",
    "description": "Return the size of a file, reported by os.stat()."
  },
  "222": {
    "name": "isabs",
    "module": "torch.utils.data.distributed.dist.rpc.os.path",
    "fullName": "torch.utils.data.distributed.dist.rpc.os.path.isabs",
    "signature": "(s)",
    "description": "Test whether a path is absolute"
  },
  "223": {
    "name": "isdir",
    "module": "torch.utils.data.distributed.dist.rpc.os.path",
    "fullName": "torch.utils.data.distributed.dist.rpc.os.path.isdir",
    "signature": "(s)",
    "description": "Return true if the pathname refers to an existing directory."
  },
  "224": {
    "name": "isfile",
    "module": "torch.utils.data.distributed.dist.rpc.os.path",
    "fullName": "torch.utils.data.distributed.dist.rpc.os.path.isfile",
    "signature": "(path)",
    "description": "Test whether a path is a regular file"
  },
  "225": {
    "name": "islink",
    "module": "torch.utils.data.distributed.dist.rpc.os.path",
    "fullName": "torch.utils.data.distributed.dist.rpc.os.path.islink",
    "signature": "(path)",
    "description": "Test whether a path is a symbolic link"
  },
  "226": {
    "name": "ismount",
    "module": "torch.utils.data.distributed.dist.rpc.os.path",
    "fullName": "torch.utils.data.distributed.dist.rpc.os.path.ismount",
    "signature": "(path)",
    "description": "Test whether a path is a mount point"
  },
  "227": {
    "name": "join",
    "module": "torch.utils.data.distributed.dist.rpc.os.path",
    "fullName": "torch.utils.data.distributed.dist.rpc.os.path.join",
    "signature": "(a, *p)",
    "description": "Join two or more pathname components, inserting '/' as needed."
  },
  "228": {
    "name": "lexists",
    "module": "torch.utils.data.distributed.dist.rpc.os.path",
    "fullName": "torch.utils.data.distributed.dist.rpc.os.path.lexists",
    "signature": "(path)",
    "description": "Test whether a path exists.  Returns True for broken symbolic links"
  },
  "229": {
    "name": "normcase",
    "module": "torch.utils.data.distributed.dist.rpc.os.path",
    "fullName": "torch.utils.data.distributed.dist.rpc.os.path.normcase",
    "signature": "(s)",
    "description": "Normalize case of pathname.  Has no effect under Posix"
  },
  "230": {
    "name": "normpath",
    "module": "torch.utils.data.distributed.dist.rpc.os.path",
    "fullName": "torch.utils.data.distributed.dist.rpc.os.path.normpath",
    "signature": "(path)",
    "description": "Normalize path, eliminating double slashes, etc."
  },
  "231": {
    "name": "realpath",
    "module": "torch.utils.data.distributed.dist.rpc.os.path",
    "fullName": "torch.utils.data.distributed.dist.rpc.os.path.realpath",
    "signature": "(filename)",
    "description": "Return the canonical path of the specified filename, eliminating any"
  },
  "232": {
    "name": "relpath",
    "module": "torch.utils.data.distributed.dist.rpc.os.path",
    "fullName": "torch.utils.data.distributed.dist.rpc.os.path.relpath",
    "signature": "(path, start=None)",
    "description": "Return a relative version of a path"
  },
  "233": {
    "name": "samefile",
    "module": "torch.utils.data.distributed.dist.rpc.os.path",
    "fullName": "torch.utils.data.distributed.dist.rpc.os.path.samefile",
    "signature": "(f1, f2)",
    "description": "Test whether two pathnames reference the same actual file or directory"
  },
  "234": {
    "name": "sameopenfile",
    "module": "torch.utils.data.distributed.dist.rpc.os.path",
    "fullName": "torch.utils.data.distributed.dist.rpc.os.path.sameopenfile",
    "signature": "(fp1, fp2)",
    "description": "Test whether two open file objects reference the same file"
  },
  "235": {
    "name": "samestat",
    "module": "torch.utils.data.distributed.dist.rpc.os.path",
    "fullName": "torch.utils.data.distributed.dist.rpc.os.path.samestat",
    "signature": "(s1, s2)",
    "description": "Test whether two stat buffers reference the same file"
  },
  "236": {
    "name": "split",
    "module": "torch.utils.data.distributed.dist.rpc.os.path",
    "fullName": "torch.utils.data.distributed.dist.rpc.os.path.split",
    "signature": "(p)",
    "description": "Split a pathname.  Returns tuple \"(head, tail)\" where \"tail\" is"
  },
  "237": {
    "name": "splitdrive",
    "module": "torch.utils.data.distributed.dist.rpc.os.path",
    "fullName": "torch.utils.data.distributed.dist.rpc.os.path.splitdrive",
    "signature": "(p)",
    "description": "Split a pathname into drive and path. On Posix, drive is always"
  },
  "238": {
    "name": "splitext",
    "module": "torch.utils.data.distributed.dist.rpc.os.path",
    "fullName": "torch.utils.data.distributed.dist.rpc.os.path.splitext",
    "signature": "(p)",
    "description": "Split the extension from a pathname."
  },
  "239": {
    "name": "commonprefix",
    "module": "torch.utils.data.distributed.dist.rpc.os.path.genericpath",
    "fullName": "torch.utils.data.distributed.dist.rpc.os.path.genericpath.commonprefix",
    "signature": "(m)",
    "description": "Given a list of pathnames, returns the longest common leading component"
  },
  "240": {
    "name": "exists",
    "module": "torch.utils.data.distributed.dist.rpc.os.path.genericpath",
    "fullName": "torch.utils.data.distributed.dist.rpc.os.path.genericpath.exists",
    "signature": "(path)",
    "description": "Test whether a path exists.  Returns False for broken symbolic links"
  },
  "241": {
    "name": "getatime",
    "module": "torch.utils.data.distributed.dist.rpc.os.path.genericpath",
    "fullName": "torch.utils.data.distributed.dist.rpc.os.path.genericpath.getatime",
    "signature": "(filename)",
    "description": "Return the last access time of a file, reported by os.stat()."
  },
  "242": {
    "name": "getctime",
    "module": "torch.utils.data.distributed.dist.rpc.os.path.genericpath",
    "fullName": "torch.utils.data.distributed.dist.rpc.os.path.genericpath.getctime",
    "signature": "(filename)",
    "description": "Return the metadata change time of a file, reported by os.stat()."
  },
  "243": {
    "name": "getmtime",
    "module": "torch.utils.data.distributed.dist.rpc.os.path.genericpath",
    "fullName": "torch.utils.data.distributed.dist.rpc.os.path.genericpath.getmtime",
    "signature": "(filename)",
    "description": "Return the last modification time of a file, reported by os.stat()."
  },
  "244": {
    "name": "getsize",
    "module": "torch.utils.data.distributed.dist.rpc.os.path.genericpath",
    "fullName": "torch.utils.data.distributed.dist.rpc.os.path.genericpath.getsize",
    "signature": "(filename)",
    "description": "Return the size of a file, reported by os.stat()."
  },
  "245": {
    "name": "isdir",
    "module": "torch.utils.data.distributed.dist.rpc.os.path.genericpath",
    "fullName": "torch.utils.data.distributed.dist.rpc.os.path.genericpath.isdir",
    "signature": "(s)",
    "description": "Return true if the pathname refers to an existing directory."
  },
  "246": {
    "name": "isfile",
    "module": "torch.utils.data.distributed.dist.rpc.os.path.genericpath",
    "fullName": "torch.utils.data.distributed.dist.rpc.os.path.genericpath.isfile",
    "signature": "(path)",
    "description": "Test whether a path is a regular file"
  },
  "247": {
    "name": "samefile",
    "module": "torch.utils.data.distributed.dist.rpc.os.path.genericpath",
    "fullName": "torch.utils.data.distributed.dist.rpc.os.path.genericpath.samefile",
    "signature": "(f1, f2)",
    "description": "Test whether two pathnames reference the same actual file or directory"
  },
  "248": {
    "name": "sameopenfile",
    "module": "torch.utils.data.distributed.dist.rpc.os.path.genericpath",
    "fullName": "torch.utils.data.distributed.dist.rpc.os.path.genericpath.sameopenfile",
    "signature": "(fp1, fp2)",
    "description": "Test whether two open file objects reference the same file"
  },
  "249": {
    "name": "samestat",
    "module": "torch.utils.data.distributed.dist.rpc.os.path.genericpath",
    "fullName": "torch.utils.data.distributed.dist.rpc.os.path.genericpath.samestat",
    "signature": "(s1, s2)",
    "description": "Test whether two stat buffers reference the same file"
  },
  "250": {
    "name": "DeviceType",
    "module": "torch.utils.data.distributed.dist.rpc.options",
    "fullName": "torch.utils.data.distributed.dist.rpc.options.DeviceType",
    "signature": "(*args, **kwargs)",
    "description": "No description available."
  },
  "251": {
    "name": "abstractmethod",
    "module": "torch.utils.data.distributed.dist.rpc.numbers",
    "fullName": "torch.utils.data.distributed.dist.rpc.numbers.abstractmethod",
    "signature": "(funcobj)",
    "description": "A decorator indicating abstract methods."
  },
  "252": {
    "name": "addLevelName",
    "module": "torch.utils.data.distributed.dist.rpc.logging",
    "fullName": "torch.utils.data.distributed.dist.rpc.logging.addLevelName",
    "signature": "(level, levelName)",
    "description": "Associate 'levelName' with 'level'."
  },
  "253": {
    "name": "basicConfig",
    "module": "torch.utils.data.distributed.dist.rpc.logging",
    "fullName": "torch.utils.data.distributed.dist.rpc.logging.basicConfig",
    "signature": "(**kwargs)",
    "description": "Do basic configuration for the logging system."
  },
  "254": {
    "name": "captureWarnings",
    "module": "torch.utils.data.distributed.dist.rpc.logging",
    "fullName": "torch.utils.data.distributed.dist.rpc.logging.captureWarnings",
    "signature": "(capture)",
    "description": "If capture is true, redirect all warnings to the logging package."
  },
  "255": {
    "name": "critical",
    "module": "torch.utils.data.distributed.dist.rpc.logging",
    "fullName": "torch.utils.data.distributed.dist.rpc.logging.critical",
    "signature": "(msg, *args, **kwargs)",
    "description": "Log a message with severity 'CRITICAL' on the root logger. If the logger"
  },
  "256": {
    "name": "currentframe",
    "module": "torch.utils.data.distributed.dist.rpc.logging",
    "fullName": "torch.utils.data.distributed.dist.rpc.logging.currentframe",
    "signature": "()",
    "description": "No description available."
  },
  "257": {
    "name": "debug",
    "module": "torch.utils.data.distributed.dist.rpc.logging",
    "fullName": "torch.utils.data.distributed.dist.rpc.logging.debug",
    "signature": "(msg, *args, **kwargs)",
    "description": "Log a message with severity 'DEBUG' on the root logger. If the logger has"
  },
  "258": {
    "name": "disable",
    "module": "torch.utils.data.distributed.dist.rpc.logging",
    "fullName": "torch.utils.data.distributed.dist.rpc.logging.disable",
    "signature": "(level=50)",
    "description": "Disable all logging calls of severity 'level' and below."
  },
  "259": {
    "name": "error",
    "module": "torch.utils.data.distributed.dist.rpc.logging",
    "fullName": "torch.utils.data.distributed.dist.rpc.logging.error",
    "signature": "(msg, *args, **kwargs)",
    "description": "Log a message with severity 'ERROR' on the root logger. If the logger has"
  },
  "260": {
    "name": "exception",
    "module": "torch.utils.data.distributed.dist.rpc.logging",
    "fullName": "torch.utils.data.distributed.dist.rpc.logging.exception",
    "signature": "(msg, *args, exc_info=True, **kwargs)",
    "description": "Log a message with severity 'ERROR' on the root logger, with exception"
  },
  "261": {
    "name": "fatal",
    "module": "torch.utils.data.distributed.dist.rpc.logging",
    "fullName": "torch.utils.data.distributed.dist.rpc.logging.fatal",
    "signature": "(msg, *args, **kwargs)",
    "description": "Log a message with severity 'CRITICAL' on the root logger. If the logger"
  },
  "262": {
    "name": "getLevelName",
    "module": "torch.utils.data.distributed.dist.rpc.logging",
    "fullName": "torch.utils.data.distributed.dist.rpc.logging.getLevelName",
    "signature": "(level)",
    "description": "Return the textual or numeric representation of logging level 'level'."
  },
  "263": {
    "name": "getLogRecordFactory",
    "module": "torch.utils.data.distributed.dist.rpc.logging",
    "fullName": "torch.utils.data.distributed.dist.rpc.logging.getLogRecordFactory",
    "signature": "()",
    "description": "Return the factory to be used when instantiating a log record."
  },
  "264": {
    "name": "getLogger",
    "module": "torch.utils.data.distributed.dist.rpc.logging",
    "fullName": "torch.utils.data.distributed.dist.rpc.logging.getLogger",
    "signature": "(name=None)",
    "description": "Return a logger with the specified name, creating it if necessary."
  },
  "265": {
    "name": "getLoggerClass",
    "module": "torch.utils.data.distributed.dist.rpc.logging",
    "fullName": "torch.utils.data.distributed.dist.rpc.logging.getLoggerClass",
    "signature": "()",
    "description": "Return the class to be used when instantiating a logger."
  },
  "266": {
    "name": "info",
    "module": "torch.utils.data.distributed.dist.rpc.logging",
    "fullName": "torch.utils.data.distributed.dist.rpc.logging.info",
    "signature": "(msg, *args, **kwargs)",
    "description": "Log a message with severity 'INFO' on the root logger. If the logger has"
  },
  "267": {
    "name": "log",
    "module": "torch.utils.data.distributed.dist.rpc.logging",
    "fullName": "torch.utils.data.distributed.dist.rpc.logging.log",
    "signature": "(level, msg, *args, **kwargs)",
    "description": "Log 'msg % args' with the integer severity 'level' on the root logger. If"
  },
  "268": {
    "name": "makeLogRecord",
    "module": "torch.utils.data.distributed.dist.rpc.logging",
    "fullName": "torch.utils.data.distributed.dist.rpc.logging.makeLogRecord",
    "signature": "(dict)",
    "description": "Make a LogRecord whose attributes are defined by the specified dictionary,"
  },
  "269": {
    "name": "setLogRecordFactory",
    "module": "torch.utils.data.distributed.dist.rpc.logging",
    "fullName": "torch.utils.data.distributed.dist.rpc.logging.setLogRecordFactory",
    "signature": "(factory)",
    "description": "Set the factory to be used when instantiating a log record."
  },
  "270": {
    "name": "setLoggerClass",
    "module": "torch.utils.data.distributed.dist.rpc.logging",
    "fullName": "torch.utils.data.distributed.dist.rpc.logging.setLoggerClass",
    "signature": "(klass)",
    "description": "Set the class to be used when instantiating a logger. The class should"
  },
  "271": {
    "name": "shutdown",
    "module": "torch.utils.data.distributed.dist.rpc.logging",
    "fullName": "torch.utils.data.distributed.dist.rpc.logging.shutdown",
    "signature": "(handlerList=[<weakref at 0x7fce1ad01a40; to '_StderrHandler' at 0x7fce1ad7e880>])",
    "description": "Perform any cleanup actions in the logging system (e.g. flushing"
  },
  "272": {
    "name": "warn",
    "module": "torch.utils.data.distributed.dist.rpc.logging",
    "fullName": "torch.utils.data.distributed.dist.rpc.logging.warn",
    "signature": "(msg, *args, **kwargs)",
    "description": "No description available."
  },
  "273": {
    "name": "warning",
    "module": "torch.utils.data.distributed.dist.rpc.logging",
    "fullName": "torch.utils.data.distributed.dist.rpc.logging.warning",
    "signature": "(msg, *args, **kwargs)",
    "description": "Log a message with severity 'WARNING' on the root logger. If the logger has"
  },
  "274": {
    "name": "clear_frames",
    "module": "torch.utils.data.distributed.dist.rpc.logging.traceback",
    "fullName": "torch.utils.data.distributed.dist.rpc.logging.traceback.clear_frames",
    "signature": "(tb)",
    "description": "Clear all references to local variables in the frames of a traceback."
  },
  "275": {
    "name": "extract_stack",
    "module": "torch.utils.data.distributed.dist.rpc.logging.traceback",
    "fullName": "torch.utils.data.distributed.dist.rpc.logging.traceback.extract_stack",
    "signature": "(f=None, limit=None)",
    "description": "Extract the raw traceback from the current stack frame."
  },
  "276": {
    "name": "extract_tb",
    "module": "torch.utils.data.distributed.dist.rpc.logging.traceback",
    "fullName": "torch.utils.data.distributed.dist.rpc.logging.traceback.extract_tb",
    "signature": "(tb, limit=None)",
    "description": "Return a StackSummary object representing a list of"
  },
  "277": {
    "name": "format_exc",
    "module": "torch.utils.data.distributed.dist.rpc.logging.traceback",
    "fullName": "torch.utils.data.distributed.dist.rpc.logging.traceback.format_exc",
    "signature": "(limit=None, chain=True)",
    "description": "Like print_exc() but return a string."
  },
  "278": {
    "name": "format_exception",
    "module": "torch.utils.data.distributed.dist.rpc.logging.traceback",
    "fullName": "torch.utils.data.distributed.dist.rpc.logging.traceback.format_exception",
    "signature": "(etype, value, tb, limit=None, chain=True)",
    "description": "Format a stack trace and the exception information."
  },
  "279": {
    "name": "format_exception_only",
    "module": "torch.utils.data.distributed.dist.rpc.logging.traceback",
    "fullName": "torch.utils.data.distributed.dist.rpc.logging.traceback.format_exception_only",
    "signature": "(etype, value)",
    "description": "Format the exception part of a traceback."
  },
  "280": {
    "name": "format_list",
    "module": "torch.utils.data.distributed.dist.rpc.logging.traceback",
    "fullName": "torch.utils.data.distributed.dist.rpc.logging.traceback.format_list",
    "signature": "(extracted_list)",
    "description": "Format a list of tuples or FrameSummary objects for printing."
  },
  "281": {
    "name": "format_stack",
    "module": "torch.utils.data.distributed.dist.rpc.logging.traceback",
    "fullName": "torch.utils.data.distributed.dist.rpc.logging.traceback.format_stack",
    "signature": "(f=None, limit=None)",
    "description": "Shorthand for 'format_list(extract_stack(f, limit))'."
  },
  "282": {
    "name": "format_tb",
    "module": "torch.utils.data.distributed.dist.rpc.logging.traceback",
    "fullName": "torch.utils.data.distributed.dist.rpc.logging.traceback.format_tb",
    "signature": "(tb, limit=None)",
    "description": "A shorthand for 'format_list(extract_tb(tb, limit))'."
  },
  "283": {
    "name": "print_exc",
    "module": "torch.utils.data.distributed.dist.rpc.logging.traceback",
    "fullName": "torch.utils.data.distributed.dist.rpc.logging.traceback.print_exc",
    "signature": "(limit=None, file=None, chain=True)",
    "description": "Shorthand for 'print_exception(*sys.exc_info(), limit, file)'."
  },
  "284": {
    "name": "print_exception",
    "module": "torch.utils.data.distributed.dist.rpc.logging.traceback",
    "fullName": "torch.utils.data.distributed.dist.rpc.logging.traceback.print_exception",
    "signature": "(etype, value, tb, limit=None, file=None, chain=True)",
    "description": "Print exception up to 'limit' stack trace entries from 'tb' to 'file'."
  },
  "285": {
    "name": "print_last",
    "module": "torch.utils.data.distributed.dist.rpc.logging.traceback",
    "fullName": "torch.utils.data.distributed.dist.rpc.logging.traceback.print_last",
    "signature": "(limit=None, file=None, chain=True)",
    "description": "This is a shorthand for 'print_exception(sys.last_type,"
  },
  "286": {
    "name": "print_list",
    "module": "torch.utils.data.distributed.dist.rpc.logging.traceback",
    "fullName": "torch.utils.data.distributed.dist.rpc.logging.traceback.print_list",
    "signature": "(extracted_list, file=None)",
    "description": "Print the list of tuples as returned by extract_tb() or"
  },
  "287": {
    "name": "print_stack",
    "module": "torch.utils.data.distributed.dist.rpc.logging.traceback",
    "fullName": "torch.utils.data.distributed.dist.rpc.logging.traceback.print_stack",
    "signature": "(f=None, limit=None, file=None)",
    "description": "Print a stack trace from its invocation point."
  },
  "288": {
    "name": "print_tb",
    "module": "torch.utils.data.distributed.dist.rpc.logging.traceback",
    "fullName": "torch.utils.data.distributed.dist.rpc.logging.traceback.print_tb",
    "signature": "(tb, limit=None, file=None)",
    "description": "Print up to 'limit' stack trace entries from the traceback 'tb'."
  },
  "289": {
    "name": "walk_stack",
    "module": "torch.utils.data.distributed.dist.rpc.logging.traceback",
    "fullName": "torch.utils.data.distributed.dist.rpc.logging.traceback.walk_stack",
    "signature": "(f)",
    "description": "Walk a stack yielding the frame and line number for each frame."
  },
  "290": {
    "name": "walk_tb",
    "module": "torch.utils.data.distributed.dist.rpc.logging.traceback",
    "fullName": "torch.utils.data.distributed.dist.rpc.logging.traceback.walk_tb",
    "signature": "(tb)",
    "description": "Walk a traceback yielding the frame and line number for each frame."
  },
  "291": {
    "name": "checkcache",
    "module": "torch.utils.data.distributed.dist.rpc.logging.traceback.linecache",
    "fullName": "torch.utils.data.distributed.dist.rpc.logging.traceback.linecache.checkcache",
    "signature": "(filename=None)",
    "description": "Discard cache entries that are out of date."
  },
  "292": {
    "name": "clearcache",
    "module": "torch.utils.data.distributed.dist.rpc.logging.traceback.linecache",
    "fullName": "torch.utils.data.distributed.dist.rpc.logging.traceback.linecache.clearcache",
    "signature": "()",
    "description": "Clear the cache entirely."
  },
  "293": {
    "name": "getline",
    "module": "torch.utils.data.distributed.dist.rpc.logging.traceback.linecache",
    "fullName": "torch.utils.data.distributed.dist.rpc.logging.traceback.linecache.getline",
    "signature": "(filename, lineno, module_globals=None)",
    "description": "Get a line for a Python source file from the cache."
  },
  "294": {
    "name": "getlines",
    "module": "torch.utils.data.distributed.dist.rpc.logging.traceback.linecache",
    "fullName": "torch.utils.data.distributed.dist.rpc.logging.traceback.linecache.getlines",
    "signature": "(filename, module_globals=None)",
    "description": "Get the lines for a Python source file from the cache."
  },
  "295": {
    "name": "lazycache",
    "module": "torch.utils.data.distributed.dist.rpc.logging.traceback.linecache",
    "fullName": "torch.utils.data.distributed.dist.rpc.logging.traceback.linecache.lazycache",
    "signature": "(filename, module_globals)",
    "description": "Seed the cache for filename with module_globals."
  },
  "296": {
    "name": "updatecache",
    "module": "torch.utils.data.distributed.dist.rpc.logging.traceback.linecache",
    "fullName": "torch.utils.data.distributed.dist.rpc.logging.traceback.linecache.updatecache",
    "signature": "(filename, module_globals=None)",
    "description": "Update a cache entry and return its list of lines."
  },
  "297": {
    "name": "ISEOF",
    "module": "torch.utils.data.distributed.dist.rpc.logging.traceback.linecache.tokenize",
    "fullName": "torch.utils.data.distributed.dist.rpc.logging.traceback.linecache.tokenize.ISEOF",
    "signature": "(x)",
    "description": "No description available."
  },
  "298": {
    "name": "ISNONTERMINAL",
    "module": "torch.utils.data.distributed.dist.rpc.logging.traceback.linecache.tokenize",
    "fullName": "torch.utils.data.distributed.dist.rpc.logging.traceback.linecache.tokenize.ISNONTERMINAL",
    "signature": "(x)",
    "description": "No description available."
  },
  "299": {
    "name": "ISTERMINAL",
    "module": "torch.utils.data.distributed.dist.rpc.logging.traceback.linecache.tokenize",
    "fullName": "torch.utils.data.distributed.dist.rpc.logging.traceback.linecache.tokenize.ISTERMINAL",
    "signature": "(x)",
    "description": "No description available."
  },
  "300": {
    "name": "any",
    "module": "torch.utils.data.distributed.dist.rpc.logging.traceback.linecache.tokenize",
    "fullName": "torch.utils.data.distributed.dist.rpc.logging.traceback.linecache.tokenize.any",
    "signature": "(*choices)",
    "description": "No description available."
  },
  "301": {
    "name": "detect_encoding",
    "module": "torch.utils.data.distributed.dist.rpc.logging.traceback.linecache.tokenize",
    "fullName": "torch.utils.data.distributed.dist.rpc.logging.traceback.linecache.tokenize.detect_encoding",
    "signature": "(readline)",
    "description": "The detect_encoding() function is used to detect the encoding that should"
  },
  "302": {
    "name": "generate_tokens",
    "module": "torch.utils.data.distributed.dist.rpc.logging.traceback.linecache.tokenize",
    "fullName": "torch.utils.data.distributed.dist.rpc.logging.traceback.linecache.tokenize.generate_tokens",
    "signature": "(readline)",
    "description": "Tokenize a source reading Python code as unicode strings."
  },
  "303": {
    "name": "group",
    "module": "torch.utils.data.distributed.dist.rpc.logging.traceback.linecache.tokenize",
    "fullName": "torch.utils.data.distributed.dist.rpc.logging.traceback.linecache.tokenize.group",
    "signature": "(*choices)",
    "description": "No description available."
  },
  "304": {
    "name": "main",
    "module": "torch.utils.data.distributed.dist.rpc.logging.traceback.linecache.tokenize",
    "fullName": "torch.utils.data.distributed.dist.rpc.logging.traceback.linecache.tokenize.main",
    "signature": "()",
    "description": "No description available."
  },
  "305": {
    "name": "maybe",
    "module": "torch.utils.data.distributed.dist.rpc.logging.traceback.linecache.tokenize",
    "fullName": "torch.utils.data.distributed.dist.rpc.logging.traceback.linecache.tokenize.maybe",
    "signature": "(*choices)",
    "description": "No description available."
  },
  "306": {
    "name": "open",
    "module": "torch.utils.data.distributed.dist.rpc.logging.traceback.linecache.tokenize",
    "fullName": "torch.utils.data.distributed.dist.rpc.logging.traceback.linecache.tokenize.open",
    "signature": "(filename)",
    "description": "Open a file in read only mode using the encoding detected by"
  },
  "307": {
    "name": "tokenize",
    "module": "torch.utils.data.distributed.dist.rpc.logging.traceback.linecache.tokenize",
    "fullName": "torch.utils.data.distributed.dist.rpc.logging.traceback.linecache.tokenize.tokenize",
    "signature": "(readline)",
    "description": "The tokenize() generator requires one argument, readline, which"
  },
  "308": {
    "name": "untokenize",
    "module": "torch.utils.data.distributed.dist.rpc.logging.traceback.linecache.tokenize",
    "fullName": "torch.utils.data.distributed.dist.rpc.logging.traceback.linecache.tokenize.untokenize",
    "signature": "(iterable)",
    "description": "Transform tokens back into Python source code."
  },
  "309": {
    "name": "deserialize",
    "module": "torch.utils.data.distributed.dist.rpc.internal",
    "fullName": "torch.utils.data.distributed.dist.rpc.internal.deserialize",
    "signature": "(binary_data, tensor_table)",
    "description": "No description available."
  },
  "310": {
    "name": "serialize",
    "module": "torch.utils.data.distributed.dist.rpc.internal",
    "fullName": "torch.utils.data.distributed.dist.rpc.internal.serialize",
    "signature": "(obj)",
    "description": "No description available."
  },
  "311": {
    "name": "async_execution",
    "module": "torch.utils.data.distributed.dist.rpc.functions",
    "fullName": "torch.utils.data.distributed.dist.rpc.functions.async_execution",
    "signature": "(fn)",
    "description": "A decorator for a function indicating that the return value of the function"
  },
  "312": {
    "name": "is_available",
    "module": "torch.utils.data.distributed.dist.rpc.dist_autograd",
    "fullName": "torch.utils.data.distributed.dist.rpc.dist_autograd.is_available",
    "signature": "()",
    "description": "No description available."
  },
  "313": {
    "name": "backend_registered",
    "module": "torch.utils.data.distributed.dist.rpc.backend_registry",
    "fullName": "torch.utils.data.distributed.dist.rpc.backend_registry.backend_registered",
    "signature": "(backend_name)",
    "description": "Checks if backend_name is registered as an RPC backend."
  },
  "314": {
    "name": "cast",
    "module": "torch.utils.data.distributed.dist.rpc.backend_registry",
    "fullName": "torch.utils.data.distributed.dist.rpc.backend_registry.cast",
    "signature": "(typ, val)",
    "description": "Cast a value to a type."
  },
  "315": {
    "name": "construct_rpc_backend_options",
    "module": "torch.utils.data.distributed.dist.rpc.backend_registry",
    "fullName": "torch.utils.data.distributed.dist.rpc.backend_registry.construct_rpc_backend_options",
    "signature": "(backend, rpc_timeout=60.0, init_method='env://', **kwargs)",
    "description": "No description available."
  },
  "316": {
    "name": "init_backend",
    "module": "torch.utils.data.distributed.dist.rpc.backend_registry",
    "fullName": "torch.utils.data.distributed.dist.rpc.backend_registry.init_backend",
    "signature": "(backend, *args, **kwargs)",
    "description": "No description available."
  },
  "317": {
    "name": "register_backend",
    "module": "torch.utils.data.distributed.dist.rpc.backend_registry",
    "fullName": "torch.utils.data.distributed.dist.rpc.backend_registry.register_backend",
    "signature": "(backend_name, construct_rpc_backend_options_handler, init_backend_handler)",
    "description": "Registers a new RPC backend."
  },
  "318": {
    "name": "get_worker_info",
    "module": "torch.utils.data.distributed.dist.rpc.backend_registry.api",
    "fullName": "torch.utils.data.distributed.dist.rpc.backend_registry.api.get_worker_info",
    "signature": "(worker_name=None)",
    "description": "Get :class:`~torch.distributed.rpc.WorkerInfo` of a given worker name."
  },
  "319": {
    "name": "method_factory",
    "module": "torch.utils.data.distributed.dist.rpc.backend_registry.api",
    "fullName": "torch.utils.data.distributed.dist.rpc.backend_registry.api.method_factory",
    "signature": "(method_name, docstring)",
    "description": "No description available."
  },
  "320": {
    "name": "new_method",
    "module": "torch.utils.data.distributed.dist.rpc.backend_registry.api",
    "fullName": "torch.utils.data.distributed.dist.rpc.backend_registry.api.new_method",
    "signature": "(self, *args, **kwargs)",
    "description": "No description available."
  },
  "321": {
    "name": "remote",
    "module": "torch.utils.data.distributed.dist.rpc.backend_registry.api",
    "fullName": "torch.utils.data.distributed.dist.rpc.backend_registry.api.remote",
    "signature": "(to, func, args=None, kwargs=None, timeout=-1.0)",
    "description": "Make a remote call to run ``func`` on worker ``to`` and return an"
  },
  "322": {
    "name": "rpc_async",
    "module": "torch.utils.data.distributed.dist.rpc.backend_registry.api",
    "fullName": "torch.utils.data.distributed.dist.rpc.backend_registry.api.rpc_async",
    "signature": "(to, func, args=None, kwargs=None, timeout=-1.0)",
    "description": "Make a non-blocking RPC call to run function ``func`` on worker ``to``. RPC"
  },
  "323": {
    "name": "rpc_sync",
    "module": "torch.utils.data.distributed.dist.rpc.backend_registry.api",
    "fullName": "torch.utils.data.distributed.dist.rpc.backend_registry.api.rpc_sync",
    "signature": "(to, func, args=None, kwargs=None, timeout=-1.0)",
    "description": "Make a blocking RPC call to run function ``func`` on worker ``to``. RPC"
  },
  "324": {
    "name": "shutdown",
    "module": "torch.utils.data.distributed.dist.rpc.backend_registry.api",
    "fullName": "torch.utils.data.distributed.dist.rpc.backend_registry.api.shutdown",
    "signature": "(graceful=True, timeout=0)",
    "description": "Perform a shutdown of the RPC agent, and then destroy the RPC agent. This"
  },
  "325": {
    "name": "classify_class_attrs",
    "module": "torch.utils.data.distributed.dist.rpc.backend_registry.api.inspect",
    "fullName": "torch.utils.data.distributed.dist.rpc.backend_registry.api.inspect.classify_class_attrs",
    "signature": "(cls)",
    "description": "Return list of attribute-descriptor tuples."
  },
  "326": {
    "name": "cleandoc",
    "module": "torch.utils.data.distributed.dist.rpc.backend_registry.api.inspect",
    "fullName": "torch.utils.data.distributed.dist.rpc.backend_registry.api.inspect.cleandoc",
    "signature": "(doc)",
    "description": "Clean up indentation from docstrings."
  },
  "327": {
    "name": "currentframe",
    "module": "torch.utils.data.distributed.dist.rpc.backend_registry.api.inspect",
    "fullName": "torch.utils.data.distributed.dist.rpc.backend_registry.api.inspect.currentframe",
    "signature": "()",
    "description": "Return the frame of the caller or None if this is not possible."
  },
  "328": {
    "name": "findsource",
    "module": "torch.utils.data.distributed.dist.rpc.backend_registry.api.inspect",
    "fullName": "torch.utils.data.distributed.dist.rpc.backend_registry.api.inspect.findsource",
    "signature": "(object)",
    "description": "Return the entire source file and starting line number for an object."
  },
  "329": {
    "name": "formatannotation",
    "module": "torch.utils.data.distributed.dist.rpc.backend_registry.api.inspect",
    "fullName": "torch.utils.data.distributed.dist.rpc.backend_registry.api.inspect.formatannotation",
    "signature": "(annotation, base_module=None)",
    "description": "No description available."
  },
  "330": {
    "name": "formatannotationrelativeto",
    "module": "torch.utils.data.distributed.dist.rpc.backend_registry.api.inspect",
    "fullName": "torch.utils.data.distributed.dist.rpc.backend_registry.api.inspect.formatannotationrelativeto",
    "signature": "(object)",
    "description": "No description available."
  },
  "331": {
    "name": "formatargvalues",
    "module": "torch.utils.data.distributed.dist.rpc.backend_registry.api.inspect",
    "fullName": "torch.utils.data.distributed.dist.rpc.backend_registry.api.inspect.formatargvalues",
    "signature": "(args, varargs, varkw, locals, formatarg=<class 'str'>, formatvarargs=<function <lambda> at 0x7fce238da820>, formatvarkw=<function <lambda> at 0x7fce238da8b0>, formatvalue=<function <lambda> at 0x7fce238da940>)",
    "description": "Format an argument spec from the 4 values returned by getargvalues."
  },
  "332": {
    "name": "getabsfile",
    "module": "torch.utils.data.distributed.dist.rpc.backend_registry.api.inspect",
    "fullName": "torch.utils.data.distributed.dist.rpc.backend_registry.api.inspect.getabsfile",
    "signature": "(object, _filename=None)",
    "description": "Return an absolute path to the source or compiled file for an object."
  },
  "333": {
    "name": "getargs",
    "module": "torch.utils.data.distributed.dist.rpc.backend_registry.api.inspect",
    "fullName": "torch.utils.data.distributed.dist.rpc.backend_registry.api.inspect.getargs",
    "signature": "(co)",
    "description": "Get information about the arguments accepted by a code object."
  },
  "334": {
    "name": "getargvalues",
    "module": "torch.utils.data.distributed.dist.rpc.backend_registry.api.inspect",
    "fullName": "torch.utils.data.distributed.dist.rpc.backend_registry.api.inspect.getargvalues",
    "signature": "(frame)",
    "description": "Get information about arguments passed into a particular frame."
  },
  "335": {
    "name": "getattr_static",
    "module": "torch.utils.data.distributed.dist.rpc.backend_registry.api.inspect",
    "fullName": "torch.utils.data.distributed.dist.rpc.backend_registry.api.inspect.getattr_static",
    "signature": "(obj, attr, default=<object object at 0x7fce23cd5ba0>)",
    "description": "Retrieve attributes without triggering dynamic lookup via the"
  },
  "336": {
    "name": "getblock",
    "module": "torch.utils.data.distributed.dist.rpc.backend_registry.api.inspect",
    "fullName": "torch.utils.data.distributed.dist.rpc.backend_registry.api.inspect.getblock",
    "signature": "(lines)",
    "description": "Extract the block of code at the top of the given list of lines."
  },
  "337": {
    "name": "getcallargs",
    "module": "torch.utils.data.distributed.dist.rpc.backend_registry.api.inspect",
    "fullName": "torch.utils.data.distributed.dist.rpc.backend_registry.api.inspect.getcallargs",
    "signature": "(func, /, *positional, **named)",
    "description": "Get the mapping of arguments to values."
  },
  "338": {
    "name": "getclasstree",
    "module": "torch.utils.data.distributed.dist.rpc.backend_registry.api.inspect",
    "fullName": "torch.utils.data.distributed.dist.rpc.backend_registry.api.inspect.getclasstree",
    "signature": "(classes, unique=False)",
    "description": "Arrange the given list of classes into a hierarchy of nested lists."
  },
  "339": {
    "name": "getclosurevars",
    "module": "torch.utils.data.distributed.dist.rpc.backend_registry.api.inspect",
    "fullName": "torch.utils.data.distributed.dist.rpc.backend_registry.api.inspect.getclosurevars",
    "signature": "(func)",
    "description": "Get the mapping of free variables to their current values."
  },
  "340": {
    "name": "getcomments",
    "module": "torch.utils.data.distributed.dist.rpc.backend_registry.api.inspect",
    "fullName": "torch.utils.data.distributed.dist.rpc.backend_registry.api.inspect.getcomments",
    "signature": "(object)",
    "description": "Get lines of comments immediately preceding an object's source code."
  },
  "341": {
    "name": "getcoroutinelocals",
    "module": "torch.utils.data.distributed.dist.rpc.backend_registry.api.inspect",
    "fullName": "torch.utils.data.distributed.dist.rpc.backend_registry.api.inspect.getcoroutinelocals",
    "signature": "(coroutine)",
    "description": "Get the mapping of coroutine local variables to their current values."
  },
  "342": {
    "name": "getcoroutinestate",
    "module": "torch.utils.data.distributed.dist.rpc.backend_registry.api.inspect",
    "fullName": "torch.utils.data.distributed.dist.rpc.backend_registry.api.inspect.getcoroutinestate",
    "signature": "(coroutine)",
    "description": "Get current state of a coroutine object."
  },
  "343": {
    "name": "getdoc",
    "module": "torch.utils.data.distributed.dist.rpc.backend_registry.api.inspect",
    "fullName": "torch.utils.data.distributed.dist.rpc.backend_registry.api.inspect.getdoc",
    "signature": "(object)",
    "description": "Get the documentation string for an object."
  },
  "344": {
    "name": "getfile",
    "module": "torch.utils.data.distributed.dist.rpc.backend_registry.api.inspect",
    "fullName": "torch.utils.data.distributed.dist.rpc.backend_registry.api.inspect.getfile",
    "signature": "(object)",
    "description": "No description available."
  },
  "345": {
    "name": "getframeinfo",
    "module": "torch.utils.data.distributed.dist.rpc.backend_registry.api.inspect",
    "fullName": "torch.utils.data.distributed.dist.rpc.backend_registry.api.inspect.getframeinfo",
    "signature": "(frame, context=1)",
    "description": "Get information about a frame or traceback object."
  },
  "346": {
    "name": "getfullargspec",
    "module": "torch.utils.data.distributed.dist.rpc.backend_registry.api.inspect",
    "fullName": "torch.utils.data.distributed.dist.rpc.backend_registry.api.inspect.getfullargspec",
    "signature": "(func)",
    "description": "Get the names and default values of a callable object's parameters."
  },
  "347": {
    "name": "getgeneratorlocals",
    "module": "torch.utils.data.distributed.dist.rpc.backend_registry.api.inspect",
    "fullName": "torch.utils.data.distributed.dist.rpc.backend_registry.api.inspect.getgeneratorlocals",
    "signature": "(generator)",
    "description": "Get the mapping of generator local variables to their current values."
  },
  "348": {
    "name": "getgeneratorstate",
    "module": "torch.utils.data.distributed.dist.rpc.backend_registry.api.inspect",
    "fullName": "torch.utils.data.distributed.dist.rpc.backend_registry.api.inspect.getgeneratorstate",
    "signature": "(generator)",
    "description": "Get current state of a generator-iterator."
  },
  "349": {
    "name": "getinnerframes",
    "module": "torch.utils.data.distributed.dist.rpc.backend_registry.api.inspect",
    "fullName": "torch.utils.data.distributed.dist.rpc.backend_registry.api.inspect.getinnerframes",
    "signature": "(tb, context=1)",
    "description": "Get a list of records for a traceback's frame and all lower frames."
  },
  "350": {
    "name": "getlineno",
    "module": "torch.utils.data.distributed.dist.rpc.backend_registry.api.inspect",
    "fullName": "torch.utils.data.distributed.dist.rpc.backend_registry.api.inspect.getlineno",
    "signature": "(frame)",
    "description": "Get the line number from a frame object, allowing for optimization."
  },
  "351": {
    "name": "getmembers",
    "module": "torch.utils.data.distributed.dist.rpc.backend_registry.api.inspect",
    "fullName": "torch.utils.data.distributed.dist.rpc.backend_registry.api.inspect.getmembers",
    "signature": "(object, predicate=None)",
    "description": "Return all members of an object as (name, value) pairs sorted by name."
  },
  "352": {
    "name": "getmodule",
    "module": "torch.utils.data.distributed.dist.rpc.backend_registry.api.inspect",
    "fullName": "torch.utils.data.distributed.dist.rpc.backend_registry.api.inspect.getmodule",
    "signature": "(object, _filename=None)",
    "description": "Return the module an object was defined in, or None if not found."
  },
  "353": {
    "name": "getmodulename",
    "module": "torch.utils.data.distributed.dist.rpc.backend_registry.api.inspect",
    "fullName": "torch.utils.data.distributed.dist.rpc.backend_registry.api.inspect.getmodulename",
    "signature": "(path)",
    "description": "Return the module name for a given file, or None."
  },
  "354": {
    "name": "getmro",
    "module": "torch.utils.data.distributed.dist.rpc.backend_registry.api.inspect",
    "fullName": "torch.utils.data.distributed.dist.rpc.backend_registry.api.inspect.getmro",
    "signature": "(cls)",
    "description": "Return tuple of base classes (including cls) in method resolution order."
  },
  "355": {
    "name": "getouterframes",
    "module": "torch.utils.data.distributed.dist.rpc.backend_registry.api.inspect",
    "fullName": "torch.utils.data.distributed.dist.rpc.backend_registry.api.inspect.getouterframes",
    "signature": "(frame, context=1)",
    "description": "Get a list of records for a frame and all higher (calling) frames."
  },
  "356": {
    "name": "getsource",
    "module": "torch.utils.data.distributed.dist.rpc.backend_registry.api.inspect",
    "fullName": "torch.utils.data.distributed.dist.rpc.backend_registry.api.inspect.getsource",
    "signature": "(object)",
    "description": "Return the text of the source code for an object."
  },
  "357": {
    "name": "getsourcefile",
    "module": "torch.utils.data.distributed.dist.rpc.backend_registry.api.inspect",
    "fullName": "torch.utils.data.distributed.dist.rpc.backend_registry.api.inspect.getsourcefile",
    "signature": "(object)",
    "description": "Return the filename that can be used to locate an object's source."
  },
  "358": {
    "name": "getsourcelines",
    "module": "torch.utils.data.distributed.dist.rpc.backend_registry.api.inspect",
    "fullName": "torch.utils.data.distributed.dist.rpc.backend_registry.api.inspect.getsourcelines",
    "signature": "(object)",
    "description": "Return a list of source lines and starting line number for an object."
  },
  "359": {
    "name": "indentsize",
    "module": "torch.utils.data.distributed.dist.rpc.backend_registry.api.inspect",
    "fullName": "torch.utils.data.distributed.dist.rpc.backend_registry.api.inspect.indentsize",
    "signature": "(line)",
    "description": "Return the indent size, in spaces, at the start of a line of text."
  },
  "360": {
    "name": "isabstract",
    "module": "torch.utils.data.distributed.dist.rpc.backend_registry.api.inspect",
    "fullName": "torch.utils.data.distributed.dist.rpc.backend_registry.api.inspect.isabstract",
    "signature": "(object)",
    "description": "Return true if the object is an abstract base class (ABC)."
  },
  "361": {
    "name": "isasyncgen",
    "module": "torch.utils.data.distributed.dist.rpc.backend_registry.api.inspect",
    "fullName": "torch.utils.data.distributed.dist.rpc.backend_registry.api.inspect.isasyncgen",
    "signature": "(object)",
    "description": "Return true if the object is an asynchronous generator."
  },
  "362": {
    "name": "isasyncgenfunction",
    "module": "torch.utils.data.distributed.dist.rpc.backend_registry.api.inspect",
    "fullName": "torch.utils.data.distributed.dist.rpc.backend_registry.api.inspect.isasyncgenfunction",
    "signature": "(obj)",
    "description": "Return true if the object is an asynchronous generator function."
  },
  "363": {
    "name": "isawaitable",
    "module": "torch.utils.data.distributed.dist.rpc.backend_registry.api.inspect",
    "fullName": "torch.utils.data.distributed.dist.rpc.backend_registry.api.inspect.isawaitable",
    "signature": "(object)",
    "description": "Return true if object can be passed to an ``await`` expression."
  },
  "364": {
    "name": "isbuiltin",
    "module": "torch.utils.data.distributed.dist.rpc.backend_registry.api.inspect",
    "fullName": "torch.utils.data.distributed.dist.rpc.backend_registry.api.inspect.isbuiltin",
    "signature": "(object)",
    "description": "Return true if the object is a built-in function or method."
  },
  "365": {
    "name": "isclass",
    "module": "torch.utils.data.distributed.dist.rpc.backend_registry.api.inspect",
    "fullName": "torch.utils.data.distributed.dist.rpc.backend_registry.api.inspect.isclass",
    "signature": "(object)",
    "description": "Return true if the object is a class."
  },
  "366": {
    "name": "iscode",
    "module": "torch.utils.data.distributed.dist.rpc.backend_registry.api.inspect",
    "fullName": "torch.utils.data.distributed.dist.rpc.backend_registry.api.inspect.iscode",
    "signature": "(object)",
    "description": "Return true if the object is a code object."
  },
  "367": {
    "name": "iscoroutine",
    "module": "torch.utils.data.distributed.dist.rpc.backend_registry.api.inspect",
    "fullName": "torch.utils.data.distributed.dist.rpc.backend_registry.api.inspect.iscoroutine",
    "signature": "(object)",
    "description": "Return true if the object is a coroutine."
  },
  "368": {
    "name": "iscoroutinefunction",
    "module": "torch.utils.data.distributed.dist.rpc.backend_registry.api.inspect",
    "fullName": "torch.utils.data.distributed.dist.rpc.backend_registry.api.inspect.iscoroutinefunction",
    "signature": "(obj)",
    "description": "Return true if the object is a coroutine function."
  },
  "369": {
    "name": "isdatadescriptor",
    "module": "torch.utils.data.distributed.dist.rpc.backend_registry.api.inspect",
    "fullName": "torch.utils.data.distributed.dist.rpc.backend_registry.api.inspect.isdatadescriptor",
    "signature": "(object)",
    "description": "Return true if the object is a data descriptor."
  },
  "370": {
    "name": "isframe",
    "module": "torch.utils.data.distributed.dist.rpc.backend_registry.api.inspect",
    "fullName": "torch.utils.data.distributed.dist.rpc.backend_registry.api.inspect.isframe",
    "signature": "(object)",
    "description": "Return true if the object is a frame object."
  },
  "371": {
    "name": "isfunction",
    "module": "torch.utils.data.distributed.dist.rpc.backend_registry.api.inspect",
    "fullName": "torch.utils.data.distributed.dist.rpc.backend_registry.api.inspect.isfunction",
    "signature": "(object)",
    "description": "Return true if the object is a user-defined function."
  },
  "372": {
    "name": "isgenerator",
    "module": "torch.utils.data.distributed.dist.rpc.backend_registry.api.inspect",
    "fullName": "torch.utils.data.distributed.dist.rpc.backend_registry.api.inspect.isgenerator",
    "signature": "(object)",
    "description": "Return true if the object is a generator."
  },
  "373": {
    "name": "isgeneratorfunction",
    "module": "torch.utils.data.distributed.dist.rpc.backend_registry.api.inspect",
    "fullName": "torch.utils.data.distributed.dist.rpc.backend_registry.api.inspect.isgeneratorfunction",
    "signature": "(obj)",
    "description": "Return true if the object is a user-defined generator function."
  },
  "374": {
    "name": "isgetsetdescriptor",
    "module": "torch.utils.data.distributed.dist.rpc.backend_registry.api.inspect",
    "fullName": "torch.utils.data.distributed.dist.rpc.backend_registry.api.inspect.isgetsetdescriptor",
    "signature": "(object)",
    "description": "Return true if the object is a getset descriptor."
  },
  "375": {
    "name": "ismemberdescriptor",
    "module": "torch.utils.data.distributed.dist.rpc.backend_registry.api.inspect",
    "fullName": "torch.utils.data.distributed.dist.rpc.backend_registry.api.inspect.ismemberdescriptor",
    "signature": "(object)",
    "description": "Return true if the object is a member descriptor."
  },
  "376": {
    "name": "ismethod",
    "module": "torch.utils.data.distributed.dist.rpc.backend_registry.api.inspect",
    "fullName": "torch.utils.data.distributed.dist.rpc.backend_registry.api.inspect.ismethod",
    "signature": "(object)",
    "description": "Return true if the object is an instance method."
  },
  "377": {
    "name": "ismethoddescriptor",
    "module": "torch.utils.data.distributed.dist.rpc.backend_registry.api.inspect",
    "fullName": "torch.utils.data.distributed.dist.rpc.backend_registry.api.inspect.ismethoddescriptor",
    "signature": "(object)",
    "description": "Return true if the object is a method descriptor."
  },
  "378": {
    "name": "ismodule",
    "module": "torch.utils.data.distributed.dist.rpc.backend_registry.api.inspect",
    "fullName": "torch.utils.data.distributed.dist.rpc.backend_registry.api.inspect.ismodule",
    "signature": "(object)",
    "description": "Return true if the object is a module."
  },
  "379": {
    "name": "isroutine",
    "module": "torch.utils.data.distributed.dist.rpc.backend_registry.api.inspect",
    "fullName": "torch.utils.data.distributed.dist.rpc.backend_registry.api.inspect.isroutine",
    "signature": "(object)",
    "description": "Return true if the object is any kind of function or method."
  },
  "380": {
    "name": "istraceback",
    "module": "torch.utils.data.distributed.dist.rpc.backend_registry.api.inspect",
    "fullName": "torch.utils.data.distributed.dist.rpc.backend_registry.api.inspect.istraceback",
    "signature": "(object)",
    "description": "Return true if the object is a traceback."
  },
  "381": {
    "name": "namedtuple",
    "module": "torch.utils.data.distributed.dist.rpc.backend_registry.api.inspect",
    "fullName": "torch.utils.data.distributed.dist.rpc.backend_registry.api.inspect.namedtuple",
    "signature": "(typename, field_names, *, rename=False, defaults=None, module=None)",
    "description": "Returns a new subclass of tuple with named fields."
  },
  "382": {
    "name": "signature",
    "module": "torch.utils.data.distributed.dist.rpc.backend_registry.api.inspect",
    "fullName": "torch.utils.data.distributed.dist.rpc.backend_registry.api.inspect.signature",
    "signature": "(obj, *, follow_wrapped=True)",
    "description": "Get a signature object for the passed callable."
  },
  "383": {
    "name": "stack",
    "module": "torch.utils.data.distributed.dist.rpc.backend_registry.api.inspect",
    "fullName": "torch.utils.data.distributed.dist.rpc.backend_registry.api.inspect.stack",
    "signature": "(context=1)",
    "description": "Return a list of records for the stack above the caller's frame."
  },
  "384": {
    "name": "trace",
    "module": "torch.utils.data.distributed.dist.rpc.backend_registry.api.inspect",
    "fullName": "torch.utils.data.distributed.dist.rpc.backend_registry.api.inspect.trace",
    "signature": "(context=1)",
    "description": "Return a list of records for the stack below the current exception."
  },
  "385": {
    "name": "unwrap",
    "module": "torch.utils.data.distributed.dist.rpc.backend_registry.api.inspect",
    "fullName": "torch.utils.data.distributed.dist.rpc.backend_registry.api.inspect.unwrap",
    "signature": "(func, *, stop=None)",
    "description": "Get the object wrapped by *func*."
  },
  "386": {
    "name": "walktree",
    "module": "torch.utils.data.distributed.dist.rpc.backend_registry.api.inspect",
    "fullName": "torch.utils.data.distributed.dist.rpc.backend_registry.api.inspect.walktree",
    "signature": "(classes, children, parent)",
    "description": "Recursive helper function for getclasstree()."
  },
  "387": {
    "name": "coroutine",
    "module": "torch.utils.data.distributed.dist.rpc.backend_registry.api.inspect.types",
    "fullName": "torch.utils.data.distributed.dist.rpc.backend_registry.api.inspect.types.coroutine",
    "signature": "(func)",
    "description": "Convert regular generator function to a coroutine."
  },
  "388": {
    "name": "new_class",
    "module": "torch.utils.data.distributed.dist.rpc.backend_registry.api.inspect.types",
    "fullName": "torch.utils.data.distributed.dist.rpc.backend_registry.api.inspect.types.new_class",
    "signature": "(name, bases=(), kwds=None, exec_body=None)",
    "description": "Create a class object dynamically using the appropriate metaclass."
  },
  "389": {
    "name": "prepare_class",
    "module": "torch.utils.data.distributed.dist.rpc.backend_registry.api.inspect.types",
    "fullName": "torch.utils.data.distributed.dist.rpc.backend_registry.api.inspect.types.prepare_class",
    "signature": "(name, bases=(), kwds=None)",
    "description": "Call the __prepare__ method of the appropriate metaclass."
  },
  "390": {
    "name": "resolve_bases",
    "module": "torch.utils.data.distributed.dist.rpc.backend_registry.api.inspect.types",
    "fullName": "torch.utils.data.distributed.dist.rpc.backend_registry.api.inspect.types.resolve_bases",
    "signature": "(bases)",
    "description": "Resolve MRO entries dynamically as specified by PEP 560."
  },
  "391": {
    "name": "ISEOF",
    "module": "torch.utils.data.distributed.dist.rpc.backend_registry.api.inspect.token",
    "fullName": "torch.utils.data.distributed.dist.rpc.backend_registry.api.inspect.token.ISEOF",
    "signature": "(x)",
    "description": "No description available."
  },
  "392": {
    "name": "ISNONTERMINAL",
    "module": "torch.utils.data.distributed.dist.rpc.backend_registry.api.inspect.token",
    "fullName": "torch.utils.data.distributed.dist.rpc.backend_registry.api.inspect.token.ISNONTERMINAL",
    "signature": "(x)",
    "description": "No description available."
  },
  "393": {
    "name": "ISTERMINAL",
    "module": "torch.utils.data.distributed.dist.rpc.backend_registry.api.inspect.token",
    "fullName": "torch.utils.data.distributed.dist.rpc.backend_registry.api.inspect.token.ISTERMINAL",
    "signature": "(x)",
    "description": "No description available."
  },
  "394": {
    "name": "import_module",
    "module": "torch.utils.data.distributed.dist.rpc.backend_registry.api.inspect.importlib",
    "fullName": "torch.utils.data.distributed.dist.rpc.backend_registry.api.inspect.importlib.import_module",
    "signature": "(name, package=None)",
    "description": "Import a module."
  },
  "395": {
    "name": "invalidate_caches",
    "module": "torch.utils.data.distributed.dist.rpc.backend_registry.api.inspect.importlib",
    "fullName": "torch.utils.data.distributed.dist.rpc.backend_registry.api.inspect.importlib.invalidate_caches",
    "signature": "()",
    "description": "Call the invalidate_caches() method on all meta path finders stored in"
  },
  "396": {
    "name": "reload",
    "module": "torch.utils.data.distributed.dist.rpc.backend_registry.api.inspect.importlib",
    "fullName": "torch.utils.data.distributed.dist.rpc.backend_registry.api.inspect.importlib.reload",
    "signature": "(module)",
    "description": "Reload the module and return it."
  },
  "397": {
    "name": "contextmanager",
    "module": "torch.utils.data.distributed.dist.rpc.backend_registry.api.inspect.importlib.util",
    "fullName": "torch.utils.data.distributed.dist.rpc.backend_registry.api.inspect.importlib.util.contextmanager",
    "signature": "(func)",
    "description": "@contextmanager decorator."
  },
  "398": {
    "name": "decode_source",
    "module": "torch.utils.data.distributed.dist.rpc.backend_registry.api.inspect.importlib.util",
    "fullName": "torch.utils.data.distributed.dist.rpc.backend_registry.api.inspect.importlib.util.decode_source",
    "signature": "(source_bytes)",
    "description": "Decode bytes representing source code and return the string."
  },
  "399": {
    "name": "find_spec",
    "module": "torch.utils.data.distributed.dist.rpc.backend_registry.api.inspect.importlib.util",
    "fullName": "torch.utils.data.distributed.dist.rpc.backend_registry.api.inspect.importlib.util.find_spec",
    "signature": "(name, package=None)",
    "description": "Return the spec for the specified module."
  },
  "400": {
    "name": "module_for_loader",
    "module": "torch.utils.data.distributed.dist.rpc.backend_registry.api.inspect.importlib.util",
    "fullName": "torch.utils.data.distributed.dist.rpc.backend_registry.api.inspect.importlib.util.module_for_loader",
    "signature": "(fxn)",
    "description": "Decorator to handle selecting the proper module for loaders."
  },
  "401": {
    "name": "module_from_spec",
    "module": "torch.utils.data.distributed.dist.rpc.backend_registry.api.inspect.importlib.util",
    "fullName": "torch.utils.data.distributed.dist.rpc.backend_registry.api.inspect.importlib.util.module_from_spec",
    "signature": "(spec)",
    "description": "Create a module based on the provided spec."
  },
  "402": {
    "name": "resolve_name",
    "module": "torch.utils.data.distributed.dist.rpc.backend_registry.api.inspect.importlib.util",
    "fullName": "torch.utils.data.distributed.dist.rpc.backend_registry.api.inspect.importlib.util.resolve_name",
    "signature": "(name, package)",
    "description": "Resolve a relative module name to an absolute one."
  },
  "403": {
    "name": "source_from_cache",
    "module": "torch.utils.data.distributed.dist.rpc.backend_registry.api.inspect.importlib.util",
    "fullName": "torch.utils.data.distributed.dist.rpc.backend_registry.api.inspect.importlib.util.source_from_cache",
    "signature": "(path)",
    "description": "Given the path to a .pyc. file, return the path to its .py file."
  },
  "404": {
    "name": "source_hash",
    "module": "torch.utils.data.distributed.dist.rpc.backend_registry.api.inspect.importlib.util",
    "fullName": "torch.utils.data.distributed.dist.rpc.backend_registry.api.inspect.importlib.util.source_hash",
    "signature": "(source_bytes)",
    "description": "Return the hash of *source_bytes* as used in hash-based pyc files."
  },
  "405": {
    "name": "spec_from_file_location",
    "module": "torch.utils.data.distributed.dist.rpc.backend_registry.api.inspect.importlib.util",
    "fullName": "torch.utils.data.distributed.dist.rpc.backend_registry.api.inspect.importlib.util.spec_from_file_location",
    "signature": "(name, location=None, *, loader=None, submodule_search_locations=<object object at 0x7fce23cd50d0>)",
    "description": "Return a module spec based on a file location."
  },
  "406": {
    "name": "spec_from_loader",
    "module": "torch.utils.data.distributed.dist.rpc.backend_registry.api.inspect.importlib.util",
    "fullName": "torch.utils.data.distributed.dist.rpc.backend_registry.api.inspect.importlib.util.spec_from_loader",
    "signature": "(name, loader, *, origin=None, is_package=None)",
    "description": "Return a module spec based on various loader methods."
  },
  "407": {
    "name": "runtime_checkable",
    "module": "torch.utils.data.distributed.dist.rpc.backend_registry.api.inspect.importlib.util.abc",
    "fullName": "torch.utils.data.distributed.dist.rpc.backend_registry.api.inspect.importlib.util.abc.runtime_checkable",
    "signature": "(cls)",
    "description": "Mark a protocol class as a runtime protocol."
  },
  "408": {
    "name": "all_suffixes",
    "module": "torch.utils.data.distributed.dist.rpc.backend_registry.api.inspect.importlib.util.abc.machinery",
    "fullName": "torch.utils.data.distributed.dist.rpc.backend_registry.api.inspect.importlib.util.abc.machinery.all_suffixes",
    "signature": "()",
    "description": "Returns a list of all recognized module suffixes for this process"
  },
  "409": {
    "name": "code_info",
    "module": "torch.utils.data.distributed.dist.rpc.backend_registry.api.inspect.dis",
    "fullName": "torch.utils.data.distributed.dist.rpc.backend_registry.api.inspect.dis.code_info",
    "signature": "(x)",
    "description": "Formatted details of methods, functions, or code."
  },
  "410": {
    "name": "dis",
    "module": "torch.utils.data.distributed.dist.rpc.backend_registry.api.inspect.dis",
    "fullName": "torch.utils.data.distributed.dist.rpc.backend_registry.api.inspect.dis.dis",
    "signature": "(x=None, *, file=None, depth=None)",
    "description": "Disassemble classes, methods, functions, and other compiled objects."
  },
  "411": {
    "name": "disassemble",
    "module": "torch.utils.data.distributed.dist.rpc.backend_registry.api.inspect.dis",
    "fullName": "torch.utils.data.distributed.dist.rpc.backend_registry.api.inspect.dis.disassemble",
    "signature": "(co, lasti=-1, *, file=None)",
    "description": "Disassemble a code object."
  },
  "412": {
    "name": "disco",
    "module": "torch.utils.data.distributed.dist.rpc.backend_registry.api.inspect.dis",
    "fullName": "torch.utils.data.distributed.dist.rpc.backend_registry.api.inspect.dis.disco",
    "signature": "(co, lasti=-1, *, file=None)",
    "description": "Disassemble a code object."
  },
  "413": {
    "name": "distb",
    "module": "torch.utils.data.distributed.dist.rpc.backend_registry.api.inspect.dis",
    "fullName": "torch.utils.data.distributed.dist.rpc.backend_registry.api.inspect.dis.distb",
    "signature": "(tb=None, *, file=None)",
    "description": "Disassemble a traceback (default: last traceback)."
  },
  "414": {
    "name": "findlabels",
    "module": "torch.utils.data.distributed.dist.rpc.backend_registry.api.inspect.dis",
    "fullName": "torch.utils.data.distributed.dist.rpc.backend_registry.api.inspect.dis.findlabels",
    "signature": "(code)",
    "description": "Detect all offsets in a byte code which are jump targets."
  },
  "415": {
    "name": "findlinestarts",
    "module": "torch.utils.data.distributed.dist.rpc.backend_registry.api.inspect.dis",
    "fullName": "torch.utils.data.distributed.dist.rpc.backend_registry.api.inspect.dis.findlinestarts",
    "signature": "(code)",
    "description": "Find the offsets in a byte code which are start of lines in the source."
  },
  "416": {
    "name": "get_instructions",
    "module": "torch.utils.data.distributed.dist.rpc.backend_registry.api.inspect.dis",
    "fullName": "torch.utils.data.distributed.dist.rpc.backend_registry.api.inspect.dis.get_instructions",
    "signature": "(x, *, first_line=None)",
    "description": "Iterator for the opcodes in methods, functions or code"
  },
  "417": {
    "name": "pretty_flags",
    "module": "torch.utils.data.distributed.dist.rpc.backend_registry.api.inspect.dis",
    "fullName": "torch.utils.data.distributed.dist.rpc.backend_registry.api.inspect.dis.pretty_flags",
    "signature": "(flags)",
    "description": "Return pretty representation of code flags."
  },
  "418": {
    "name": "show_code",
    "module": "torch.utils.data.distributed.dist.rpc.backend_registry.api.inspect.dis",
    "fullName": "torch.utils.data.distributed.dist.rpc.backend_registry.api.inspect.dis.show_code",
    "signature": "(co, *, file=None)",
    "description": "Print details of methods, functions, or code to *file*."
  },
  "419": {
    "name": "contextmanager",
    "module": "torch.utils.data.distributed.dist.rpc.backend_registry.api.inspect.ast",
    "fullName": "torch.utils.data.distributed.dist.rpc.backend_registry.api.inspect.ast.contextmanager",
    "signature": "(func)",
    "description": "@contextmanager decorator."
  },
  "420": {
    "name": "copy_location",
    "module": "torch.utils.data.distributed.dist.rpc.backend_registry.api.inspect.ast",
    "fullName": "torch.utils.data.distributed.dist.rpc.backend_registry.api.inspect.ast.copy_location",
    "signature": "(new_node, old_node)",
    "description": "Copy source location (`lineno`, `col_offset`, `end_lineno`, and `end_col_offset`"
  },
  "421": {
    "name": "dump",
    "module": "torch.utils.data.distributed.dist.rpc.backend_registry.api.inspect.ast",
    "fullName": "torch.utils.data.distributed.dist.rpc.backend_registry.api.inspect.ast.dump",
    "signature": "(node, annotate_fields=True, include_attributes=False, *, indent=None)",
    "description": "Return a formatted dump of the tree in node.  This is mainly useful for"
  },
  "422": {
    "name": "fix_missing_locations",
    "module": "torch.utils.data.distributed.dist.rpc.backend_registry.api.inspect.ast",
    "fullName": "torch.utils.data.distributed.dist.rpc.backend_registry.api.inspect.ast.fix_missing_locations",
    "signature": "(node)",
    "description": "When you compile a node tree with compile(), the compiler expects lineno and"
  },
  "423": {
    "name": "get_docstring",
    "module": "torch.utils.data.distributed.dist.rpc.backend_registry.api.inspect.ast",
    "fullName": "torch.utils.data.distributed.dist.rpc.backend_registry.api.inspect.ast.get_docstring",
    "signature": "(node, clean=True)",
    "description": "Return the docstring for the given node or None if no docstring can"
  },
  "424": {
    "name": "get_source_segment",
    "module": "torch.utils.data.distributed.dist.rpc.backend_registry.api.inspect.ast",
    "fullName": "torch.utils.data.distributed.dist.rpc.backend_registry.api.inspect.ast.get_source_segment",
    "signature": "(source, node, *, padded=False)",
    "description": "Get source code segment of the *source* that generated *node*."
  },
  "425": {
    "name": "increment_lineno",
    "module": "torch.utils.data.distributed.dist.rpc.backend_registry.api.inspect.ast",
    "fullName": "torch.utils.data.distributed.dist.rpc.backend_registry.api.inspect.ast.increment_lineno",
    "signature": "(node, n=1)",
    "description": "Increment the line number and end line number of each node in the tree"
  },
  "426": {
    "name": "iter_child_nodes",
    "module": "torch.utils.data.distributed.dist.rpc.backend_registry.api.inspect.ast",
    "fullName": "torch.utils.data.distributed.dist.rpc.backend_registry.api.inspect.ast.iter_child_nodes",
    "signature": "(node)",
    "description": "Yield all direct child nodes of *node*, that is, all fields that are nodes"
  },
  "427": {
    "name": "iter_fields",
    "module": "torch.utils.data.distributed.dist.rpc.backend_registry.api.inspect.ast",
    "fullName": "torch.utils.data.distributed.dist.rpc.backend_registry.api.inspect.ast.iter_fields",
    "signature": "(node)",
    "description": "Yield a tuple of ``(fieldname, value)`` for each field in ``node._fields``"
  },
  "428": {
    "name": "literal_eval",
    "module": "torch.utils.data.distributed.dist.rpc.backend_registry.api.inspect.ast",
    "fullName": "torch.utils.data.distributed.dist.rpc.backend_registry.api.inspect.ast.literal_eval",
    "signature": "(node_or_string)",
    "description": "Safely evaluate an expression node or a string containing a Python"
  },
  "429": {
    "name": "main",
    "module": "torch.utils.data.distributed.dist.rpc.backend_registry.api.inspect.ast",
    "fullName": "torch.utils.data.distributed.dist.rpc.backend_registry.api.inspect.ast.main",
    "signature": "()",
    "description": "No description available."
  },
  "430": {
    "name": "parse",
    "module": "torch.utils.data.distributed.dist.rpc.backend_registry.api.inspect.ast",
    "fullName": "torch.utils.data.distributed.dist.rpc.backend_registry.api.inspect.ast.parse",
    "signature": "(source, filename='<unknown>', mode='exec', *, type_comments=False, feature_version=None)",
    "description": "Parse the source into an AST node."
  },
  "431": {
    "name": "unparse",
    "module": "torch.utils.data.distributed.dist.rpc.backend_registry.api.inspect.ast",
    "fullName": "torch.utils.data.distributed.dist.rpc.backend_registry.api.inspect.ast.unparse",
    "signature": "(ast_obj)",
    "description": "No description available."
  },
  "432": {
    "name": "walk",
    "module": "torch.utils.data.distributed.dist.rpc.backend_registry.api.inspect.ast",
    "fullName": "torch.utils.data.distributed.dist.rpc.backend_registry.api.inspect.ast.walk",
    "signature": "(node)",
    "description": "Recursively yield all descendant nodes in the tree starting at *node*"
  },
  "433": {
    "name": "asynccontextmanager",
    "module": "torch.utils.data.distributed.dist.rpc.backend_registry.api.contextlib",
    "fullName": "torch.utils.data.distributed.dist.rpc.backend_registry.api.contextlib.asynccontextmanager",
    "signature": "(func)",
    "description": "@asynccontextmanager decorator."
  },
  "434": {
    "name": "contextmanager",
    "module": "torch.utils.data.distributed.dist.rpc.backend_registry.api.contextlib",
    "fullName": "torch.utils.data.distributed.dist.rpc.backend_registry.api.contextlib.contextmanager",
    "signature": "(func)",
    "description": "@contextmanager decorator."
  },
  "435": {
    "name": "wraps",
    "module": "torch.utils.data.distributed.dist.rpc.backend_registry.api.contextlib",
    "fullName": "torch.utils.data.distributed.dist.rpc.backend_registry.api.contextlib.wraps",
    "signature": "(wrapped, assigned=('__module__', '__name__', '__qualname__', '__doc__', '__annotations__'), updated=('__dict__',))",
    "description": "Decorator factory to apply update_wrapper() to a wrapper function"
  },
  "436": {
    "name": "all_gather",
    "module": "torch.utils.data.distributed.dist.distributed_c10d",
    "fullName": "torch.utils.data.distributed.dist.distributed_c10d.all_gather",
    "signature": "(tensor_list, tensor, group=None, async_op=False)",
    "description": "Gathers tensors from the whole group in a list."
  },
  "437": {
    "name": "all_gather_coalesced",
    "module": "torch.utils.data.distributed.dist.distributed_c10d",
    "fullName": "torch.utils.data.distributed.dist.distributed_c10d.all_gather_coalesced",
    "signature": "(output_tensor_lists, input_tensor_list, group=None, async_op=False)",
    "description": "Gathers input tensors from the whole group in a list in a coalesced manner."
  },
  "438": {
    "name": "all_gather_multigpu",
    "module": "torch.utils.data.distributed.dist.distributed_c10d",
    "fullName": "torch.utils.data.distributed.dist.distributed_c10d.all_gather_multigpu",
    "signature": "(output_tensor_lists, input_tensor_list, group=None, async_op=False)",
    "description": "Gathers tensors from the whole group in a list."
  },
  "439": {
    "name": "all_gather_object",
    "module": "torch.utils.data.distributed.dist.distributed_c10d",
    "fullName": "torch.utils.data.distributed.dist.distributed_c10d.all_gather_object",
    "signature": "(object_list, obj, group=None)",
    "description": "Gathers picklable objects from the whole group into a list. Similar to"
  },
  "440": {
    "name": "all_reduce",
    "module": "torch.utils.data.distributed.dist.distributed_c10d",
    "fullName": "torch.utils.data.distributed.dist.distributed_c10d.all_reduce",
    "signature": "(tensor, op=<ReduceOp.SUM: 0>, group=None, async_op=False)",
    "description": "Reduces the tensor data across all machines in such a way that all get"
  },
  "441": {
    "name": "all_reduce_coalesced",
    "module": "torch.utils.data.distributed.dist.distributed_c10d",
    "fullName": "torch.utils.data.distributed.dist.distributed_c10d.all_reduce_coalesced",
    "signature": "(tensors, op=<ReduceOp.SUM: 0>, group=None, async_op=False)",
    "description": "WARNING: at this time individual shape checking is not implemented across nodes."
  },
  "442": {
    "name": "all_reduce_multigpu",
    "module": "torch.utils.data.distributed.dist.distributed_c10d",
    "fullName": "torch.utils.data.distributed.dist.distributed_c10d.all_reduce_multigpu",
    "signature": "(tensor_list, op=<ReduceOp.SUM: 0>, group=None, async_op=False)",
    "description": "Reduces the tensor data across all machines in such a way that all get"
  },
  "443": {
    "name": "barrier",
    "module": "torch.utils.data.distributed.dist.distributed_c10d",
    "fullName": "torch.utils.data.distributed.dist.distributed_c10d.barrier",
    "signature": "(group=None, async_op=False, device_ids=None)",
    "description": "Synchronizes all processes."
  },
  "444": {
    "name": "batch_isend_irecv",
    "module": "torch.utils.data.distributed.dist.distributed_c10d",
    "fullName": "torch.utils.data.distributed.dist.distributed_c10d.batch_isend_irecv",
    "signature": "(p2p_op_list)",
    "description": "Send or Receive a batch of tensors asynchronously and return a list of requests."
  },
  "445": {
    "name": "broadcast",
    "module": "torch.utils.data.distributed.dist.distributed_c10d",
    "fullName": "torch.utils.data.distributed.dist.distributed_c10d.broadcast",
    "signature": "(tensor, src, group=None, async_op=False)",
    "description": "Broadcasts the tensor to the whole group."
  },
  "446": {
    "name": "broadcast_multigpu",
    "module": "torch.utils.data.distributed.dist.distributed_c10d",
    "fullName": "torch.utils.data.distributed.dist.distributed_c10d.broadcast_multigpu",
    "signature": "(tensor_list, src, group=None, async_op=False, src_tensor=0)",
    "description": "Broadcasts the tensor to the whole group with multiple GPU tensors"
  },
  "447": {
    "name": "broadcast_object_list",
    "module": "torch.utils.data.distributed.dist.distributed_c10d",
    "fullName": "torch.utils.data.distributed.dist.distributed_c10d.broadcast_object_list",
    "signature": "(object_list, src=0, group=None, device=None)",
    "description": "Broadcasts picklable objects in ``object_list`` to the whole group. Similar"
  },
  "448": {
    "name": "destroy_process_group",
    "module": "torch.utils.data.distributed.dist.distributed_c10d",
    "fullName": "torch.utils.data.distributed.dist.distributed_c10d.destroy_process_group",
    "signature": "(group=None)",
    "description": "Destroy a given process group, and deinitialize the distributed package"
  },
  "449": {
    "name": "gather",
    "module": "torch.utils.data.distributed.dist.distributed_c10d",
    "fullName": "torch.utils.data.distributed.dist.distributed_c10d.gather",
    "signature": "(tensor, gather_list=None, dst=0, group=None, async_op=False)",
    "description": "Gathers a list of tensors in a single process."
  },
  "450": {
    "name": "gather_object",
    "module": "torch.utils.data.distributed.dist.distributed_c10d",
    "fullName": "torch.utils.data.distributed.dist.distributed_c10d.gather_object",
    "signature": "(obj, object_gather_list=None, dst=0, group=None)",
    "description": "Gathers picklable objects from the whole group in a single process."
  },
  "451": {
    "name": "get_backend",
    "module": "torch.utils.data.distributed.dist.distributed_c10d",
    "fullName": "torch.utils.data.distributed.dist.distributed_c10d.get_backend",
    "signature": "(group=None)",
    "description": "Returns the backend of the given process group."
  },
  "452": {
    "name": "get_rank",
    "module": "torch.utils.data.distributed.dist.distributed_c10d",
    "fullName": "torch.utils.data.distributed.dist.distributed_c10d.get_rank",
    "signature": "(group=None)",
    "description": "Returns the rank of the current process in the provided ``group`` or the"
  },
  "453": {
    "name": "get_world_size",
    "module": "torch.utils.data.distributed.dist.distributed_c10d",
    "fullName": "torch.utils.data.distributed.dist.distributed_c10d.get_world_size",
    "signature": "(group=None)",
    "description": "Returns the number of processes in the current process group"
  },
  "454": {
    "name": "irecv",
    "module": "torch.utils.data.distributed.dist.distributed_c10d",
    "fullName": "torch.utils.data.distributed.dist.distributed_c10d.irecv",
    "signature": "(tensor, src=None, group=None, tag=0)",
    "description": "Receives a tensor asynchronously."
  },
  "455": {
    "name": "is_gloo_available",
    "module": "torch.utils.data.distributed.dist.distributed_c10d",
    "fullName": "torch.utils.data.distributed.dist.distributed_c10d.is_gloo_available",
    "signature": "()",
    "description": "Checks if the Gloo backend is available."
  },
  "456": {
    "name": "is_initialized",
    "module": "torch.utils.data.distributed.dist.distributed_c10d",
    "fullName": "torch.utils.data.distributed.dist.distributed_c10d.is_initialized",
    "signature": "()",
    "description": "Checking if the default process group has been initialized"
  },
  "457": {
    "name": "is_mpi_available",
    "module": "torch.utils.data.distributed.dist.distributed_c10d",
    "fullName": "torch.utils.data.distributed.dist.distributed_c10d.is_mpi_available",
    "signature": "()",
    "description": "Checks if the MPI backend is available."
  },
  "458": {
    "name": "is_nccl_available",
    "module": "torch.utils.data.distributed.dist.distributed_c10d",
    "fullName": "torch.utils.data.distributed.dist.distributed_c10d.is_nccl_available",
    "signature": "()",
    "description": "Checks if the NCCL backend is available."
  },
  "459": {
    "name": "is_torchelastic_launched",
    "module": "torch.utils.data.distributed.dist.distributed_c10d",
    "fullName": "torch.utils.data.distributed.dist.distributed_c10d.is_torchelastic_launched",
    "signature": "()",
    "description": "Checks whether this process was launched with ``torch.distributed.elastic``"
  },
  "460": {
    "name": "isend",
    "module": "torch.utils.data.distributed.dist.distributed_c10d",
    "fullName": "torch.utils.data.distributed.dist.distributed_c10d.isend",
    "signature": "(tensor, dst, group=None, tag=0)",
    "description": "Sends a tensor asynchronously."
  },
  "461": {
    "name": "monitored_barrier",
    "module": "torch.utils.data.distributed.dist.distributed_c10d",
    "fullName": "torch.utils.data.distributed.dist.distributed_c10d.monitored_barrier",
    "signature": "(group=None, timeout=None, wait_all_ranks=False)",
    "description": "Synchronizes all processes similar to ``torch.distributed.barrier``, but takes"
  },
  "462": {
    "name": "new_group",
    "module": "torch.utils.data.distributed.dist.distributed_c10d",
    "fullName": "torch.utils.data.distributed.dist.distributed_c10d.new_group",
    "signature": "(ranks=None, timeout=datetime.timedelta(seconds=1800), backend=None, pg_options=None)",
    "description": "Creates a new distributed group."
  },
  "463": {
    "name": "new_subgroups",
    "module": "torch.utils.data.distributed.dist.distributed_c10d",
    "fullName": "torch.utils.data.distributed.dist.distributed_c10d.new_subgroups",
    "signature": "(group_size=None, group=None, timeout=datetime.timedelta(seconds=1800), backend=None, pg_options=None)",
    "description": "Creates GPU subgroups of equal size. By default, it creates intra-machine subgroups,"
  },
  "464": {
    "name": "new_subgroups_by_enumeration",
    "module": "torch.utils.data.distributed.dist.distributed_c10d",
    "fullName": "torch.utils.data.distributed.dist.distributed_c10d.new_subgroups_by_enumeration",
    "signature": "(ranks_per_subgroup_list, timeout=datetime.timedelta(seconds=1800), backend=None, pg_options=None)",
    "description": "Creates GPU subgroups by dividing the global world, where the division is specified by"
  },
  "465": {
    "name": "recv",
    "module": "torch.utils.data.distributed.dist.distributed_c10d",
    "fullName": "torch.utils.data.distributed.dist.distributed_c10d.recv",
    "signature": "(tensor, src=None, group=None, tag=0)",
    "description": "Receives a tensor synchronously."
  },
  "466": {
    "name": "reduce",
    "module": "torch.utils.data.distributed.dist.distributed_c10d",
    "fullName": "torch.utils.data.distributed.dist.distributed_c10d.reduce",
    "signature": "(tensor, dst, op=<ReduceOp.SUM: 0>, group=None, async_op=False)",
    "description": "Reduces the tensor data across all machines."
  },
  "467": {
    "name": "reduce_multigpu",
    "module": "torch.utils.data.distributed.dist.distributed_c10d",
    "fullName": "torch.utils.data.distributed.dist.distributed_c10d.reduce_multigpu",
    "signature": "(tensor_list, dst, op=<ReduceOp.SUM: 0>, group=None, async_op=False, dst_tensor=0)",
    "description": "Reduces the tensor data on multiple GPUs across all machines. Each tensor"
  },
  "468": {
    "name": "reduce_scatter",
    "module": "torch.utils.data.distributed.dist.distributed_c10d",
    "fullName": "torch.utils.data.distributed.dist.distributed_c10d.reduce_scatter",
    "signature": "(output, input_list, op=<ReduceOp.SUM: 0>, group=None, async_op=False)",
    "description": "Reduces, then scatters a list of tensors to all processes in a group."
  },
  "469": {
    "name": "reduce_scatter_multigpu",
    "module": "torch.utils.data.distributed.dist.distributed_c10d",
    "fullName": "torch.utils.data.distributed.dist.distributed_c10d.reduce_scatter_multigpu",
    "signature": "(output_tensor_list, input_tensor_lists, op=<ReduceOp.SUM: 0>, group=None, async_op=False)",
    "description": "Reduce and scatter a list of tensors to the whole group.  Only nccl backend"
  },
  "470": {
    "name": "register_rendezvous_handler",
    "module": "torch.utils.data.distributed.dist.distributed_c10d",
    "fullName": "torch.utils.data.distributed.dist.distributed_c10d.register_rendezvous_handler",
    "signature": "(scheme, handler)",
    "description": "Registers a new rendezvous handler."
  },
  "471": {
    "name": "rendezvous",
    "module": "torch.utils.data.distributed.dist.distributed_c10d",
    "fullName": "torch.utils.data.distributed.dist.distributed_c10d.rendezvous",
    "signature": "(url: str, rank: int = -1, world_size: int = -1, **kwargs)",
    "description": "No description available."
  },
  "472": {
    "name": "scatter",
    "module": "torch.utils.data.distributed.dist.distributed_c10d",
    "fullName": "torch.utils.data.distributed.dist.distributed_c10d.scatter",
    "signature": "(tensor, scatter_list=None, src=0, group=None, async_op=False)",
    "description": "Scatters a list of tensors to all processes in a group."
  },
  "473": {
    "name": "scatter_object_list",
    "module": "torch.utils.data.distributed.dist.distributed_c10d",
    "fullName": "torch.utils.data.distributed.dist.distributed_c10d.scatter_object_list",
    "signature": "(scatter_object_output_list, scatter_object_input_list, src=0, group=None)",
    "description": "Scatters picklable objects in ``scatter_object_input_list`` to the whole"
  },
  "474": {
    "name": "send",
    "module": "torch.utils.data.distributed.dist.distributed_c10d",
    "fullName": "torch.utils.data.distributed.dist.distributed_c10d.send",
    "signature": "(tensor, dst, group=None, tag=0)",
    "description": "Sends a tensor synchronously."
  },
  "475": {
    "name": "supports_complex",
    "module": "torch.utils.data.distributed.dist.distributed_c10d",
    "fullName": "torch.utils.data.distributed.dist.distributed_c10d.supports_complex",
    "signature": "(reduceOp: torch._C._distributed_c10d.ReduceOp) -> bool",
    "description": "No description available."
  },
  "476": {
    "name": "NamedTuple",
    "module": "torch.utils.data.distributed.dist.algorithms.join",
    "fullName": "torch.utils.data.distributed.dist.algorithms.join.NamedTuple",
    "signature": "(typename, fields=None, /, **kwargs)",
    "description": "Typed version of namedtuple."
  },
  "477": {
    "name": "abstractmethod",
    "module": "torch.utils.data.distributed.dist.algorithms.join",
    "fullName": "torch.utils.data.distributed.dist.algorithms.join.abstractmethod",
    "signature": "(funcobj)",
    "description": "A decorator indicating abstract methods."
  },
  "478": {
    "name": "random_split",
    "module": "torch.utils.data.dataset",
    "fullName": "torch.utils.data.dataset.random_split",
    "signature": "(dataset: torch.utils.data.dataset.Dataset[~T], lengths: Sequence[int], generator: Optional[torch._C.Generator] = <torch._C.Generator object at 0x7fce236379b0>) -> List[torch.utils.data.dataset.Subset[~T]]",
    "description": "Randomly split a dataset into non-overlapping new datasets of given lengths."
  },
  "479": {
    "name": "audiohandler",
    "module": "torch.utils.data.datapipes.utils.decoder",
    "fullName": "torch.utils.data.datapipes.utils.decoder.audiohandler",
    "signature": "(extension, data)",
    "description": "No description available."
  },
  "480": {
    "name": "basichandlers",
    "module": "torch.utils.data.datapipes.utils.decoder",
    "fullName": "torch.utils.data.datapipes.utils.decoder.basichandlers",
    "signature": "(extension, data)",
    "description": "No description available."
  },
  "481": {
    "name": "extension_extract_fn",
    "module": "torch.utils.data.datapipes.utils.decoder",
    "fullName": "torch.utils.data.datapipes.utils.decoder.extension_extract_fn",
    "signature": "(pathname)",
    "description": "No description available."
  },
  "482": {
    "name": "handle_extension",
    "module": "torch.utils.data.datapipes.utils.decoder",
    "fullName": "torch.utils.data.datapipes.utils.decoder.handle_extension",
    "signature": "(extensions, f)",
    "description": "Returns a decoder handler function for the list of extensions."
  },
  "483": {
    "name": "imagehandler",
    "module": "torch.utils.data.datapipes.utils.decoder",
    "fullName": "torch.utils.data.datapipes.utils.decoder.imagehandler",
    "signature": "(imagespec)",
    "description": "No description available."
  },
  "484": {
    "name": "mathandler",
    "module": "torch.utils.data.datapipes.utils.decoder",
    "fullName": "torch.utils.data.datapipes.utils.decoder.mathandler",
    "signature": "(**loadmat_kwargs)",
    "description": "No description available."
  },
  "485": {
    "name": "videohandler",
    "module": "torch.utils.data.datapipes.utils.decoder",
    "fullName": "torch.utils.data.datapipes.utils.decoder.videohandler",
    "signature": "(extension, data)",
    "description": "No description available."
  },
  "486": {
    "name": "NamedTemporaryFile",
    "module": "torch.utils.data.datapipes.utils.decoder.tempfile",
    "fullName": "torch.utils.data.datapipes.utils.decoder.tempfile.NamedTemporaryFile",
    "signature": "(mode='w+b', buffering=-1, encoding=None, newline=None, suffix=None, prefix=None, dir=None, delete=True, *, errors=None)",
    "description": "Create and return a temporary file."
  },
  "487": {
    "name": "TemporaryFile",
    "module": "torch.utils.data.datapipes.utils.decoder.tempfile",
    "fullName": "torch.utils.data.datapipes.utils.decoder.tempfile.TemporaryFile",
    "signature": "(mode='w+b', buffering=-1, encoding=None, newline=None, suffix=None, prefix=None, dir=None, *, errors=None)",
    "description": "Create and return a temporary file."
  },
  "488": {
    "name": "gettempdir",
    "module": "torch.utils.data.datapipes.utils.decoder.tempfile",
    "fullName": "torch.utils.data.datapipes.utils.decoder.tempfile.gettempdir",
    "signature": "()",
    "description": "Accessor for tempfile.tempdir."
  },
  "489": {
    "name": "gettempdirb",
    "module": "torch.utils.data.datapipes.utils.decoder.tempfile",
    "fullName": "torch.utils.data.datapipes.utils.decoder.tempfile.gettempdirb",
    "signature": "()",
    "description": "A bytes version of tempfile.gettempdir()."
  },
  "490": {
    "name": "gettempprefix",
    "module": "torch.utils.data.datapipes.utils.decoder.tempfile",
    "fullName": "torch.utils.data.datapipes.utils.decoder.tempfile.gettempprefix",
    "signature": "()",
    "description": "The default prefix for temporary directories."
  },
  "491": {
    "name": "gettempprefixb",
    "module": "torch.utils.data.datapipes.utils.decoder.tempfile",
    "fullName": "torch.utils.data.datapipes.utils.decoder.tempfile.gettempprefixb",
    "signature": "()",
    "description": "The default prefix for temporary directories as bytes."
  },
  "492": {
    "name": "mkdtemp",
    "module": "torch.utils.data.datapipes.utils.decoder.tempfile",
    "fullName": "torch.utils.data.datapipes.utils.decoder.tempfile.mkdtemp",
    "signature": "(suffix=None, prefix=None, dir=None)",
    "description": "User-callable function to create and return a unique temporary"
  },
  "493": {
    "name": "mkstemp",
    "module": "torch.utils.data.datapipes.utils.decoder.tempfile",
    "fullName": "torch.utils.data.datapipes.utils.decoder.tempfile.mkstemp",
    "signature": "(suffix=None, prefix=None, dir=None, text=False)",
    "description": "User-callable function to create and return a unique temporary"
  },
  "494": {
    "name": "mktemp",
    "module": "torch.utils.data.datapipes.utils.decoder.tempfile",
    "fullName": "torch.utils.data.datapipes.utils.decoder.tempfile.mktemp",
    "signature": "(suffix='', prefix='tmp', dir=None)",
    "description": "User-callable function to return a unique temporary file name.  The"
  },
  "495": {
    "name": "detect_encoding",
    "module": "torch.utils.data.datapipes.utils.decoder.json",
    "fullName": "torch.utils.data.datapipes.utils.decoder.json.detect_encoding",
    "signature": "(b)",
    "description": "No description available."
  },
  "496": {
    "name": "dump",
    "module": "torch.utils.data.datapipes.utils.decoder.json",
    "fullName": "torch.utils.data.datapipes.utils.decoder.json.dump",
    "signature": "(obj, fp, *, skipkeys=False, ensure_ascii=True, check_circular=True, allow_nan=True, cls=None, indent=None, separators=None, default=None, sort_keys=False, **kw)",
    "description": "Serialize ``obj`` as a JSON formatted stream to ``fp`` (a"
  },
  "497": {
    "name": "dumps",
    "module": "torch.utils.data.datapipes.utils.decoder.json",
    "fullName": "torch.utils.data.datapipes.utils.decoder.json.dumps",
    "signature": "(obj, *, skipkeys=False, ensure_ascii=True, check_circular=True, allow_nan=True, cls=None, indent=None, separators=None, default=None, sort_keys=False, **kw)",
    "description": "Serialize ``obj`` to a JSON formatted ``str``."
  },
  "498": {
    "name": "load",
    "module": "torch.utils.data.datapipes.utils.decoder.json",
    "fullName": "torch.utils.data.datapipes.utils.decoder.json.load",
    "signature": "(fp, *, cls=None, object_hook=None, parse_float=None, parse_int=None, parse_constant=None, object_pairs_hook=None, **kw)",
    "description": "Deserialize ``fp`` (a ``.read()``-supporting file-like object containing"
  },
  "499": {
    "name": "loads",
    "module": "torch.utils.data.datapipes.utils.decoder.json",
    "fullName": "torch.utils.data.datapipes.utils.decoder.json.loads",
    "signature": "(s, *, cls=None, object_hook=None, parse_float=None, parse_int=None, parse_constant=None, object_pairs_hook=None, **kw)",
    "description": "Deserialize ``s`` (a ``str``, ``bytes`` or ``bytearray`` instance"
  },
  "500": {
    "name": "py_make_scanner",
    "module": "torch.utils.data.datapipes.utils.decoder.json.scanner",
    "fullName": "torch.utils.data.datapipes.utils.decoder.json.scanner.py_make_scanner",
    "signature": "(context)",
    "description": "No description available."
  },
  "501": {
    "name": "py_encode_basestring",
    "module": "torch.utils.data.datapipes.utils.decoder.json.encoder",
    "fullName": "torch.utils.data.datapipes.utils.decoder.json.encoder.py_encode_basestring",
    "signature": "(s)",
    "description": "Return a JSON representation of a Python string"
  },
  "502": {
    "name": "py_encode_basestring_ascii",
    "module": "torch.utils.data.datapipes.utils.decoder.json.encoder",
    "fullName": "torch.utils.data.datapipes.utils.decoder.json.encoder.py_encode_basestring_ascii",
    "signature": "(s)",
    "description": "Return an ASCII-only JSON representation of a Python string"
  },
  "503": {
    "name": "JSONArray",
    "module": "torch.utils.data.datapipes.utils.decoder.json.decoder",
    "fullName": "torch.utils.data.datapipes.utils.decoder.json.decoder.JSONArray",
    "signature": "(s_and_end, scan_once, _w=<built-in method match of re.Pattern object at 0x7fce23c51cf0>, _ws=' \\t\\n\\r')",
    "description": "No description available."
  },
  "504": {
    "name": "JSONObject",
    "module": "torch.utils.data.datapipes.utils.decoder.json.decoder",
    "fullName": "torch.utils.data.datapipes.utils.decoder.json.decoder.JSONObject",
    "signature": "(s_and_end, strict, scan_once, object_hook, object_pairs_hook, memo=None, _w=<built-in method match of re.Pattern object at 0x7fce23c51cf0>, _ws=' \\t\\n\\r')",
    "description": "No description available."
  },
  "505": {
    "name": "py_scanstring",
    "module": "torch.utils.data.datapipes.utils.decoder.json.decoder",
    "fullName": "torch.utils.data.datapipes.utils.decoder.json.decoder.py_scanstring",
    "signature": "(s, end, strict=True, _b={'\"': '\"', '\\\\': '\\\\', '/': '/', 'b': '\\x08', 'f': '\\x0c', 'n': '\\n', 'r': '\\r', 't': '\\t'}, _m=<built-in method match of re.Pattern object at 0x7fce23bab570>)",
    "description": "Scan the string s for a JSON string. End is the index of the"
  },
  "506": {
    "name": "get_file_binaries_from_pathnames",
    "module": "torch.utils.data.datapipes.utils.common",
    "fullName": "torch.utils.data.datapipes.utils.common.get_file_binaries_from_pathnames",
    "signature": "(pathnames: Iterable, mode: str, encoding: Optional[str] = None)",
    "description": "No description available."
  },
  "507": {
    "name": "get_file_pathnames_from_root",
    "module": "torch.utils.data.datapipes.utils.common",
    "fullName": "torch.utils.data.datapipes.utils.common.get_file_pathnames_from_root",
    "signature": "(root: str, masks: Union[str, List[str]], recursive: bool = False, abspath: bool = False, non_deterministic: bool = False) -> Iterable[str]",
    "description": "No description available."
  },
  "508": {
    "name": "match_masks",
    "module": "torch.utils.data.datapipes.utils.common",
    "fullName": "torch.utils.data.datapipes.utils.common.match_masks",
    "signature": "(name: str, masks: Union[str, List[str]]) -> bool",
    "description": "No description available."
  },
  "509": {
    "name": "validate_pathname_binary_tuple",
    "module": "torch.utils.data.datapipes.utils.common",
    "fullName": "torch.utils.data.datapipes.utils.common.validate_pathname_binary_tuple",
    "signature": "(data: Tuple[str, io.IOBase])",
    "description": "No description available."
  },
  "510": {
    "name": "filter",
    "module": "torch.utils.data.datapipes.utils.common.fnmatch",
    "fullName": "torch.utils.data.datapipes.utils.common.fnmatch.filter",
    "signature": "(names, pat)",
    "description": "Construct a list from those elements of the iterable NAMES that match PAT."
  },
  "511": {
    "name": "fnmatch",
    "module": "torch.utils.data.datapipes.utils.common.fnmatch",
    "fullName": "torch.utils.data.datapipes.utils.common.fnmatch.fnmatch",
    "signature": "(name, pat)",
    "description": "Test whether FILENAME matches PATTERN."
  },
  "512": {
    "name": "fnmatchcase",
    "module": "torch.utils.data.datapipes.utils.common.fnmatch",
    "fullName": "torch.utils.data.datapipes.utils.common.fnmatch.fnmatchcase",
    "signature": "(name, pat)",
    "description": "Test whether FILENAME matches PATTERN, including case."
  },
  "513": {
    "name": "translate",
    "module": "torch.utils.data.datapipes.utils.common.fnmatch",
    "fullName": "torch.utils.data.datapipes.utils.common.fnmatch.translate",
    "signature": "(pat)",
    "description": "Translate a shell PATTERN to a regular expression."
  },
  "514": {
    "name": "copy",
    "module": "torch.utils.data.datapipes.map.utils.copy",
    "fullName": "torch.utils.data.datapipes.map.utils.copy.copy",
    "signature": "(x)",
    "description": "Shallow copy operation on arbitrary Python objects."
  },
  "515": {
    "name": "deepcopy",
    "module": "torch.utils.data.datapipes.map.utils.copy",
    "fullName": "torch.utils.data.datapipes.map.utils.copy.deepcopy",
    "signature": "(x, memo=None, _nil=[])",
    "description": "Deep copy operation on arbitrary Python objects."
  },
  "516": {
    "name": "default_fn",
    "module": "torch.utils.data.datapipes.map.callable",
    "fullName": "torch.utils.data.datapipes.map.callable.default_fn",
    "signature": "(data)",
    "description": "No description available."
  },
  "517": {
    "name": "concat",
    "module": "torch.utils.data.datapipes.iter.selecting.df_wrapper",
    "fullName": "torch.utils.data.datapipes.iter.selecting.df_wrapper.concat",
    "signature": "(buffer)",
    "description": "No description available."
  },
  "518": {
    "name": "create_dataframe",
    "module": "torch.utils.data.datapipes.iter.selecting.df_wrapper",
    "fullName": "torch.utils.data.datapipes.iter.selecting.df_wrapper.create_dataframe",
    "signature": "(data, columns=None)",
    "description": "No description available."
  },
  "519": {
    "name": "get_df_wrapper",
    "module": "torch.utils.data.datapipes.iter.selecting.df_wrapper",
    "fullName": "torch.utils.data.datapipes.iter.selecting.df_wrapper.get_df_wrapper",
    "signature": "()",
    "description": "No description available."
  },
  "520": {
    "name": "get_item",
    "module": "torch.utils.data.datapipes.iter.selecting.df_wrapper",
    "fullName": "torch.utils.data.datapipes.iter.selecting.df_wrapper.get_item",
    "signature": "(data, idx)",
    "description": "No description available."
  },
  "521": {
    "name": "get_len",
    "module": "torch.utils.data.datapipes.iter.selecting.df_wrapper",
    "fullName": "torch.utils.data.datapipes.iter.selecting.df_wrapper.get_len",
    "signature": "(df)",
    "description": "No description available."
  },
  "522": {
    "name": "is_column",
    "module": "torch.utils.data.datapipes.iter.selecting.df_wrapper",
    "fullName": "torch.utils.data.datapipes.iter.selecting.df_wrapper.is_column",
    "signature": "(data)",
    "description": "No description available."
  },
  "523": {
    "name": "is_dataframe",
    "module": "torch.utils.data.datapipes.iter.selecting.df_wrapper",
    "fullName": "torch.utils.data.datapipes.iter.selecting.df_wrapper.is_dataframe",
    "signature": "(data)",
    "description": "No description available."
  },
  "524": {
    "name": "iterate",
    "module": "torch.utils.data.datapipes.iter.selecting.df_wrapper",
    "fullName": "torch.utils.data.datapipes.iter.selecting.df_wrapper.iterate",
    "signature": "(data)",
    "description": "No description available."
  },
  "525": {
    "name": "set_df_wrapper",
    "module": "torch.utils.data.datapipes.iter.selecting.df_wrapper",
    "fullName": "torch.utils.data.datapipes.iter.selecting.df_wrapper.set_df_wrapper",
    "signature": "(wrapper)",
    "description": "No description available."
  },
  "526": {
    "name": "decoder_basichandlers",
    "module": "torch.utils.data.datapipes.iter.routeddecoder",
    "fullName": "torch.utils.data.datapipes.iter.routeddecoder.decoder_basichandlers",
    "signature": "(extension, data)",
    "description": "No description available."
  },
  "527": {
    "name": "decoder_imagehandler",
    "module": "torch.utils.data.datapipes.iter.routeddecoder",
    "fullName": "torch.utils.data.datapipes.iter.routeddecoder.decoder_imagehandler",
    "signature": "(imagespec)",
    "description": "No description available."
  },
  "528": {
    "name": "extension_extract_fn",
    "module": "torch.utils.data.datapipes.iter.routeddecoder",
    "fullName": "torch.utils.data.datapipes.iter.routeddecoder.extension_extract_fn",
    "signature": "(pathname)",
    "description": "No description available."
  },
  "529": {
    "name": "get_file_binaries_from_pathnames",
    "module": "torch.utils.data.datapipes.iter.fileopener",
    "fullName": "torch.utils.data.datapipes.iter.fileopener.get_file_binaries_from_pathnames",
    "signature": "(pathnames: Iterable, mode: str, encoding: Optional[str] = None)",
    "description": "No description available."
  },
  "530": {
    "name": "get_file_pathnames_from_root",
    "module": "torch.utils.data.datapipes.iter.filelister",
    "fullName": "torch.utils.data.datapipes.iter.filelister.get_file_pathnames_from_root",
    "signature": "(root: str, masks: Union[str, List[str]], recursive: bool = False, abspath: bool = False, non_deterministic: bool = False) -> Iterable[str]",
    "description": "No description available."
  },
  "531": {
    "name": "default_collate",
    "module": "torch.utils.data.datapipes.iter.callable",
    "fullName": "torch.utils.data.datapipes.iter.callable.default_collate",
    "signature": "(batch)",
    "description": "Function that takes in a batch of data and puts the elements within the batch"
  },
  "532": {
    "name": "get_val",
    "module": "torch.utils.data.datapipes.dataframe.dataframes",
    "fullName": "torch.utils.data.datapipes.dataframe.dataframes.get_val",
    "signature": "(capture)",
    "description": "No description available."
  },
  "533": {
    "name": "DataPipeBehindQueues",
    "module": "torch.utils.data.dataloader_experimental.communication.map",
    "fullName": "torch.utils.data.dataloader_experimental.communication.map.DataPipeBehindQueues",
    "signature": "(source_datapipe, protocol, full_stop=False, blocking_request_get=False)",
    "description": "Indefinitely iterates over req_queue and passing values from source_datapipe to res_queue"
  },
  "534": {
    "name": "EnsureNonBlockingMapDataPipe",
    "module": "torch.utils.data.dataloader_experimental.communication.map",
    "fullName": "torch.utils.data.dataloader_experimental.communication.map.EnsureNonBlockingMapDataPipe",
    "signature": "(validated_datapipe)",
    "description": "No description available."
  },
  "535": {
    "name": "default_not_available_hook",
    "module": "torch.utils.data.dataloader_experimental.communication.map",
    "fullName": "torch.utils.data.dataloader_experimental.communication.map.default_not_available_hook",
    "signature": "()",
    "description": "No description available."
  },
  "536": {
    "name": "DataPipeBehindQueues",
    "module": "torch.utils.data.dataloader_experimental.communication.iter",
    "fullName": "torch.utils.data.dataloader_experimental.communication.iter.DataPipeBehindQueues",
    "signature": "(source_datapipe, protocol, full_stop=False, blocking_request_get=False)",
    "description": "Indefinitely iterates over req_queue and passing values from source_datapipe to res_queue"
  },
  "537": {
    "name": "EnsureNonBlockingDataPipe",
    "module": "torch.utils.data.dataloader_experimental.communication.iter",
    "fullName": "torch.utils.data.dataloader_experimental.communication.iter.EnsureNonBlockingDataPipe",
    "signature": "(validated_datapipe)",
    "description": "No description available."
  },
  "538": {
    "name": "default_not_available_hook",
    "module": "torch.utils.data.dataloader_experimental.communication.iter",
    "fullName": "torch.utils.data.dataloader_experimental.communication.iter.default_not_available_hook",
    "signature": "()",
    "description": "No description available."
  },
  "539": {
    "name": "DataPipeToQueuesLoop",
    "module": "torch.utils.data.dataloader_experimental.communication.eventloop",
    "fullName": "torch.utils.data.dataloader_experimental.communication.eventloop.DataPipeToQueuesLoop",
    "signature": "(source_datapipe, req_queue, res_queue)",
    "description": "No description available."
  },
  "540": {
    "name": "SpawnProcessForDataPipeline",
    "module": "torch.utils.data.dataloader_experimental.communication.eventloop",
    "fullName": "torch.utils.data.dataloader_experimental.communication.eventloop.SpawnProcessForDataPipeline",
    "signature": "(multiprocessing_ctx, datapipe)",
    "description": "No description available."
  },
  "541": {
    "name": "SpawnThreadForDataPipeline",
    "module": "torch.utils.data.dataloader_experimental.communication.eventloop",
    "fullName": "torch.utils.data.dataloader_experimental.communication.eventloop.SpawnThreadForDataPipeline",
    "signature": "(datapipe)",
    "description": "Given a DataPipe, creates a copy of the DataPipe, starts a new Thread with DataPipeToQueuesLoop as target,"
  },
  "542": {
    "name": "default_collate",
    "module": "torch.utils.data.dataloader",
    "fullName": "torch.utils.data.dataloader.default_collate",
    "signature": "(batch)",
    "description": "Function that takes in a batch of data and puts the elements within the batch"
  },
  "543": {
    "name": "default_convert",
    "module": "torch.utils.data.dataloader",
    "fullName": "torch.utils.data.dataloader.default_convert",
    "signature": "(data)",
    "description": "Function that converts each NumPy array element into a :class:`torch.Tensor`. If the input is a `Sequence`,"
  },
  "544": {
    "name": "get_worker_info",
    "module": "torch.utils.data.dataloader",
    "fullName": "torch.utils.data.dataloader.get_worker_info",
    "signature": "()",
    "description": "Returns the information about the current"
  },
  "545": {
    "name": "active_children",
    "module": "torch.utils.data.dataloader.python_multiprocessing",
    "fullName": "torch.utils.data.dataloader.python_multiprocessing.active_children",
    "signature": "()",
    "description": "Return list of process objects corresponding to live child processes"
  },
  "546": {
    "name": "current_process",
    "module": "torch.utils.data.dataloader.python_multiprocessing",
    "fullName": "torch.utils.data.dataloader.python_multiprocessing.current_process",
    "signature": "()",
    "description": "Return process object representing the current process"
  },
  "547": {
    "name": "parent_process",
    "module": "torch.utils.data.dataloader.python_multiprocessing",
    "fullName": "torch.utils.data.dataloader.python_multiprocessing.parent_process",
    "signature": "()",
    "description": "Return process object representing the parent process"
  },
  "548": {
    "name": "close_all_fds_except",
    "module": "torch.utils.data.dataloader.python_multiprocessing.util",
    "fullName": "torch.utils.data.dataloader.python_multiprocessing.util.close_all_fds_except",
    "signature": "(fds)",
    "description": "No description available."
  },
  "549": {
    "name": "close_fds",
    "module": "torch.utils.data.dataloader.python_multiprocessing.util",
    "fullName": "torch.utils.data.dataloader.python_multiprocessing.util.close_fds",
    "signature": "(*fds)",
    "description": "Close each file descriptor given as an argument"
  },
  "550": {
    "name": "debug",
    "module": "torch.utils.data.dataloader.python_multiprocessing.util",
    "fullName": "torch.utils.data.dataloader.python_multiprocessing.util.debug",
    "signature": "(msg, *args)",
    "description": "No description available."
  },
  "551": {
    "name": "get_logger",
    "module": "torch.utils.data.dataloader.python_multiprocessing.util",
    "fullName": "torch.utils.data.dataloader.python_multiprocessing.util.get_logger",
    "signature": "()",
    "description": "Returns logger used by multiprocessing"
  },
  "552": {
    "name": "get_temp_dir",
    "module": "torch.utils.data.dataloader.python_multiprocessing.util",
    "fullName": "torch.utils.data.dataloader.python_multiprocessing.util.get_temp_dir",
    "signature": "()",
    "description": "No description available."
  },
  "553": {
    "name": "info",
    "module": "torch.utils.data.dataloader.python_multiprocessing.util",
    "fullName": "torch.utils.data.dataloader.python_multiprocessing.util.info",
    "signature": "(msg, *args)",
    "description": "No description available."
  },
  "554": {
    "name": "is_abstract_socket_namespace",
    "module": "torch.utils.data.dataloader.python_multiprocessing.util",
    "fullName": "torch.utils.data.dataloader.python_multiprocessing.util.is_abstract_socket_namespace",
    "signature": "(address)",
    "description": "No description available."
  },
  "555": {
    "name": "is_exiting",
    "module": "torch.utils.data.dataloader.python_multiprocessing.util",
    "fullName": "torch.utils.data.dataloader.python_multiprocessing.util.is_exiting",
    "signature": "()",
    "description": "Returns true if the process is shutting down"
  },
  "556": {
    "name": "log_to_stderr",
    "module": "torch.utils.data.dataloader.python_multiprocessing.util",
    "fullName": "torch.utils.data.dataloader.python_multiprocessing.util.log_to_stderr",
    "signature": "(level=None)",
    "description": "Turn on logging and add a handler which prints to stderr"
  },
  "557": {
    "name": "register_after_fork",
    "module": "torch.utils.data.dataloader.python_multiprocessing.util",
    "fullName": "torch.utils.data.dataloader.python_multiprocessing.util.register_after_fork",
    "signature": "(obj, func)",
    "description": "No description available."
  },
  "558": {
    "name": "spawnv_passfds",
    "module": "torch.utils.data.dataloader.python_multiprocessing.util",
    "fullName": "torch.utils.data.dataloader.python_multiprocessing.util.spawnv_passfds",
    "signature": "(path, args, passfds)",
    "description": "No description available."
  },
  "559": {
    "name": "sub_debug",
    "module": "torch.utils.data.dataloader.python_multiprocessing.util",
    "fullName": "torch.utils.data.dataloader.python_multiprocessing.util.sub_debug",
    "signature": "(msg, *args)",
    "description": "No description available."
  },
  "560": {
    "name": "sub_warning",
    "module": "torch.utils.data.dataloader.python_multiprocessing.util",
    "fullName": "torch.utils.data.dataloader.python_multiprocessing.util.sub_warning",
    "signature": "(msg, *args)",
    "description": "No description available."
  },
  "561": {
    "name": "active_children",
    "module": "torch.utils.data.dataloader.python_multiprocessing.util.process",
    "fullName": "torch.utils.data.dataloader.python_multiprocessing.util.process.active_children",
    "signature": "()",
    "description": "Return list of process objects corresponding to live child processes"
  },
  "562": {
    "name": "current_process",
    "module": "torch.utils.data.dataloader.python_multiprocessing.util.process",
    "fullName": "torch.utils.data.dataloader.python_multiprocessing.util.process.current_process",
    "signature": "()",
    "description": "Return process object representing the current process"
  },
  "563": {
    "name": "parent_process",
    "module": "torch.utils.data.dataloader.python_multiprocessing.util.process",
    "fullName": "torch.utils.data.dataloader.python_multiprocessing.util.process.parent_process",
    "signature": "()",
    "description": "Return process object representing the parent process"
  },
  "564": {
    "name": "signum",
    "module": "torch.utils.data.dataloader.python_multiprocessing.util.process",
    "fullName": "torch.utils.data.dataloader.python_multiprocessing.util.process.signum",
    "signature": "(value)",
    "description": "Convert an IntEnum member to a numeric value."
  },
  "565": {
    "name": "getsignal",
    "module": "torch.utils.data.dataloader.python_multiprocessing.util.process.signal",
    "fullName": "torch.utils.data.dataloader.python_multiprocessing.util.process.signal.getsignal",
    "signature": "(signalnum)",
    "description": "Return the current action for the given signal."
  },
  "566": {
    "name": "pthread_sigmask",
    "module": "torch.utils.data.dataloader.python_multiprocessing.util.process.signal",
    "fullName": "torch.utils.data.dataloader.python_multiprocessing.util.process.signal.pthread_sigmask",
    "signature": "(how, mask)",
    "description": "Fetch and/or change the signal mask of the calling thread."
  },
  "567": {
    "name": "signal",
    "module": "torch.utils.data.dataloader.python_multiprocessing.util.process.signal",
    "fullName": "torch.utils.data.dataloader.python_multiprocessing.util.process.signal.signal",
    "signature": "(signalnum, handler)",
    "description": "Set the action for the given signal."
  },
  "568": {
    "name": "sigpending",
    "module": "torch.utils.data.dataloader.python_multiprocessing.util.process.signal",
    "fullName": "torch.utils.data.dataloader.python_multiprocessing.util.process.signal.sigpending",
    "signature": "()",
    "description": "Examine pending signals."
  },
  "569": {
    "name": "sigwait",
    "module": "torch.utils.data.dataloader.python_multiprocessing.util.process.signal",
    "fullName": "torch.utils.data.dataloader.python_multiprocessing.util.process.signal.sigwait",
    "signature": "(sigset)",
    "description": "Wait for a signal."
  },
  "570": {
    "name": "valid_signals",
    "module": "torch.utils.data.dataloader.python_multiprocessing.util.process.signal",
    "fullName": "torch.utils.data.dataloader.python_multiprocessing.util.process.signal.valid_signals",
    "signature": "()",
    "description": "Return a set of valid signal numbers on this platform."
  },
  "571": {
    "name": "create_connection",
    "module": "torch.utils.data.dataloader.python_multiprocessing.resource_sharer.socket",
    "fullName": "torch.utils.data.dataloader.python_multiprocessing.resource_sharer.socket.create_connection",
    "signature": "(address, timeout=<object object at 0x7fcdd1b24220>, source_address=None)",
    "description": "Connect to *address* and return the socket object."
  },
  "572": {
    "name": "create_server",
    "module": "torch.utils.data.dataloader.python_multiprocessing.resource_sharer.socket",
    "fullName": "torch.utils.data.dataloader.python_multiprocessing.resource_sharer.socket.create_server",
    "signature": "(address, *, family=<AddressFamily.AF_INET: 2>, backlog=None, reuse_port=False, dualstack_ipv6=False)",
    "description": "Convenience function which creates a SOCK_STREAM type socket"
  },
  "573": {
    "name": "fromfd",
    "module": "torch.utils.data.dataloader.python_multiprocessing.resource_sharer.socket",
    "fullName": "torch.utils.data.dataloader.python_multiprocessing.resource_sharer.socket.fromfd",
    "signature": "(fd, family, type, proto=0)",
    "description": "fromfd(fd, family, type[, proto]) -> socket object"
  },
  "574": {
    "name": "getaddrinfo",
    "module": "torch.utils.data.dataloader.python_multiprocessing.resource_sharer.socket",
    "fullName": "torch.utils.data.dataloader.python_multiprocessing.resource_sharer.socket.getaddrinfo",
    "signature": "(host, port, family=0, type=0, proto=0, flags=0)",
    "description": "Resolve host and port into list of address info entries."
  },
  "575": {
    "name": "getfqdn",
    "module": "torch.utils.data.dataloader.python_multiprocessing.resource_sharer.socket",
    "fullName": "torch.utils.data.dataloader.python_multiprocessing.resource_sharer.socket.getfqdn",
    "signature": "(name='')",
    "description": "Get fully qualified domain name from name."
  },
  "576": {
    "name": "has_dualstack_ipv6",
    "module": "torch.utils.data.dataloader.python_multiprocessing.resource_sharer.socket",
    "fullName": "torch.utils.data.dataloader.python_multiprocessing.resource_sharer.socket.has_dualstack_ipv6",
    "signature": "()",
    "description": "Return True if the platform supports creating a SOCK_STREAM socket"
  },
  "577": {
    "name": "recv_fds",
    "module": "torch.utils.data.dataloader.python_multiprocessing.resource_sharer.socket",
    "fullName": "torch.utils.data.dataloader.python_multiprocessing.resource_sharer.socket.recv_fds",
    "signature": "(sock, bufsize, maxfds, flags=0)",
    "description": "recv_fds(sock, bufsize, maxfds[, flags]) -> (data, list of file"
  },
  "578": {
    "name": "send_fds",
    "module": "torch.utils.data.dataloader.python_multiprocessing.resource_sharer.socket",
    "fullName": "torch.utils.data.dataloader.python_multiprocessing.resource_sharer.socket.send_fds",
    "signature": "(sock, buffers, fds, flags=0, address=None)",
    "description": "send_fds(sock, buffers, fds[, flags[, address]]) -> integer"
  },
  "579": {
    "name": "socketpair",
    "module": "torch.utils.data.dataloader.python_multiprocessing.resource_sharer.socket",
    "fullName": "torch.utils.data.dataloader.python_multiprocessing.resource_sharer.socket.socketpair",
    "signature": "(family=None, type=<SocketKind.SOCK_STREAM: 1>, proto=0)",
    "description": "socketpair([family[, type[, proto]]]) -> (socket object, socket object)"
  },
  "580": {
    "name": "abstractmethod",
    "module": "torch.utils.data.dataloader.python_multiprocessing.resource_sharer.socket.selectors",
    "fullName": "torch.utils.data.dataloader.python_multiprocessing.resource_sharer.socket.selectors.abstractmethod",
    "signature": "(funcobj)",
    "description": "A decorator indicating abstract methods."
  },
  "581": {
    "name": "namedtuple",
    "module": "torch.utils.data.dataloader.python_multiprocessing.resource_sharer.socket.selectors",
    "fullName": "torch.utils.data.dataloader.python_multiprocessing.resource_sharer.socket.selectors.namedtuple",
    "signature": "(typename, field_names, *, rename=False, defaults=None, module=None)",
    "description": "Returns a new subclass of tuple with named fields."
  },
  "582": {
    "name": "DupFd",
    "module": "torch.utils.data.dataloader.python_multiprocessing.resource_sharer.reduction",
    "fullName": "torch.utils.data.dataloader.python_multiprocessing.resource_sharer.reduction.DupFd",
    "signature": "(fd)",
    "description": "Return a wrapper for an fd."
  },
  "583": {
    "name": "dump",
    "module": "torch.utils.data.dataloader.python_multiprocessing.resource_sharer.reduction",
    "fullName": "torch.utils.data.dataloader.python_multiprocessing.resource_sharer.reduction.dump",
    "signature": "(obj, file, protocol=None)",
    "description": "Replacement for pickle.dump() using ForkingPickler."
  },
  "584": {
    "name": "recv_handle",
    "module": "torch.utils.data.dataloader.python_multiprocessing.resource_sharer.reduction",
    "fullName": "torch.utils.data.dataloader.python_multiprocessing.resource_sharer.reduction.recv_handle",
    "signature": "(conn)",
    "description": "Receive a handle over a local connection."
  },
  "585": {
    "name": "recvfds",
    "module": "torch.utils.data.dataloader.python_multiprocessing.resource_sharer.reduction",
    "fullName": "torch.utils.data.dataloader.python_multiprocessing.resource_sharer.reduction.recvfds",
    "signature": "(sock, size)",
    "description": "Receive an array of fds over an AF_UNIX socket."
  },
  "586": {
    "name": "send_handle",
    "module": "torch.utils.data.dataloader.python_multiprocessing.resource_sharer.reduction",
    "fullName": "torch.utils.data.dataloader.python_multiprocessing.resource_sharer.reduction.send_handle",
    "signature": "(conn, handle, destination_pid)",
    "description": "Send a handle over a local connection."
  },
  "587": {
    "name": "sendfds",
    "module": "torch.utils.data.dataloader.python_multiprocessing.resource_sharer.reduction",
    "fullName": "torch.utils.data.dataloader.python_multiprocessing.resource_sharer.reduction.sendfds",
    "signature": "(sock, fds)",
    "description": "Send an array of fds over an AF_UNIX socket."
  },
  "588": {
    "name": "assert_spawning",
    "module": "torch.utils.data.dataloader.python_multiprocessing.resource_sharer.reduction.context",
    "fullName": "torch.utils.data.dataloader.python_multiprocessing.resource_sharer.reduction.context.assert_spawning",
    "signature": "(obj)",
    "description": "No description available."
  },
  "589": {
    "name": "get_spawning_popen",
    "module": "torch.utils.data.dataloader.python_multiprocessing.resource_sharer.reduction.context",
    "fullName": "torch.utils.data.dataloader.python_multiprocessing.resource_sharer.reduction.context.get_spawning_popen",
    "signature": "()",
    "description": "No description available."
  },
  "590": {
    "name": "set_spawning_popen",
    "module": "torch.utils.data.dataloader.python_multiprocessing.resource_sharer.reduction.context",
    "fullName": "torch.utils.data.dataloader.python_multiprocessing.resource_sharer.reduction.context.set_spawning_popen",
    "signature": "(popen)",
    "description": "No description available."
  },
  "591": {
    "name": "Client",
    "module": "torch.utils.data.dataloader.python_multiprocessing.connection",
    "fullName": "torch.utils.data.dataloader.python_multiprocessing.connection.Client",
    "signature": "(address, family=None, authkey=None)",
    "description": "Returns a connection to the address of a `Listener`"
  },
  "592": {
    "name": "Pipe",
    "module": "torch.utils.data.dataloader.python_multiprocessing.connection",
    "fullName": "torch.utils.data.dataloader.python_multiprocessing.connection.Pipe",
    "signature": "(duplex=True)",
    "description": "Returns pair of connection objects at either end of a pipe"
  },
  "593": {
    "name": "SocketClient",
    "module": "torch.utils.data.dataloader.python_multiprocessing.connection",
    "fullName": "torch.utils.data.dataloader.python_multiprocessing.connection.SocketClient",
    "signature": "(address)",
    "description": "Return a connection object connected to the socket given by `address`"
  },
  "594": {
    "name": "XmlClient",
    "module": "torch.utils.data.dataloader.python_multiprocessing.connection",
    "fullName": "torch.utils.data.dataloader.python_multiprocessing.connection.XmlClient",
    "signature": "(*args, **kwds)",
    "description": "No description available."
  },
  "595": {
    "name": "address_type",
    "module": "torch.utils.data.dataloader.python_multiprocessing.connection",
    "fullName": "torch.utils.data.dataloader.python_multiprocessing.connection.address_type",
    "signature": "(address)",
    "description": "Return the types of the address"
  },
  "596": {
    "name": "answer_challenge",
    "module": "torch.utils.data.dataloader.python_multiprocessing.connection",
    "fullName": "torch.utils.data.dataloader.python_multiprocessing.connection.answer_challenge",
    "signature": "(connection, authkey)",
    "description": "No description available."
  },
  "597": {
    "name": "arbitrary_address",
    "module": "torch.utils.data.dataloader.python_multiprocessing.connection",
    "fullName": "torch.utils.data.dataloader.python_multiprocessing.connection.arbitrary_address",
    "signature": "(family)",
    "description": "Return an arbitrary free address for the given family"
  },
  "598": {
    "name": "deliver_challenge",
    "module": "torch.utils.data.dataloader.python_multiprocessing.connection",
    "fullName": "torch.utils.data.dataloader.python_multiprocessing.connection.deliver_challenge",
    "signature": "(connection, authkey)",
    "description": "No description available."
  },
  "599": {
    "name": "rebuild_connection",
    "module": "torch.utils.data.dataloader.python_multiprocessing.connection",
    "fullName": "torch.utils.data.dataloader.python_multiprocessing.connection.rebuild_connection",
    "signature": "(df, readable, writable)",
    "description": "No description available."
  },
  "600": {
    "name": "reduce_connection",
    "module": "torch.utils.data.dataloader.python_multiprocessing.connection",
    "fullName": "torch.utils.data.dataloader.python_multiprocessing.connection.reduce_connection",
    "signature": "(conn)",
    "description": "No description available."
  },
  "601": {
    "name": "wait",
    "module": "torch.utils.data.dataloader.python_multiprocessing.connection",
    "fullName": "torch.utils.data.dataloader.python_multiprocessing.connection.wait",
    "signature": "(object_list, timeout=None)",
    "description": "Wait till an object in object_list is ready/readable."
  },
  "602": {
    "name": "active_children",
    "module": "torch.utils.data.dataloader.multiprocessing",
    "fullName": "torch.utils.data.dataloader.multiprocessing.active_children",
    "signature": "()",
    "description": "Return list of process objects corresponding to live child processes"
  },
  "603": {
    "name": "current_process",
    "module": "torch.utils.data.dataloader.multiprocessing",
    "fullName": "torch.utils.data.dataloader.multiprocessing.current_process",
    "signature": "()",
    "description": "Return process object representing the current process"
  },
  "604": {
    "name": "get_all_sharing_strategies",
    "module": "torch.utils.data.dataloader.multiprocessing",
    "fullName": "torch.utils.data.dataloader.multiprocessing.get_all_sharing_strategies",
    "signature": "()",
    "description": "Returns a set of sharing strategies supported on a current system."
  },
  "605": {
    "name": "get_sharing_strategy",
    "module": "torch.utils.data.dataloader.multiprocessing",
    "fullName": "torch.utils.data.dataloader.multiprocessing.get_sharing_strategy",
    "signature": "()",
    "description": "Returns the current strategy for sharing CPU tensors."
  },
  "606": {
    "name": "init_reductions",
    "module": "torch.utils.data.dataloader.multiprocessing",
    "fullName": "torch.utils.data.dataloader.multiprocessing.init_reductions",
    "signature": "()",
    "description": "No description available."
  },
  "607": {
    "name": "parent_process",
    "module": "torch.utils.data.dataloader.multiprocessing",
    "fullName": "torch.utils.data.dataloader.multiprocessing.parent_process",
    "signature": "()",
    "description": "Return process object representing the parent process"
  },
  "608": {
    "name": "set_sharing_strategy",
    "module": "torch.utils.data.dataloader.multiprocessing",
    "fullName": "torch.utils.data.dataloader.multiprocessing.set_sharing_strategy",
    "signature": "(new_strategy)",
    "description": "Sets the strategy for sharing CPU tensors."
  },
  "609": {
    "name": "start_processes",
    "module": "torch.utils.data.dataloader.multiprocessing",
    "fullName": "torch.utils.data.dataloader.multiprocessing.start_processes",
    "signature": "(fn, args=(), nprocs=1, join=True, daemon=False, start_method='spawn')",
    "description": "No description available."
  },
  "610": {
    "name": "check_serializing_named_tensor",
    "module": "torch.utils.data.dataloader.multiprocessing.reductions",
    "fullName": "torch.utils.data.dataloader.multiprocessing.reductions.check_serializing_named_tensor",
    "signature": "(tensor)",
    "description": "No description available."
  },
  "611": {
    "name": "fd_id",
    "module": "torch.utils.data.dataloader.multiprocessing.reductions",
    "fullName": "torch.utils.data.dataloader.multiprocessing.reductions.fd_id",
    "signature": "(fd)",
    "description": "No description available."
  },
  "612": {
    "name": "init_reductions",
    "module": "torch.utils.data.dataloader.multiprocessing.reductions",
    "fullName": "torch.utils.data.dataloader.multiprocessing.reductions.init_reductions",
    "signature": "()",
    "description": "No description available."
  },
  "613": {
    "name": "rebuild_cuda_tensor",
    "module": "torch.utils.data.dataloader.multiprocessing.reductions",
    "fullName": "torch.utils.data.dataloader.multiprocessing.reductions.rebuild_cuda_tensor",
    "signature": "(tensor_cls, tensor_size, tensor_stride, tensor_offset, storage_cls, dtype, storage_device, storage_handle, storage_size_bytes, storage_offset_bytes, requires_grad, ref_counter_handle, ref_counter_offset, event_handle, event_sync_required)",
    "description": "No description available."
  },
  "614": {
    "name": "rebuild_event",
    "module": "torch.utils.data.dataloader.multiprocessing.reductions",
    "fullName": "torch.utils.data.dataloader.multiprocessing.reductions.rebuild_event",
    "signature": "(device, handle)",
    "description": "No description available."
  },
  "615": {
    "name": "rebuild_storage_empty",
    "module": "torch.utils.data.dataloader.multiprocessing.reductions",
    "fullName": "torch.utils.data.dataloader.multiprocessing.reductions.rebuild_storage_empty",
    "signature": "(cls)",
    "description": "No description available."
  },
  "616": {
    "name": "rebuild_storage_fd",
    "module": "torch.utils.data.dataloader.multiprocessing.reductions",
    "fullName": "torch.utils.data.dataloader.multiprocessing.reductions.rebuild_storage_fd",
    "signature": "(cls, df, size)",
    "description": "No description available."
  },
  "617": {
    "name": "rebuild_storage_filename",
    "module": "torch.utils.data.dataloader.multiprocessing.reductions",
    "fullName": "torch.utils.data.dataloader.multiprocessing.reductions.rebuild_storage_filename",
    "signature": "(cls, manager, handle, size, dtype=None)",
    "description": "No description available."
  },
  "618": {
    "name": "rebuild_tensor",
    "module": "torch.utils.data.dataloader.multiprocessing.reductions",
    "fullName": "torch.utils.data.dataloader.multiprocessing.reductions.rebuild_tensor",
    "signature": "(cls, storage, metadata)",
    "description": "No description available."
  },
  "619": {
    "name": "rebuild_typed_storage",
    "module": "torch.utils.data.dataloader.multiprocessing.reductions",
    "fullName": "torch.utils.data.dataloader.multiprocessing.reductions.rebuild_typed_storage",
    "signature": "(storage, dtype)",
    "description": "No description available."
  },
  "620": {
    "name": "rebuild_typed_storage_child",
    "module": "torch.utils.data.dataloader.multiprocessing.reductions",
    "fullName": "torch.utils.data.dataloader.multiprocessing.reductions.rebuild_typed_storage_child",
    "signature": "(storage, storage_type)",
    "description": "No description available."
  },
  "621": {
    "name": "reduce_event",
    "module": "torch.utils.data.dataloader.multiprocessing.reductions",
    "fullName": "torch.utils.data.dataloader.multiprocessing.reductions.reduce_event",
    "signature": "(event)",
    "description": "No description available."
  },
  "622": {
    "name": "reduce_storage",
    "module": "torch.utils.data.dataloader.multiprocessing.reductions",
    "fullName": "torch.utils.data.dataloader.multiprocessing.reductions.reduce_storage",
    "signature": "(storage)",
    "description": "No description available."
  },
  "623": {
    "name": "reduce_tensor",
    "module": "torch.utils.data.dataloader.multiprocessing.reductions",
    "fullName": "torch.utils.data.dataloader.multiprocessing.reductions.reduce_tensor",
    "signature": "(tensor)",
    "description": "No description available."
  },
  "624": {
    "name": "reduce_typed_storage",
    "module": "torch.utils.data.dataloader.multiprocessing.reductions",
    "fullName": "torch.utils.data.dataloader.multiprocessing.reductions.reduce_typed_storage",
    "signature": "(storage)",
    "description": "No description available."
  },
  "625": {
    "name": "reduce_typed_storage_child",
    "module": "torch.utils.data.dataloader.multiprocessing.reductions",
    "fullName": "torch.utils.data.dataloader.multiprocessing.reductions.reduce_typed_storage_child",
    "signature": "(storage)",
    "description": "No description available."
  },
  "626": {
    "name": "register_after_fork",
    "module": "torch.utils.data.dataloader.multiprocessing.reductions",
    "fullName": "torch.utils.data.dataloader.multiprocessing.reductions.register_after_fork",
    "signature": "(obj, func)",
    "description": "No description available."
  },
  "627": {
    "name": "storage_from_cache",
    "module": "torch.utils.data.dataloader.multiprocessing.reductions",
    "fullName": "torch.utils.data.dataloader.multiprocessing.reductions.storage_from_cache",
    "signature": "(cls, key)",
    "description": "No description available."
  },
  "628": {
    "name": "worker_init_fn",
    "module": "torch.utils.data.backward_compatibility",
    "fullName": "torch.utils.data.backward_compatibility.worker_init_fn",
    "signature": "(worker_id)",
    "description": "No description available."
  },
  "629": {
    "name": "Device",
    "module": "torch.types",
    "fullName": "torch.types.Device",
    "signature": "(*args, **kwargs)",
    "description": "No description available."
  },
  "630": {
    "name": "Number",
    "module": "torch.types",
    "fullName": "torch.types.Number",
    "signature": "(*args, **kwargs)",
    "description": "No description available."
  },
  "631": {
    "name": "dedent",
    "module": "torch.textwrap",
    "fullName": "torch.textwrap.dedent",
    "signature": "(text)",
    "description": "Remove any common leading whitespace from every line in `text`."
  },
  "632": {
    "name": "fill",
    "module": "torch.textwrap",
    "fullName": "torch.textwrap.fill",
    "signature": "(text, width=70, **kwargs)",
    "description": "Fill a single paragraph of text, returning a new string."
  },
  "633": {
    "name": "indent",
    "module": "torch.textwrap",
    "fullName": "torch.textwrap.indent",
    "signature": "(text, prefix, predicate=None)",
    "description": "Adds 'prefix' to the beginning of selected lines in 'text'."
  },
  "634": {
    "name": "shorten",
    "module": "torch.textwrap",
    "fullName": "torch.textwrap.shorten",
    "signature": "(text, width, **kwargs)",
    "description": "Collapse and truncate the given text to fit in the given width."
  },
  "635": {
    "name": "wrap",
    "module": "torch.textwrap",
    "fullName": "torch.textwrap.wrap",
    "signature": "(text, width=70, **kwargs)",
    "description": "Wrap a single paragraph of text, returning a list of wrapped lines."
  },
  "636": {
    "name": "cast",
    "module": "torch.storage",
    "fullName": "torch.storage.cast",
    "signature": "(typ, val)",
    "description": "Cast a value to a type."
  },
  "637": {
    "name": "lru_cache",
    "module": "torch.storage",
    "fullName": "torch.storage.lru_cache",
    "signature": "(maxsize=128, typed=False)",
    "description": "Least-recently-used cache decorator."
  },
  "638": {
    "name": "add_newdoc",
    "module": "torch.storage.np",
    "fullName": "torch.storage.np.add_newdoc",
    "signature": "(place, obj, doc, warn_on_python=True)",
    "description": "Add documentation to an existing object, typically one defined in C"
  },
  "639": {
    "name": "all",
    "module": "torch.storage.np",
    "fullName": "torch.storage.np.all",
    "signature": "(a, axis=None, out=None, keepdims=<no value>, *, where=<no value>)",
    "description": "Test whether all array elements along a given axis evaluate to True."
  },
  "640": {
    "name": "allclose",
    "module": "torch.storage.np",
    "fullName": "torch.storage.np.allclose",
    "signature": "(a, b, rtol=1e-05, atol=1e-08, equal_nan=False)",
    "description": "Returns True if two arrays are element-wise equal within a tolerance."
  },
  "641": {
    "name": "alltrue",
    "module": "torch.storage.np",
    "fullName": "torch.storage.np.alltrue",
    "signature": "(*args, **kwargs)",
    "description": "Check if all elements of input array are true."
  },
  "642": {
    "name": "amax",
    "module": "torch.storage.np",
    "fullName": "torch.storage.np.amax",
    "signature": "(a, axis=None, out=None, keepdims=<no value>, initial=<no value>, where=<no value>)",
    "description": "Return the maximum of an array or maximum along an axis."
  },
  "643": {
    "name": "amin",
    "module": "torch.storage.np",
    "fullName": "torch.storage.np.amin",
    "signature": "(a, axis=None, out=None, keepdims=<no value>, initial=<no value>, where=<no value>)",
    "description": "Return the minimum of an array or minimum along an axis."
  },
  "644": {
    "name": "angle",
    "module": "torch.storage.np",
    "fullName": "torch.storage.np.angle",
    "signature": "(z, deg=False)",
    "description": "Return the angle of the complex argument."
  },
  "645": {
    "name": "any",
    "module": "torch.storage.np",
    "fullName": "torch.storage.np.any",
    "signature": "(a, axis=None, out=None, keepdims=<no value>, *, where=<no value>)",
    "description": "Test whether any array element along a given axis evaluates to True."
  },
  "646": {
    "name": "append",
    "module": "torch.storage.np",
    "fullName": "torch.storage.np.append",
    "signature": "(arr, values, axis=None)",
    "description": "Append values to the end of an array."
  },
  "647": {
    "name": "apply_along_axis",
    "module": "torch.storage.np",
    "fullName": "torch.storage.np.apply_along_axis",
    "signature": "(func1d, axis, arr, *args, **kwargs)",
    "description": "Apply a function to 1-D slices along the given axis."
  },
  "648": {
    "name": "apply_over_axes",
    "module": "torch.storage.np",
    "fullName": "torch.storage.np.apply_over_axes",
    "signature": "(func, a, axes)",
    "description": "Apply a function repeatedly over multiple axes."
  },
  "649": {
    "name": "argmax",
    "module": "torch.storage.np",
    "fullName": "torch.storage.np.argmax",
    "signature": "(a, axis=None, out=None, *, keepdims=<no value>)",
    "description": "Returns the indices of the maximum values along an axis."
  },
  "650": {
    "name": "argmin",
    "module": "torch.storage.np",
    "fullName": "torch.storage.np.argmin",
    "signature": "(a, axis=None, out=None, *, keepdims=<no value>)",
    "description": "Returns the indices of the minimum values along an axis."
  },
  "651": {
    "name": "argsort",
    "module": "torch.storage.np",
    "fullName": "torch.storage.np.argsort",
    "signature": "(a, axis=-1, kind=None, order=None)",
    "description": "Returns the indices that would sort an array."
  },
  "652": {
    "name": "argwhere",
    "module": "torch.storage.np",
    "fullName": "torch.storage.np.argwhere",
    "signature": "(a)",
    "description": "Find the indices of array elements that are non-zero, grouped by element."
  },
  "653": {
    "name": "around",
    "module": "torch.storage.np",
    "fullName": "torch.storage.np.around",
    "signature": "(a, decimals=0, out=None)",
    "description": "Evenly round to the given number of decimals."
  },
  "654": {
    "name": "array_equal",
    "module": "torch.storage.np",
    "fullName": "torch.storage.np.array_equal",
    "signature": "(a1, a2, equal_nan=False)",
    "description": "True if two arrays have the same shape and elements, False otherwise."
  },
  "655": {
    "name": "array_equiv",
    "module": "torch.storage.np",
    "fullName": "torch.storage.np.array_equiv",
    "signature": "(a1, a2)",
    "description": "Returns True if input arrays are shape consistent and all elements equal."
  },
  "656": {
    "name": "array_repr",
    "module": "torch.storage.np",
    "fullName": "torch.storage.np.array_repr",
    "signature": "(arr, max_line_width=None, precision=None, suppress_small=None)",
    "description": "Return the string representation of an array."
  },
  "657": {
    "name": "array_split",
    "module": "torch.storage.np",
    "fullName": "torch.storage.np.array_split",
    "signature": "(ary, indices_or_sections, axis=0)",
    "description": "Split an array into multiple sub-arrays."
  },
  "658": {
    "name": "array_str",
    "module": "torch.storage.np",
    "fullName": "torch.storage.np.array_str",
    "signature": "(a, max_line_width=None, precision=None, suppress_small=None)",
    "description": "Return a string representation of the data in an array."
  },
  "659": {
    "name": "asarray_chkfinite",
    "module": "torch.storage.np",
    "fullName": "torch.storage.np.asarray_chkfinite",
    "signature": "(a, dtype=None, order=None)",
    "description": "Convert the input to an array, checking for NaNs or Infs."
  },
  "660": {
    "name": "asfarray",
    "module": "torch.storage.np",
    "fullName": "torch.storage.np.asfarray",
    "signature": "(a, dtype=<class 'numpy.float64'>)",
    "description": "Return an array converted to a float type."
  },
  "661": {
    "name": "asmatrix",
    "module": "torch.storage.np",
    "fullName": "torch.storage.np.asmatrix",
    "signature": "(data, dtype=None)",
    "description": "Interpret the input as a matrix."
  },
  "662": {
    "name": "atleast_1d",
    "module": "torch.storage.np",
    "fullName": "torch.storage.np.atleast_1d",
    "signature": "(*arys)",
    "description": "Convert inputs to arrays with at least one dimension."
  },
  "663": {
    "name": "atleast_2d",
    "module": "torch.storage.np",
    "fullName": "torch.storage.np.atleast_2d",
    "signature": "(*arys)",
    "description": "View inputs as arrays with at least two dimensions."
  },
  "664": {
    "name": "atleast_3d",
    "module": "torch.storage.np",
    "fullName": "torch.storage.np.atleast_3d",
    "signature": "(*arys)",
    "description": "View inputs as arrays with at least three dimensions."
  },
  "665": {
    "name": "average",
    "module": "torch.storage.np",
    "fullName": "torch.storage.np.average",
    "signature": "(a, axis=None, weights=None, returned=False, *, keepdims=<no value>)",
    "description": "Compute the weighted average along the specified axis."
  },
  "666": {
    "name": "bartlett",
    "module": "torch.storage.np",
    "fullName": "torch.storage.np.bartlett",
    "signature": "(M)",
    "description": "Return the Bartlett window."
  },
  "667": {
    "name": "base_repr",
    "module": "torch.storage.np",
    "fullName": "torch.storage.np.base_repr",
    "signature": "(number, base=2, padding=0)",
    "description": "Return a string representation of a number in the given base system."
  },
  "668": {
    "name": "bincount",
    "module": "torch.storage.np",
    "fullName": "torch.storage.np.bincount",
    "signature": "N/A",
    "description": "bincount(x, /, weights=None, minlength=0)"
  },
  "669": {
    "name": "blackman",
    "module": "torch.storage.np",
    "fullName": "torch.storage.np.blackman",
    "signature": "(M)",
    "description": "Return the Blackman window."
  },
  "670": {
    "name": "block",
    "module": "torch.storage.np",
    "fullName": "torch.storage.np.block",
    "signature": "(arrays)",
    "description": "Assemble an nd-array from nested lists of blocks."
  },
  "671": {
    "name": "bmat",
    "module": "torch.storage.np",
    "fullName": "torch.storage.np.bmat",
    "signature": "(obj, ldict=None, gdict=None)",
    "description": "Build a matrix object from a string, nested sequence, or array."
  },
  "672": {
    "name": "broadcast_shapes",
    "module": "torch.storage.np",
    "fullName": "torch.storage.np.broadcast_shapes",
    "signature": "(*args)",
    "description": "Broadcast the input shapes into a single shape."
  },
  "673": {
    "name": "broadcast_to",
    "module": "torch.storage.np",
    "fullName": "torch.storage.np.broadcast_to",
    "signature": "(array, shape, subok=False)",
    "description": "Broadcast an array to a new shape."
  },
  "674": {
    "name": "busday_count",
    "module": "torch.storage.np",
    "fullName": "torch.storage.np.busday_count",
    "signature": "N/A",
    "description": "busday_count(begindates, enddates, weekmask='1111100', holidays=[], busdaycal=None, out=None)"
  },
  "675": {
    "name": "busday_offset",
    "module": "torch.storage.np",
    "fullName": "torch.storage.np.busday_offset",
    "signature": "N/A",
    "description": "busday_offset(dates, offsets, roll='raise', weekmask='1111100', holidays=None, busdaycal=None, out=None)"
  },
  "676": {
    "name": "byte_bounds",
    "module": "torch.storage.np",
    "fullName": "torch.storage.np.byte_bounds",
    "signature": "(a)",
    "description": "Returns pointers to the end-points of an array."
  },
  "677": {
    "name": "can_cast",
    "module": "torch.storage.np",
    "fullName": "torch.storage.np.can_cast",
    "signature": "N/A",
    "description": "can_cast(from_, to, casting='safe')"
  },
  "678": {
    "name": "choose",
    "module": "torch.storage.np",
    "fullName": "torch.storage.np.choose",
    "signature": "(a, choices, out=None, mode='raise')",
    "description": "Construct an array from an index array and a list of arrays to choose from."
  },
  "679": {
    "name": "clip",
    "module": "torch.storage.np",
    "fullName": "torch.storage.np.clip",
    "signature": "(a, a_min, a_max, out=None, **kwargs)",
    "description": "Clip (limit) the values in an array."
  },
  "680": {
    "name": "column_stack",
    "module": "torch.storage.np",
    "fullName": "torch.storage.np.column_stack",
    "signature": "(tup)",
    "description": "Stack 1-D arrays as columns into a 2-D array."
  },
  "681": {
    "name": "common_type",
    "module": "torch.storage.np",
    "fullName": "torch.storage.np.common_type",
    "signature": "(*arrays)",
    "description": "Return a scalar type which is common to the input arrays."
  },
  "682": {
    "name": "compress",
    "module": "torch.storage.np",
    "fullName": "torch.storage.np.compress",
    "signature": "(condition, a, axis=None, out=None)",
    "description": "Return selected slices of an array along given axis."
  },
  "683": {
    "name": "concatenate",
    "module": "torch.storage.np",
    "fullName": "torch.storage.np.concatenate",
    "signature": "N/A",
    "description": "concatenate((a1, a2, ...), axis=0, out=None, dtype=None, casting=\"same_kind\")"
  },
  "684": {
    "name": "convolve",
    "module": "torch.storage.np",
    "fullName": "torch.storage.np.convolve",
    "signature": "(a, v, mode='full')",
    "description": "Returns the discrete, linear convolution of two one-dimensional sequences."
  },
  "685": {
    "name": "copy",
    "module": "torch.storage.np",
    "fullName": "torch.storage.np.copy",
    "signature": "(a, order='K', subok=False)",
    "description": "Return an array copy of the given object."
  },
  "686": {
    "name": "copyto",
    "module": "torch.storage.np",
    "fullName": "torch.storage.np.copyto",
    "signature": "N/A",
    "description": "copyto(dst, src, casting='same_kind', where=True)"
  },
  "687": {
    "name": "correlate",
    "module": "torch.storage.np",
    "fullName": "torch.storage.np.correlate",
    "signature": "(a, v, mode='valid')",
    "description": "Cross-correlation of two 1-dimensional sequences."
  },
  "688": {
    "name": "count_nonzero",
    "module": "torch.storage.np",
    "fullName": "torch.storage.np.count_nonzero",
    "signature": "(a, axis=None, *, keepdims=False)",
    "description": "Counts the number of non-zero values in the array ``a``."
  },
  "689": {
    "name": "cov",
    "module": "torch.storage.np",
    "fullName": "torch.storage.np.cov",
    "signature": "(m, y=None, rowvar=True, bias=False, ddof=None, fweights=None, aweights=None, *, dtype=None)",
    "description": "Estimate a covariance matrix, given data and weights."
  },
  "690": {
    "name": "cross",
    "module": "torch.storage.np",
    "fullName": "torch.storage.np.cross",
    "signature": "(a, b, axisa=-1, axisb=-1, axisc=-1, axis=None)",
    "description": "Return the cross product of two (arrays of) vectors."
  },
  "691": {
    "name": "cumprod",
    "module": "torch.storage.np",
    "fullName": "torch.storage.np.cumprod",
    "signature": "(a, axis=None, dtype=None, out=None)",
    "description": "Return the cumulative product of elements along a given axis."
  },
  "692": {
    "name": "cumproduct",
    "module": "torch.storage.np",
    "fullName": "torch.storage.np.cumproduct",
    "signature": "(*args, **kwargs)",
    "description": "Return the cumulative product over the given axis."
  },
  "693": {
    "name": "cumsum",
    "module": "torch.storage.np",
    "fullName": "torch.storage.np.cumsum",
    "signature": "(a, axis=None, dtype=None, out=None)",
    "description": "Return the cumulative sum of the elements along a given axis."
  },
  "694": {
    "name": "datetime_as_string",
    "module": "torch.storage.np",
    "fullName": "torch.storage.np.datetime_as_string",
    "signature": "N/A",
    "description": "datetime_as_string(arr, unit=None, timezone='naive', casting='same_kind')"
  },
  "695": {
    "name": "delete",
    "module": "torch.storage.np",
    "fullName": "torch.storage.np.delete",
    "signature": "(arr, obj, axis=None)",
    "description": "Return a new array with sub-arrays along an axis deleted. For a one"
  },
  "696": {
    "name": "deprecate_with_doc",
    "module": "torch.storage.np",
    "fullName": "torch.storage.np.deprecate_with_doc",
    "signature": "(msg)",
    "description": "Deprecates a function and includes the deprecation in its docstring."
  },
  "697": {
    "name": "diag",
    "module": "torch.storage.np",
    "fullName": "torch.storage.np.diag",
    "signature": "(v, k=0)",
    "description": "Extract a diagonal or construct a diagonal array."
  },
  "698": {
    "name": "diag_indices",
    "module": "torch.storage.np",
    "fullName": "torch.storage.np.diag_indices",
    "signature": "(n, ndim=2)",
    "description": "Return the indices to access the main diagonal of an array."
  },
  "699": {
    "name": "diag_indices_from",
    "module": "torch.storage.np",
    "fullName": "torch.storage.np.diag_indices_from",
    "signature": "(arr)",
    "description": "Return the indices to access the main diagonal of an n-dimensional array."
  },
  "700": {
    "name": "diagflat",
    "module": "torch.storage.np",
    "fullName": "torch.storage.np.diagflat",
    "signature": "(v, k=0)",
    "description": "Create a two-dimensional array with the flattened input as a diagonal."
  },
  "701": {
    "name": "diff",
    "module": "torch.storage.np",
    "fullName": "torch.storage.np.diff",
    "signature": "(a, n=1, axis=-1, prepend=<no value>, append=<no value>)",
    "description": "Calculate the n-th discrete difference along the given axis."
  },
  "702": {
    "name": "digitize",
    "module": "torch.storage.np",
    "fullName": "torch.storage.np.digitize",
    "signature": "(x, bins, right=False)",
    "description": "Return the indices of the bins to which each value in input array belongs."
  },
  "703": {
    "name": "disp",
    "module": "torch.storage.np",
    "fullName": "torch.storage.np.disp",
    "signature": "(mesg, device=None, linefeed=True)",
    "description": "Display a message on a device."
  },
  "704": {
    "name": "dot",
    "module": "torch.storage.np",
    "fullName": "torch.storage.np.dot",
    "signature": "N/A",
    "description": "dot(a, b, out=None)"
  },
  "705": {
    "name": "dsplit",
    "module": "torch.storage.np",
    "fullName": "torch.storage.np.dsplit",
    "signature": "(ary, indices_or_sections)",
    "description": "Split array into multiple sub-arrays along the 3rd axis (depth)."
  },
  "706": {
    "name": "dstack",
    "module": "torch.storage.np",
    "fullName": "torch.storage.np.dstack",
    "signature": "(tup)",
    "description": "Stack arrays in sequence depth wise (along third axis)."
  },
  "707": {
    "name": "ediff1d",
    "module": "torch.storage.np",
    "fullName": "torch.storage.np.ediff1d",
    "signature": "(ary, to_end=None, to_begin=None)",
    "description": "The differences between consecutive elements of an array."
  },
  "708": {
    "name": "einsum",
    "module": "torch.storage.np",
    "fullName": "torch.storage.np.einsum",
    "signature": "(*operands, out=None, optimize=False, **kwargs)",
    "description": "einsum(subscripts, *operands, out=None, dtype=None, order='K',"
  },
  "709": {
    "name": "einsum_path",
    "module": "torch.storage.np",
    "fullName": "torch.storage.np.einsum_path",
    "signature": "(*operands, optimize='greedy', einsum_call=False)",
    "description": "einsum_path(subscripts, *operands, optimize='greedy')"
  },
  "710": {
    "name": "empty_like",
    "module": "torch.storage.np",
    "fullName": "torch.storage.np.empty_like",
    "signature": "N/A",
    "description": "empty_like(prototype, dtype=None, order='K', subok=True, shape=None)"
  },
  "711": {
    "name": "extract",
    "module": "torch.storage.np",
    "fullName": "torch.storage.np.extract",
    "signature": "(condition, arr)",
    "description": "Return the elements of an array that satisfy some condition."
  },
  "712": {
    "name": "eye",
    "module": "torch.storage.np",
    "fullName": "torch.storage.np.eye",
    "signature": "(N, M=None, k=0, dtype=<class 'float'>, order='C', *, like=None)",
    "description": "Return a 2-D array with ones on the diagonal and zeros elsewhere."
  },
  "713": {
    "name": "fill_diagonal",
    "module": "torch.storage.np",
    "fullName": "torch.storage.np.fill_diagonal",
    "signature": "(a, val, wrap=False)",
    "description": "Fill the main diagonal of the given array of any dimensionality."
  },
  "714": {
    "name": "find_common_type",
    "module": "torch.storage.np",
    "fullName": "torch.storage.np.find_common_type",
    "signature": "(array_types, scalar_types)",
    "description": "Determine common type following standard coercion rules."
  },
  "715": {
    "name": "fix",
    "module": "torch.storage.np",
    "fullName": "torch.storage.np.fix",
    "signature": "(x, out=None)",
    "description": "Round to nearest integer towards zero."
  },
  "716": {
    "name": "flatnonzero",
    "module": "torch.storage.np",
    "fullName": "torch.storage.np.flatnonzero",
    "signature": "(a)",
    "description": "Return indices that are non-zero in the flattened version of a."
  },
  "717": {
    "name": "flip",
    "module": "torch.storage.np",
    "fullName": "torch.storage.np.flip",
    "signature": "(m, axis=None)",
    "description": "Reverse the order of elements in an array along the given axis."
  },
  "718": {
    "name": "fliplr",
    "module": "torch.storage.np",
    "fullName": "torch.storage.np.fliplr",
    "signature": "(m)",
    "description": "Reverse the order of elements along axis 1 (left/right)."
  },
  "719": {
    "name": "flipud",
    "module": "torch.storage.np",
    "fullName": "torch.storage.np.flipud",
    "signature": "(m)",
    "description": "Reverse the order of elements along axis 0 (up/down)."
  },
  "720": {
    "name": "format_float_positional",
    "module": "torch.storage.np",
    "fullName": "torch.storage.np.format_float_positional",
    "signature": "(x, precision=None, unique=True, fractional=True, trim='k', sign=False, pad_left=None, pad_right=None, min_digits=None)",
    "description": "Format a floating-point scalar as a decimal string in positional notation."
  },
  "721": {
    "name": "format_float_scientific",
    "module": "torch.storage.np",
    "fullName": "torch.storage.np.format_float_scientific",
    "signature": "(x, precision=None, unique=True, trim='k', sign=False, pad_left=None, exp_digits=None, min_digits=None)",
    "description": "Format a floating-point scalar as a decimal string in scientific notation."
  },
  "722": {
    "name": "fromfunction",
    "module": "torch.storage.np",
    "fullName": "torch.storage.np.fromfunction",
    "signature": "(function, shape, *, dtype=<class 'float'>, like=None, **kwargs)",
    "description": "Construct an array by executing a function over each coordinate."
  },
  "723": {
    "name": "fromregex",
    "module": "torch.storage.np",
    "fullName": "torch.storage.np.fromregex",
    "signature": "(file, regexp, dtype, encoding=None)",
    "description": "Construct an array from a text file, using regular expression parsing."
  },
  "724": {
    "name": "full",
    "module": "torch.storage.np",
    "fullName": "torch.storage.np.full",
    "signature": "(shape, fill_value, dtype=None, order='C', *, like=None)",
    "description": "Return a new array of given shape and type, filled with `fill_value`."
  },
  "725": {
    "name": "full_like",
    "module": "torch.storage.np",
    "fullName": "torch.storage.np.full_like",
    "signature": "(a, fill_value, dtype=None, order='K', subok=True, shape=None)",
    "description": "Return a full array with the same shape and type as a given array."
  },
  "726": {
    "name": "genfromtxt",
    "module": "torch.storage.np",
    "fullName": "torch.storage.np.genfromtxt",
    "signature": "(fname, dtype=<class 'float'>, comments='#', delimiter=None, skip_header=0, skip_footer=0, converters=None, missing_values=None, filling_values=None, usecols=None, names=None, excludelist=None, deletechars=\" !#$%&'()*+,-./:;<=>?@[\\\\]^{|}~\", replace_space='_', autostrip=False, case_sensitive=True, defaultfmt='f%i', unpack=None, usemask=False, loose=True, invalid_raise=True, max_rows=None, encoding='bytes', *, ndmin=0, like=None)",
    "description": "Load data from a text file, with missing values handled as specified."
  },
  "727": {
    "name": "geomspace",
    "module": "torch.storage.np",
    "fullName": "torch.storage.np.geomspace",
    "signature": "(start, stop, num=50, endpoint=True, dtype=None, axis=0)",
    "description": "Return numbers spaced evenly on a log scale (a geometric progression)."
  },
  "728": {
    "name": "get_array_wrap",
    "module": "torch.storage.np",
    "fullName": "torch.storage.np.get_array_wrap",
    "signature": "(*args)",
    "description": "Find the wrapper for the array with the highest priority."
  },
  "729": {
    "name": "get_include",
    "module": "torch.storage.np",
    "fullName": "torch.storage.np.get_include",
    "signature": "()",
    "description": "Return the directory that contains the NumPy \\*.h header files."
  },
  "730": {
    "name": "get_printoptions",
    "module": "torch.storage.np",
    "fullName": "torch.storage.np.get_printoptions",
    "signature": "()",
    "description": "Return the current print options."
  },
  "731": {
    "name": "getbufsize",
    "module": "torch.storage.np",
    "fullName": "torch.storage.np.getbufsize",
    "signature": "()",
    "description": "Return the size of the buffer used in ufuncs."
  },
  "732": {
    "name": "geterr",
    "module": "torch.storage.np",
    "fullName": "torch.storage.np.geterr",
    "signature": "()",
    "description": "Get the current way of handling floating-point errors."
  },
  "733": {
    "name": "geterrcall",
    "module": "torch.storage.np",
    "fullName": "torch.storage.np.geterrcall",
    "signature": "()",
    "description": "Return the current callback function used on floating-point errors."
  },
  "734": {
    "name": "gradient",
    "module": "torch.storage.np",
    "fullName": "torch.storage.np.gradient",
    "signature": "(f, *varargs, axis=None, edge_order=1)",
    "description": "Return the gradient of an N-dimensional array."
  },
  "735": {
    "name": "hamming",
    "module": "torch.storage.np",
    "fullName": "torch.storage.np.hamming",
    "signature": "(M)",
    "description": "Return the Hamming window."
  },
  "736": {
    "name": "hanning",
    "module": "torch.storage.np",
    "fullName": "torch.storage.np.hanning",
    "signature": "(M)",
    "description": "Return the Hanning window."
  },
  "737": {
    "name": "histogram",
    "module": "torch.storage.np",
    "fullName": "torch.storage.np.histogram",
    "signature": "(a, bins=10, range=None, density=None, weights=None)",
    "description": "Compute the histogram of a dataset."
  },
  "738": {
    "name": "histogram2d",
    "module": "torch.storage.np",
    "fullName": "torch.storage.np.histogram2d",
    "signature": "(x, y, bins=10, range=None, density=None, weights=None)",
    "description": "Compute the bi-dimensional histogram of two data samples."
  },
  "739": {
    "name": "histogram_bin_edges",
    "module": "torch.storage.np",
    "fullName": "torch.storage.np.histogram_bin_edges",
    "signature": "(a, bins=10, range=None, weights=None)",
    "description": "Function to calculate only the edges of the bins used by the `histogram`"
  },
  "740": {
    "name": "histogramdd",
    "module": "torch.storage.np",
    "fullName": "torch.storage.np.histogramdd",
    "signature": "(sample, bins=10, range=None, density=None, weights=None)",
    "description": "Compute the multidimensional histogram of some data."
  },
  "741": {
    "name": "hsplit",
    "module": "torch.storage.np",
    "fullName": "torch.storage.np.hsplit",
    "signature": "(ary, indices_or_sections)",
    "description": "Split an array into multiple sub-arrays horizontally (column-wise)."
  },
  "742": {
    "name": "hstack",
    "module": "torch.storage.np",
    "fullName": "torch.storage.np.hstack",
    "signature": "(tup, *, dtype=None, casting='same_kind')",
    "description": "Stack arrays in sequence horizontally (column wise)."
  },
  "743": {
    "name": "i0",
    "module": "torch.storage.np",
    "fullName": "torch.storage.np.i0",
    "signature": "(x)",
    "description": "Modified Bessel function of the first kind, order 0."
  },
  "744": {
    "name": "identity",
    "module": "torch.storage.np",
    "fullName": "torch.storage.np.identity",
    "signature": "(n, dtype=None, *, like=None)",
    "description": "Return the identity array."
  },
  "745": {
    "name": "imag",
    "module": "torch.storage.np",
    "fullName": "torch.storage.np.imag",
    "signature": "(val)",
    "description": "Return the imaginary part of the complex argument."
  },
  "746": {
    "name": "in1d",
    "module": "torch.storage.np",
    "fullName": "torch.storage.np.in1d",
    "signature": "(ar1, ar2, assume_unique=False, invert=False, *, kind=None)",
    "description": "Test whether each element of a 1-D array is also present in a second array."
  },
  "747": {
    "name": "indices",
    "module": "torch.storage.np",
    "fullName": "torch.storage.np.indices",
    "signature": "(dimensions, dtype=<class 'int'>, sparse=False)",
    "description": "Return an array representing the indices of a grid."
  },
  "748": {
    "name": "info",
    "module": "torch.storage.np",
    "fullName": "torch.storage.np.info",
    "signature": "(object=None, maxwidth=76, output=None, toplevel='numpy')",
    "description": "Get help information for a function, class, or module."
  },
  "749": {
    "name": "inner",
    "module": "torch.storage.np",
    "fullName": "torch.storage.np.inner",
    "signature": "N/A",
    "description": "inner(a, b, /)"
  },
  "750": {
    "name": "insert",
    "module": "torch.storage.np",
    "fullName": "torch.storage.np.insert",
    "signature": "(arr, obj, values, axis=None)",
    "description": "Insert values along the given axis before the given indices."
  },
  "751": {
    "name": "interp",
    "module": "torch.storage.np",
    "fullName": "torch.storage.np.interp",
    "signature": "(x, xp, fp, left=None, right=None, period=None)",
    "description": "One-dimensional linear interpolation for monotonically increasing sample points."
  },
  "752": {
    "name": "intersect1d",
    "module": "torch.storage.np",
    "fullName": "torch.storage.np.intersect1d",
    "signature": "(ar1, ar2, assume_unique=False, return_indices=False)",
    "description": "Find the intersection of two arrays."
  },
  "753": {
    "name": "is_busday",
    "module": "torch.storage.np",
    "fullName": "torch.storage.np.is_busday",
    "signature": "N/A",
    "description": "is_busday(dates, weekmask='1111100', holidays=None, busdaycal=None, out=None)"
  },
  "754": {
    "name": "isclose",
    "module": "torch.storage.np",
    "fullName": "torch.storage.np.isclose",
    "signature": "(a, b, rtol=1e-05, atol=1e-08, equal_nan=False)",
    "description": "Returns a boolean array where two arrays are element-wise equal within a"
  },
  "755": {
    "name": "iscomplex",
    "module": "torch.storage.np",
    "fullName": "torch.storage.np.iscomplex",
    "signature": "(x)",
    "description": "Returns a bool array, where True if input element is complex."
  },
  "756": {
    "name": "iscomplexobj",
    "module": "torch.storage.np",
    "fullName": "torch.storage.np.iscomplexobj",
    "signature": "(x)",
    "description": "Check for a complex type or an array of complex numbers."
  },
  "757": {
    "name": "isfortran",
    "module": "torch.storage.np",
    "fullName": "torch.storage.np.isfortran",
    "signature": "(a)",
    "description": "Check if the array is Fortran contiguous but *not* C contiguous."
  },
  "758": {
    "name": "isin",
    "module": "torch.storage.np",
    "fullName": "torch.storage.np.isin",
    "signature": "(element, test_elements, assume_unique=False, invert=False, *, kind=None)",
    "description": "Calculates ``element in test_elements``, broadcasting over `element` only."
  },
  "759": {
    "name": "isneginf",
    "module": "torch.storage.np",
    "fullName": "torch.storage.np.isneginf",
    "signature": "(x, out=None)",
    "description": "Test element-wise for negative infinity, return result as bool array."
  },
  "760": {
    "name": "isposinf",
    "module": "torch.storage.np",
    "fullName": "torch.storage.np.isposinf",
    "signature": "(x, out=None)",
    "description": "Test element-wise for positive infinity, return result as bool array."
  },
  "761": {
    "name": "isreal",
    "module": "torch.storage.np",
    "fullName": "torch.storage.np.isreal",
    "signature": "(x)",
    "description": "Returns a bool array, where True if input element is real."
  },
  "762": {
    "name": "isrealobj",
    "module": "torch.storage.np",
    "fullName": "torch.storage.np.isrealobj",
    "signature": "(x)",
    "description": "Return True if x is a not complex type or an array of complex numbers."
  },
  "763": {
    "name": "isscalar",
    "module": "torch.storage.np",
    "fullName": "torch.storage.np.isscalar",
    "signature": "(element)",
    "description": "Returns True if the type of `element` is a scalar type."
  },
  "764": {
    "name": "issctype",
    "module": "torch.storage.np",
    "fullName": "torch.storage.np.issctype",
    "signature": "(rep)",
    "description": "Determines whether the given object represents a scalar data-type."
  },
  "765": {
    "name": "issubclass_",
    "module": "torch.storage.np",
    "fullName": "torch.storage.np.issubclass_",
    "signature": "(arg1, arg2)",
    "description": "Determine if a class is a subclass of a second class."
  },
  "766": {
    "name": "issubdtype",
    "module": "torch.storage.np",
    "fullName": "torch.storage.np.issubdtype",
    "signature": "(arg1, arg2)",
    "description": "Returns True if first argument is a typecode lower/equal in type hierarchy."
  },
  "767": {
    "name": "issubsctype",
    "module": "torch.storage.np",
    "fullName": "torch.storage.np.issubsctype",
    "signature": "(arg1, arg2)",
    "description": "Determine if the first argument is a subclass of the second argument."
  },
  "768": {
    "name": "iterable",
    "module": "torch.storage.np",
    "fullName": "torch.storage.np.iterable",
    "signature": "(y)",
    "description": "Check whether or not an object can be iterated over."
  },
  "769": {
    "name": "ix_",
    "module": "torch.storage.np",
    "fullName": "torch.storage.np.ix_",
    "signature": "(*args)",
    "description": "Construct an open mesh from multiple sequences."
  },
  "770": {
    "name": "kaiser",
    "module": "torch.storage.np",
    "fullName": "torch.storage.np.kaiser",
    "signature": "(M, beta)",
    "description": "Return the Kaiser window."
  },
  "771": {
    "name": "kron",
    "module": "torch.storage.np",
    "fullName": "torch.storage.np.kron",
    "signature": "(a, b)",
    "description": "Kronecker product of two arrays."
  },
  "772": {
    "name": "lexsort",
    "module": "torch.storage.np",
    "fullName": "torch.storage.np.lexsort",
    "signature": "N/A",
    "description": "lexsort(keys, axis=-1)"
  },
  "773": {
    "name": "linspace",
    "module": "torch.storage.np",
    "fullName": "torch.storage.np.linspace",
    "signature": "(start, stop, num=50, endpoint=True, retstep=False, dtype=None, axis=0)",
    "description": "Return evenly spaced numbers over a specified interval."
  },
  "774": {
    "name": "load",
    "module": "torch.storage.np",
    "fullName": "torch.storage.np.load",
    "signature": "(file, mmap_mode=None, allow_pickle=False, fix_imports=True, encoding='ASCII', *, max_header_size=10000)",
    "description": "Load arrays or pickled objects from ``.npy``, ``.npz`` or pickled files."
  },
  "775": {
    "name": "loadtxt",
    "module": "torch.storage.np",
    "fullName": "torch.storage.np.loadtxt",
    "signature": "(fname, dtype=<class 'float'>, comments='#', delimiter=None, converters=None, skiprows=0, usecols=None, unpack=False, ndmin=0, encoding='bytes', max_rows=None, *, quotechar=None, like=None)",
    "description": "Load data from a text file."
  },
  "776": {
    "name": "logspace",
    "module": "torch.storage.np",
    "fullName": "torch.storage.np.logspace",
    "signature": "(start, stop, num=50, endpoint=True, base=10.0, dtype=None, axis=0)",
    "description": "Return numbers spaced evenly on a log scale."
  },
  "777": {
    "name": "lookfor",
    "module": "torch.storage.np",
    "fullName": "torch.storage.np.lookfor",
    "signature": "(what, module=None, import_modules=True, regenerate=False, output=None)",
    "description": "Do a keyword search on docstrings."
  },
  "778": {
    "name": "mask_indices",
    "module": "torch.storage.np",
    "fullName": "torch.storage.np.mask_indices",
    "signature": "(n, mask_func, k=0)",
    "description": "Return the indices to access (n, n) arrays, given a masking function."
  },
  "779": {
    "name": "mat",
    "module": "torch.storage.np",
    "fullName": "torch.storage.np.mat",
    "signature": "(data, dtype=None)",
    "description": "Interpret the input as a matrix."
  },
  "780": {
    "name": "max",
    "module": "torch.storage.np",
    "fullName": "torch.storage.np.max",
    "signature": "(a, axis=None, out=None, keepdims=<no value>, initial=<no value>, where=<no value>)",
    "description": "Return the maximum of an array or maximum along an axis."
  },
  "781": {
    "name": "maximum_sctype",
    "module": "torch.storage.np",
    "fullName": "torch.storage.np.maximum_sctype",
    "signature": "(t)",
    "description": "Return the scalar type of highest precision of the same kind as the input."
  },
  "782": {
    "name": "may_share_memory",
    "module": "torch.storage.np",
    "fullName": "torch.storage.np.may_share_memory",
    "signature": "N/A",
    "description": "may_share_memory(a, b, /, max_work=None)"
  },
  "783": {
    "name": "mean",
    "module": "torch.storage.np",
    "fullName": "torch.storage.np.mean",
    "signature": "(a, axis=None, dtype=None, out=None, keepdims=<no value>, *, where=<no value>)",
    "description": "Compute the arithmetic mean along the specified axis."
  },
  "784": {
    "name": "median",
    "module": "torch.storage.np",
    "fullName": "torch.storage.np.median",
    "signature": "(a, axis=None, out=None, overwrite_input=False, keepdims=False)",
    "description": "Compute the median along the specified axis."
  },
  "785": {
    "name": "meshgrid",
    "module": "torch.storage.np",
    "fullName": "torch.storage.np.meshgrid",
    "signature": "(*xi, copy=True, sparse=False, indexing='xy')",
    "description": "Return coordinate matrices from coordinate vectors."
  },
  "786": {
    "name": "min",
    "module": "torch.storage.np",
    "fullName": "torch.storage.np.min",
    "signature": "(a, axis=None, out=None, keepdims=<no value>, initial=<no value>, where=<no value>)",
    "description": "Return the minimum of an array or minimum along an axis."
  },
  "787": {
    "name": "min_scalar_type",
    "module": "torch.storage.np",
    "fullName": "torch.storage.np.min_scalar_type",
    "signature": "N/A",
    "description": "min_scalar_type(a, /)"
  },
  "788": {
    "name": "mintypecode",
    "module": "torch.storage.np",
    "fullName": "torch.storage.np.mintypecode",
    "signature": "(typechars, typeset='GDFgdf', default='d')",
    "description": "Return the character for the minimum-size type to which given types can"
  },
  "789": {
    "name": "moveaxis",
    "module": "torch.storage.np",
    "fullName": "torch.storage.np.moveaxis",
    "signature": "(a, source, destination)",
    "description": "Move axes of an array to new positions."
  },
  "790": {
    "name": "nan_to_num",
    "module": "torch.storage.np",
    "fullName": "torch.storage.np.nan_to_num",
    "signature": "(x, copy=True, nan=0.0, posinf=None, neginf=None)",
    "description": "Replace NaN with zero and infinity with large finite numbers (default"
  },
  "791": {
    "name": "nanargmax",
    "module": "torch.storage.np",
    "fullName": "torch.storage.np.nanargmax",
    "signature": "(a, axis=None, out=None, *, keepdims=<no value>)",
    "description": "Return the indices of the maximum values in the specified axis ignoring"
  },
  "792": {
    "name": "nanargmin",
    "module": "torch.storage.np",
    "fullName": "torch.storage.np.nanargmin",
    "signature": "(a, axis=None, out=None, *, keepdims=<no value>)",
    "description": "Return the indices of the minimum values in the specified axis ignoring"
  },
  "793": {
    "name": "nancumprod",
    "module": "torch.storage.np",
    "fullName": "torch.storage.np.nancumprod",
    "signature": "(a, axis=None, dtype=None, out=None)",
    "description": "Return the cumulative product of array elements over a given axis treating Not a"
  },
  "794": {
    "name": "nancumsum",
    "module": "torch.storage.np",
    "fullName": "torch.storage.np.nancumsum",
    "signature": "(a, axis=None, dtype=None, out=None)",
    "description": "Return the cumulative sum of array elements over a given axis treating Not a"
  },
  "795": {
    "name": "nanmax",
    "module": "torch.storage.np",
    "fullName": "torch.storage.np.nanmax",
    "signature": "(a, axis=None, out=None, keepdims=<no value>, initial=<no value>, where=<no value>)",
    "description": "Return the maximum of an array or maximum along an axis, ignoring any"
  },
  "796": {
    "name": "nanmean",
    "module": "torch.storage.np",
    "fullName": "torch.storage.np.nanmean",
    "signature": "(a, axis=None, dtype=None, out=None, keepdims=<no value>, *, where=<no value>)",
    "description": "Compute the arithmetic mean along the specified axis, ignoring NaNs."
  },
  "797": {
    "name": "nanmedian",
    "module": "torch.storage.np",
    "fullName": "torch.storage.np.nanmedian",
    "signature": "(a, axis=None, out=None, overwrite_input=False, keepdims=<no value>)",
    "description": "Compute the median along the specified axis, while ignoring NaNs."
  },
  "798": {
    "name": "nanmin",
    "module": "torch.storage.np",
    "fullName": "torch.storage.np.nanmin",
    "signature": "(a, axis=None, out=None, keepdims=<no value>, initial=<no value>, where=<no value>)",
    "description": "Return minimum of an array or minimum along an axis, ignoring any NaNs."
  },
  "799": {
    "name": "nanprod",
    "module": "torch.storage.np",
    "fullName": "torch.storage.np.nanprod",
    "signature": "(a, axis=None, dtype=None, out=None, keepdims=<no value>, initial=<no value>, where=<no value>)",
    "description": "Return the product of array elements over a given axis treating Not a"
  },
  "800": {
    "name": "nanstd",
    "module": "torch.storage.np",
    "fullName": "torch.storage.np.nanstd",
    "signature": "(a, axis=None, dtype=None, out=None, ddof=0, keepdims=<no value>, *, where=<no value>)",
    "description": "Compute the standard deviation along the specified axis, while"
  },
  "801": {
    "name": "nansum",
    "module": "torch.storage.np",
    "fullName": "torch.storage.np.nansum",
    "signature": "(a, axis=None, dtype=None, out=None, keepdims=<no value>, initial=<no value>, where=<no value>)",
    "description": "Return the sum of array elements over a given axis treating Not a"
  },
  "802": {
    "name": "nanvar",
    "module": "torch.storage.np",
    "fullName": "torch.storage.np.nanvar",
    "signature": "(a, axis=None, dtype=None, out=None, ddof=0, keepdims=<no value>, *, where=<no value>)",
    "description": "Compute the variance along the specified axis, while ignoring NaNs."
  },
  "803": {
    "name": "ndim",
    "module": "torch.storage.np",
    "fullName": "torch.storage.np.ndim",
    "signature": "(a)",
    "description": "Return the number of dimensions of an array."
  },
  "804": {
    "name": "obj2sctype",
    "module": "torch.storage.np",
    "fullName": "torch.storage.np.obj2sctype",
    "signature": "(rep, default=None)",
    "description": "Return the scalar dtype or NumPy equivalent of Python type of an object."
  },
  "805": {
    "name": "ones",
    "module": "torch.storage.np",
    "fullName": "torch.storage.np.ones",
    "signature": "(shape, dtype=None, order='C', *, like=None)",
    "description": "Return a new array of given shape and type, filled with ones."
  },
  "806": {
    "name": "ones_like",
    "module": "torch.storage.np",
    "fullName": "torch.storage.np.ones_like",
    "signature": "(a, dtype=None, order='K', subok=True, shape=None)",
    "description": "Return an array of ones with the same shape and type as a given array."
  },
  "807": {
    "name": "outer",
    "module": "torch.storage.np",
    "fullName": "torch.storage.np.outer",
    "signature": "(a, b, out=None)",
    "description": "Compute the outer product of two vectors."
  },
  "808": {
    "name": "packbits",
    "module": "torch.storage.np",
    "fullName": "torch.storage.np.packbits",
    "signature": "N/A",
    "description": "packbits(a, /, axis=None, bitorder='big')"
  },
  "809": {
    "name": "pad",
    "module": "torch.storage.np",
    "fullName": "torch.storage.np.pad",
    "signature": "(array, pad_width, mode='constant', **kwargs)",
    "description": "Pad an array."
  },
  "810": {
    "name": "piecewise",
    "module": "torch.storage.np",
    "fullName": "torch.storage.np.piecewise",
    "signature": "(x, condlist, funclist, *args, **kw)",
    "description": "Evaluate a piecewise-defined function."
  },
  "811": {
    "name": "place",
    "module": "torch.storage.np",
    "fullName": "torch.storage.np.place",
    "signature": "(arr, mask, vals)",
    "description": "Change elements of an array based on conditional and input values."
  },
  "812": {
    "name": "poly",
    "module": "torch.storage.np",
    "fullName": "torch.storage.np.poly",
    "signature": "(seq_of_zeros)",
    "description": "Find the coefficients of a polynomial with the given sequence of roots."
  },
  "813": {
    "name": "polyadd",
    "module": "torch.storage.np",
    "fullName": "torch.storage.np.polyadd",
    "signature": "(a1, a2)",
    "description": "Find the sum of two polynomials."
  },
  "814": {
    "name": "polyder",
    "module": "torch.storage.np",
    "fullName": "torch.storage.np.polyder",
    "signature": "(p, m=1)",
    "description": "Return the derivative of the specified order of a polynomial."
  },
  "815": {
    "name": "polydiv",
    "module": "torch.storage.np",
    "fullName": "torch.storage.np.polydiv",
    "signature": "(u, v)",
    "description": "Returns the quotient and remainder of polynomial division."
  },
  "816": {
    "name": "polyfit",
    "module": "torch.storage.np",
    "fullName": "torch.storage.np.polyfit",
    "signature": "(x, y, deg, rcond=None, full=False, w=None, cov=False)",
    "description": "Least squares polynomial fit."
  },
  "817": {
    "name": "polyint",
    "module": "torch.storage.np",
    "fullName": "torch.storage.np.polyint",
    "signature": "(p, m=1, k=None)",
    "description": "Return an antiderivative (indefinite integral) of a polynomial."
  },
  "818": {
    "name": "polymul",
    "module": "torch.storage.np",
    "fullName": "torch.storage.np.polymul",
    "signature": "(a1, a2)",
    "description": "Find the product of two polynomials."
  },
  "819": {
    "name": "polysub",
    "module": "torch.storage.np",
    "fullName": "torch.storage.np.polysub",
    "signature": "(a1, a2)",
    "description": "Difference (subtraction) of two polynomials."
  },
  "820": {
    "name": "polyval",
    "module": "torch.storage.np",
    "fullName": "torch.storage.np.polyval",
    "signature": "(p, x)",
    "description": "Evaluate a polynomial at specific values."
  },
  "821": {
    "name": "printoptions",
    "module": "torch.storage.np",
    "fullName": "torch.storage.np.printoptions",
    "signature": "(*args, **kwargs)",
    "description": "Context manager for setting print options."
  },
  "822": {
    "name": "prod",
    "module": "torch.storage.np",
    "fullName": "torch.storage.np.prod",
    "signature": "(a, axis=None, dtype=None, out=None, keepdims=<no value>, initial=<no value>, where=<no value>)",
    "description": "Return the product of array elements over a given axis."
  },
  "823": {
    "name": "product",
    "module": "torch.storage.np",
    "fullName": "torch.storage.np.product",
    "signature": "(*args, **kwargs)",
    "description": "Return the product of array elements over a given axis."
  },
  "824": {
    "name": "ptp",
    "module": "torch.storage.np",
    "fullName": "torch.storage.np.ptp",
    "signature": "(a, axis=None, out=None, keepdims=<no value>)",
    "description": "Range of values (maximum - minimum) along an axis."
  },
  "825": {
    "name": "put",
    "module": "torch.storage.np",
    "fullName": "torch.storage.np.put",
    "signature": "(a, ind, v, mode='raise')",
    "description": "Replaces specified elements of an array with given values."
  },
  "826": {
    "name": "put_along_axis",
    "module": "torch.storage.np",
    "fullName": "torch.storage.np.put_along_axis",
    "signature": "(arr, indices, values, axis)",
    "description": "Put values into the destination array by matching 1d index and data slices."
  },
  "827": {
    "name": "putmask",
    "module": "torch.storage.np",
    "fullName": "torch.storage.np.putmask",
    "signature": "N/A",
    "description": "putmask(a, mask, values)"
  },
  "828": {
    "name": "ravel",
    "module": "torch.storage.np",
    "fullName": "torch.storage.np.ravel",
    "signature": "(a, order='C')",
    "description": "Return a contiguous flattened array."
  },
  "829": {
    "name": "ravel_multi_index",
    "module": "torch.storage.np",
    "fullName": "torch.storage.np.ravel_multi_index",
    "signature": "N/A",
    "description": "ravel_multi_index(multi_index, dims, mode='raise', order='C')"
  },
  "830": {
    "name": "real",
    "module": "torch.storage.np",
    "fullName": "torch.storage.np.real",
    "signature": "(val)",
    "description": "Return the real part of the complex argument."
  },
  "831": {
    "name": "real_if_close",
    "module": "torch.storage.np",
    "fullName": "torch.storage.np.real_if_close",
    "signature": "(a, tol=100)",
    "description": "If input is complex with all imaginary parts close to zero, return"
  },
  "832": {
    "name": "recfromcsv",
    "module": "torch.storage.np",
    "fullName": "torch.storage.np.recfromcsv",
    "signature": "(fname, **kwargs)",
    "description": "Load ASCII data stored in a comma-separated file."
  },
  "833": {
    "name": "recfromtxt",
    "module": "torch.storage.np",
    "fullName": "torch.storage.np.recfromtxt",
    "signature": "(fname, **kwargs)",
    "description": "Load ASCII data from a file and return it in a record array."
  },
  "834": {
    "name": "repeat",
    "module": "torch.storage.np",
    "fullName": "torch.storage.np.repeat",
    "signature": "(a, repeats, axis=None)",
    "description": "Repeat elements of an array."
  },
  "835": {
    "name": "require",
    "module": "torch.storage.np",
    "fullName": "torch.storage.np.require",
    "signature": "(a, dtype=None, requirements=None, *, like=None)",
    "description": "Return an ndarray of the provided type that satisfies requirements."
  },
  "836": {
    "name": "reshape",
    "module": "torch.storage.np",
    "fullName": "torch.storage.np.reshape",
    "signature": "(a, newshape, order='C')",
    "description": "Gives a new shape to an array without changing its data."
  },
  "837": {
    "name": "resize",
    "module": "torch.storage.np",
    "fullName": "torch.storage.np.resize",
    "signature": "(a, new_shape)",
    "description": "Return a new array with the specified shape."
  },
  "838": {
    "name": "result_type",
    "module": "torch.storage.np",
    "fullName": "torch.storage.np.result_type",
    "signature": "N/A",
    "description": "result_type(*arrays_and_dtypes)"
  },
  "839": {
    "name": "roll",
    "module": "torch.storage.np",
    "fullName": "torch.storage.np.roll",
    "signature": "(a, shift, axis=None)",
    "description": "Roll array elements along a given axis."
  },
  "840": {
    "name": "rollaxis",
    "module": "torch.storage.np",
    "fullName": "torch.storage.np.rollaxis",
    "signature": "(a, axis, start=0)",
    "description": "Roll the specified axis backwards, until it lies in a given position."
  },
  "841": {
    "name": "roots",
    "module": "torch.storage.np",
    "fullName": "torch.storage.np.roots",
    "signature": "(p)",
    "description": "Return the roots of a polynomial with coefficients given in p."
  },
  "842": {
    "name": "rot90",
    "module": "torch.storage.np",
    "fullName": "torch.storage.np.rot90",
    "signature": "(m, k=1, axes=(0, 1))",
    "description": "Rotate an array by 90 degrees in the plane specified by axes."
  },
  "843": {
    "name": "round",
    "module": "torch.storage.np",
    "fullName": "torch.storage.np.round",
    "signature": "(a, decimals=0, out=None)",
    "description": "Round an array to the given number of decimals."
  },
  "844": {
    "name": "round_",
    "module": "torch.storage.np",
    "fullName": "torch.storage.np.round_",
    "signature": "(a, decimals=0, out=None)",
    "description": "Round an array to the given number of decimals."
  },
  "845": {
    "name": "row_stack",
    "module": "torch.storage.np",
    "fullName": "torch.storage.np.row_stack",
    "signature": "(tup, *, dtype=None, casting='same_kind')",
    "description": "Stack arrays in sequence vertically (row wise)."
  },
  "846": {
    "name": "safe_eval",
    "module": "torch.storage.np",
    "fullName": "torch.storage.np.safe_eval",
    "signature": "(source)",
    "description": "Protected string evaluation."
  },
  "847": {
    "name": "save",
    "module": "torch.storage.np",
    "fullName": "torch.storage.np.save",
    "signature": "(file, arr, allow_pickle=True, fix_imports=True)",
    "description": "Save an array to a binary file in NumPy ``.npy`` format."
  },
  "848": {
    "name": "savetxt",
    "module": "torch.storage.np",
    "fullName": "torch.storage.np.savetxt",
    "signature": "(fname, X, fmt='%.18e', delimiter=' ', newline='\\n', header='', footer='', comments='# ', encoding=None)",
    "description": "Save an array to a text file."
  },
  "849": {
    "name": "savez",
    "module": "torch.storage.np",
    "fullName": "torch.storage.np.savez",
    "signature": "(file, *args, **kwds)",
    "description": "Save several arrays into a single file in uncompressed ``.npz`` format."
  },
  "850": {
    "name": "savez_compressed",
    "module": "torch.storage.np",
    "fullName": "torch.storage.np.savez_compressed",
    "signature": "(file, *args, **kwds)",
    "description": "Save several arrays into a single file in compressed ``.npz`` format."
  },
  "851": {
    "name": "sctype2char",
    "module": "torch.storage.np",
    "fullName": "torch.storage.np.sctype2char",
    "signature": "(sctype)",
    "description": "Return the string representation of a scalar dtype."
  },
  "852": {
    "name": "searchsorted",
    "module": "torch.storage.np",
    "fullName": "torch.storage.np.searchsorted",
    "signature": "(a, v, side='left', sorter=None)",
    "description": "Find indices where elements should be inserted to maintain order."
  },
  "853": {
    "name": "select",
    "module": "torch.storage.np",
    "fullName": "torch.storage.np.select",
    "signature": "(condlist, choicelist, default=0)",
    "description": "Return an array drawn from elements in choicelist, depending on conditions."
  },
  "854": {
    "name": "set_printoptions",
    "module": "torch.storage.np",
    "fullName": "torch.storage.np.set_printoptions",
    "signature": "(precision=None, threshold=None, edgeitems=None, linewidth=None, suppress=None, nanstr=None, infstr=None, formatter=None, sign=None, floatmode=None, *, legacy=None)",
    "description": "Set printing options."
  },
  "855": {
    "name": "set_string_function",
    "module": "torch.storage.np",
    "fullName": "torch.storage.np.set_string_function",
    "signature": "(f, repr=True)",
    "description": "Set a Python function to be used when pretty printing arrays."
  },
  "856": {
    "name": "setbufsize",
    "module": "torch.storage.np",
    "fullName": "torch.storage.np.setbufsize",
    "signature": "(size)",
    "description": "Set the size of the buffer used in ufuncs."
  },
  "857": {
    "name": "setdiff1d",
    "module": "torch.storage.np",
    "fullName": "torch.storage.np.setdiff1d",
    "signature": "(ar1, ar2, assume_unique=False)",
    "description": "Find the set difference of two arrays."
  },
  "858": {
    "name": "seterr",
    "module": "torch.storage.np",
    "fullName": "torch.storage.np.seterr",
    "signature": "(all=None, divide=None, over=None, under=None, invalid=None)",
    "description": "Set how floating-point errors are handled."
  },
  "859": {
    "name": "seterrcall",
    "module": "torch.storage.np",
    "fullName": "torch.storage.np.seterrcall",
    "signature": "(func)",
    "description": "Set the floating-point error callback function or log object."
  },
  "860": {
    "name": "setxor1d",
    "module": "torch.storage.np",
    "fullName": "torch.storage.np.setxor1d",
    "signature": "(ar1, ar2, assume_unique=False)",
    "description": "Find the set exclusive-or of two arrays."
  },
  "861": {
    "name": "shape",
    "module": "torch.storage.np",
    "fullName": "torch.storage.np.shape",
    "signature": "(a)",
    "description": "Return the shape of an array."
  },
  "862": {
    "name": "shares_memory",
    "module": "torch.storage.np",
    "fullName": "torch.storage.np.shares_memory",
    "signature": "N/A",
    "description": "shares_memory(a, b, /, max_work=None)"
  },
  "863": {
    "name": "show_config",
    "module": "torch.storage.np",
    "fullName": "torch.storage.np.show_config",
    "signature": "()",
    "description": "Show libraries in the system on which NumPy was built."
  },
  "864": {
    "name": "show_runtime",
    "module": "torch.storage.np",
    "fullName": "torch.storage.np.show_runtime",
    "signature": "()",
    "description": "Print information about various resources in the system"
  },
  "865": {
    "name": "sinc",
    "module": "torch.storage.np",
    "fullName": "torch.storage.np.sinc",
    "signature": "(x)",
    "description": "Return the normalized sinc function."
  },
  "866": {
    "name": "size",
    "module": "torch.storage.np",
    "fullName": "torch.storage.np.size",
    "signature": "(a, axis=None)",
    "description": "Return the number of elements along a given axis."
  },
  "867": {
    "name": "sometrue",
    "module": "torch.storage.np",
    "fullName": "torch.storage.np.sometrue",
    "signature": "(*args, **kwargs)",
    "description": "Check whether some values are true."
  },
  "868": {
    "name": "sort",
    "module": "torch.storage.np",
    "fullName": "torch.storage.np.sort",
    "signature": "(a, axis=-1, kind=None, order=None)",
    "description": "Return a sorted copy of an array."
  },
  "869": {
    "name": "sort_complex",
    "module": "torch.storage.np",
    "fullName": "torch.storage.np.sort_complex",
    "signature": "(a)",
    "description": "Sort a complex array using the real part first, then the imaginary part."
  },
  "870": {
    "name": "source",
    "module": "torch.storage.np",
    "fullName": "torch.storage.np.source",
    "signature": "(object, output=<_io.TextIOWrapper name='<stdout>' mode='w' encoding='utf-8'>)",
    "description": "Print or write to a file the source code for a NumPy object."
  },
  "871": {
    "name": "split",
    "module": "torch.storage.np",
    "fullName": "torch.storage.np.split",
    "signature": "(ary, indices_or_sections, axis=0)",
    "description": "Split an array into multiple sub-arrays as views into `ary`."
  },
  "872": {
    "name": "squeeze",
    "module": "torch.storage.np",
    "fullName": "torch.storage.np.squeeze",
    "signature": "(a, axis=None)",
    "description": "Remove axes of length one from `a`."
  },
  "873": {
    "name": "stack",
    "module": "torch.storage.np",
    "fullName": "torch.storage.np.stack",
    "signature": "(arrays, axis=0, out=None, *, dtype=None, casting='same_kind')",
    "description": "Join a sequence of arrays along a new axis."
  },
  "874": {
    "name": "std",
    "module": "torch.storage.np",
    "fullName": "torch.storage.np.std",
    "signature": "(a, axis=None, dtype=None, out=None, ddof=0, keepdims=<no value>, *, where=<no value>)",
    "description": "Compute the standard deviation along the specified axis."
  },
  "875": {
    "name": "sum",
    "module": "torch.storage.np",
    "fullName": "torch.storage.np.sum",
    "signature": "(a, axis=None, dtype=None, out=None, keepdims=<no value>, initial=<no value>, where=<no value>)",
    "description": "Sum of array elements over a given axis."
  },
  "876": {
    "name": "swapaxes",
    "module": "torch.storage.np",
    "fullName": "torch.storage.np.swapaxes",
    "signature": "(a, axis1, axis2)",
    "description": "Interchange two axes of an array."
  },
  "877": {
    "name": "take",
    "module": "torch.storage.np",
    "fullName": "torch.storage.np.take",
    "signature": "(a, indices, axis=None, out=None, mode='raise')",
    "description": "Take elements from an array along an axis."
  },
  "878": {
    "name": "take_along_axis",
    "module": "torch.storage.np",
    "fullName": "torch.storage.np.take_along_axis",
    "signature": "(arr, indices, axis)",
    "description": "Take values from the input array by matching 1d index and data slices."
  },
  "879": {
    "name": "tensordot",
    "module": "torch.storage.np",
    "fullName": "torch.storage.np.tensordot",
    "signature": "(a, b, axes=2)",
    "description": "Compute tensor dot product along specified axes."
  },
  "880": {
    "name": "tile",
    "module": "torch.storage.np",
    "fullName": "torch.storage.np.tile",
    "signature": "(A, reps)",
    "description": "Construct an array by repeating A the number of times given by reps."
  },
  "881": {
    "name": "trace",
    "module": "torch.storage.np",
    "fullName": "torch.storage.np.trace",
    "signature": "(a, offset=0, axis1=0, axis2=1, dtype=None, out=None)",
    "description": "Return the sum along diagonals of the array."
  },
  "882": {
    "name": "transpose",
    "module": "torch.storage.np",
    "fullName": "torch.storage.np.transpose",
    "signature": "(a, axes=None)",
    "description": "Returns an array with axes transposed."
  },
  "883": {
    "name": "trapz",
    "module": "torch.storage.np",
    "fullName": "torch.storage.np.trapz",
    "signature": "(y, x=None, dx=1.0, axis=-1)",
    "description": "Integrate along the given axis using the composite trapezoidal rule."
  },
  "884": {
    "name": "tri",
    "module": "torch.storage.np",
    "fullName": "torch.storage.np.tri",
    "signature": "(N, M=None, k=0, dtype=<class 'float'>, *, like=None)",
    "description": "An array with ones at and below the given diagonal and zeros elsewhere."
  },
  "885": {
    "name": "tril",
    "module": "torch.storage.np",
    "fullName": "torch.storage.np.tril",
    "signature": "(m, k=0)",
    "description": "Lower triangle of an array."
  },
  "886": {
    "name": "tril_indices",
    "module": "torch.storage.np",
    "fullName": "torch.storage.np.tril_indices",
    "signature": "(n, k=0, m=None)",
    "description": "Return the indices for the lower-triangle of an (n, m) array."
  },
  "887": {
    "name": "tril_indices_from",
    "module": "torch.storage.np",
    "fullName": "torch.storage.np.tril_indices_from",
    "signature": "(arr, k=0)",
    "description": "Return the indices for the lower-triangle of arr."
  },
  "888": {
    "name": "trim_zeros",
    "module": "torch.storage.np",
    "fullName": "torch.storage.np.trim_zeros",
    "signature": "(filt, trim='fb')",
    "description": "Trim the leading and/or trailing zeros from a 1-D array or sequence."
  },
  "889": {
    "name": "triu",
    "module": "torch.storage.np",
    "fullName": "torch.storage.np.triu",
    "signature": "(m, k=0)",
    "description": "Upper triangle of an array."
  },
  "890": {
    "name": "triu_indices",
    "module": "torch.storage.np",
    "fullName": "torch.storage.np.triu_indices",
    "signature": "(n, k=0, m=None)",
    "description": "Return the indices for the upper-triangle of an (n, m) array."
  },
  "891": {
    "name": "triu_indices_from",
    "module": "torch.storage.np",
    "fullName": "torch.storage.np.triu_indices_from",
    "signature": "(arr, k=0)",
    "description": "Return the indices for the upper-triangle of arr."
  },
  "892": {
    "name": "typename",
    "module": "torch.storage.np",
    "fullName": "torch.storage.np.typename",
    "signature": "(char)",
    "description": "Return a description for the given data type code."
  },
  "893": {
    "name": "union1d",
    "module": "torch.storage.np",
    "fullName": "torch.storage.np.union1d",
    "signature": "(ar1, ar2)",
    "description": "Find the union of two arrays."
  },
  "894": {
    "name": "unique",
    "module": "torch.storage.np",
    "fullName": "torch.storage.np.unique",
    "signature": "(ar, return_index=False, return_inverse=False, return_counts=False, axis=None, *, equal_nan=True)",
    "description": "Find the unique elements of an array."
  },
  "895": {
    "name": "unpackbits",
    "module": "torch.storage.np",
    "fullName": "torch.storage.np.unpackbits",
    "signature": "N/A",
    "description": "unpackbits(a, /, axis=None, count=None, bitorder='big')"
  },
  "896": {
    "name": "unravel_index",
    "module": "torch.storage.np",
    "fullName": "torch.storage.np.unravel_index",
    "signature": "N/A",
    "description": "unravel_index(indices, shape, order='C')"
  },
  "897": {
    "name": "unwrap",
    "module": "torch.storage.np",
    "fullName": "torch.storage.np.unwrap",
    "signature": "(p, discont=None, axis=-1, *, period=6.283185307179586)",
    "description": "Unwrap by taking the complement of large deltas with respect to the period."
  },
  "898": {
    "name": "vander",
    "module": "torch.storage.np",
    "fullName": "torch.storage.np.vander",
    "signature": "(x, N=None, increasing=False)",
    "description": "Generate a Vandermonde matrix."
  },
  "899": {
    "name": "var",
    "module": "torch.storage.np",
    "fullName": "torch.storage.np.var",
    "signature": "(a, axis=None, dtype=None, out=None, ddof=0, keepdims=<no value>, *, where=<no value>)",
    "description": "Compute the variance along the specified axis."
  },
  "900": {
    "name": "vdot",
    "module": "torch.storage.np",
    "fullName": "torch.storage.np.vdot",
    "signature": "N/A",
    "description": "vdot(a, b, /)"
  },
  "901": {
    "name": "vsplit",
    "module": "torch.storage.np",
    "fullName": "torch.storage.np.vsplit",
    "signature": "(ary, indices_or_sections)",
    "description": "Split an array into multiple sub-arrays vertically (row-wise)."
  },
  "902": {
    "name": "vstack",
    "module": "torch.storage.np",
    "fullName": "torch.storage.np.vstack",
    "signature": "(tup, *, dtype=None, casting='same_kind')",
    "description": "Stack arrays in sequence vertically (row wise)."
  },
  "903": {
    "name": "where",
    "module": "torch.storage.np",
    "fullName": "torch.storage.np.where",
    "signature": "N/A",
    "description": "where(condition, [x, y], /)"
  },
  "904": {
    "name": "who",
    "module": "torch.storage.np",
    "fullName": "torch.storage.np.who",
    "signature": "(vardict=None)",
    "description": "Print the NumPy arrays in the given dictionary."
  },
  "905": {
    "name": "zeros_like",
    "module": "torch.storage.np",
    "fullName": "torch.storage.np.zeros_like",
    "signature": "(a, dtype=None, order='K', subok=True, shape=None)",
    "description": "Return an array of zeros with the same shape and type as a given array."
  },
  "906": {
    "name": "assert_",
    "module": "torch.storage.np.testing",
    "fullName": "torch.storage.np.testing.assert_",
    "signature": "(val, msg='')",
    "description": "Assert that works in release mode."
  },
  "907": {
    "name": "assert_allclose",
    "module": "torch.storage.np.testing",
    "fullName": "torch.storage.np.testing.assert_allclose",
    "signature": "(actual, desired, rtol=1e-07, atol=0, equal_nan=True, err_msg='', verbose=True)",
    "description": "Raises an AssertionError if two objects are not equal up to desired"
  },
  "908": {
    "name": "assert_almost_equal",
    "module": "torch.storage.np.testing",
    "fullName": "torch.storage.np.testing.assert_almost_equal",
    "signature": "(actual, desired, decimal=7, err_msg='', verbose=True)",
    "description": "Raises an AssertionError if two items are not equal up to desired"
  },
  "909": {
    "name": "assert_approx_equal",
    "module": "torch.storage.np.testing",
    "fullName": "torch.storage.np.testing.assert_approx_equal",
    "signature": "(actual, desired, significant=7, err_msg='', verbose=True)",
    "description": "Raises an AssertionError if two items are not equal up to significant"
  },
  "910": {
    "name": "assert_array_almost_equal",
    "module": "torch.storage.np.testing",
    "fullName": "torch.storage.np.testing.assert_array_almost_equal",
    "signature": "(x, y, decimal=6, err_msg='', verbose=True)",
    "description": "Raises an AssertionError if two objects are not equal up to desired"
  },
  "911": {
    "name": "assert_array_almost_equal_nulp",
    "module": "torch.storage.np.testing",
    "fullName": "torch.storage.np.testing.assert_array_almost_equal_nulp",
    "signature": "(x, y, nulp=1)",
    "description": "Compare two arrays relatively to their spacing."
  },
  "912": {
    "name": "assert_array_compare",
    "module": "torch.storage.np.testing",
    "fullName": "torch.storage.np.testing.assert_array_compare",
    "signature": "(comparison, x, y, err_msg='', verbose=True, header='', precision=6, equal_nan=True, equal_inf=True, *, strict=False)",
    "description": "No description available."
  },
  "913": {
    "name": "assert_array_equal",
    "module": "torch.storage.np.testing",
    "fullName": "torch.storage.np.testing.assert_array_equal",
    "signature": "(x, y, err_msg='', verbose=True, *, strict=False)",
    "description": "Raises an AssertionError if two array_like objects are not equal."
  },
  "914": {
    "name": "assert_array_less",
    "module": "torch.storage.np.testing",
    "fullName": "torch.storage.np.testing.assert_array_less",
    "signature": "(x, y, err_msg='', verbose=True)",
    "description": "Raises an AssertionError if two array_like objects are not ordered by less"
  },
  "915": {
    "name": "assert_array_max_ulp",
    "module": "torch.storage.np.testing",
    "fullName": "torch.storage.np.testing.assert_array_max_ulp",
    "signature": "(a, b, maxulp=1, dtype=None)",
    "description": "Check that all items of arrays differ in at most N Units in the Last Place."
  },
  "916": {
    "name": "assert_equal",
    "module": "torch.storage.np.testing",
    "fullName": "torch.storage.np.testing.assert_equal",
    "signature": "(actual, desired, err_msg='', verbose=True)",
    "description": "Raises an AssertionError if two objects are not equal."
  },
  "917": {
    "name": "assert_no_gc_cycles",
    "module": "torch.storage.np.testing",
    "fullName": "torch.storage.np.testing.assert_no_gc_cycles",
    "signature": "(*args, **kwargs)",
    "description": "Fail if the given callable produces any reference cycles."
  },
  "918": {
    "name": "assert_no_warnings",
    "module": "torch.storage.np.testing",
    "fullName": "torch.storage.np.testing.assert_no_warnings",
    "signature": "(*args, **kwargs)",
    "description": "Fail if the given callable produces any warnings."
  },
  "919": {
    "name": "assert_raises",
    "module": "torch.storage.np.testing",
    "fullName": "torch.storage.np.testing.assert_raises",
    "signature": "(*args, **kwargs)",
    "description": "assert_raises(exception_class, callable, *args, **kwargs)"
  },
  "920": {
    "name": "assert_raises_regex",
    "module": "torch.storage.np.testing",
    "fullName": "torch.storage.np.testing.assert_raises_regex",
    "signature": "(exception_class, expected_regexp, *args, **kwargs)",
    "description": "assert_raises_regex(exception_class, expected_regexp, callable, *args,"
  },
  "921": {
    "name": "assert_string_equal",
    "module": "torch.storage.np.testing",
    "fullName": "torch.storage.np.testing.assert_string_equal",
    "signature": "(actual, desired)",
    "description": "Test if two strings are equal."
  },
  "922": {
    "name": "break_cycles",
    "module": "torch.storage.np.testing",
    "fullName": "torch.storage.np.testing.break_cycles",
    "signature": "()",
    "description": "Break reference cycles by calling gc.collect"
  },
  "923": {
    "name": "build_err_msg",
    "module": "torch.storage.np.testing",
    "fullName": "torch.storage.np.testing.build_err_msg",
    "signature": "(arrays, err_msg, header='Items are not equal:', verbose=True, names=('ACTUAL', 'DESIRED'), precision=8)",
    "description": "No description available."
  },
  "924": {
    "name": "decorate_methods",
    "module": "torch.storage.np.testing",
    "fullName": "torch.storage.np.testing.decorate_methods",
    "signature": "(cls, decorator, testmatch=None)",
    "description": "Apply a decorator to all methods in a class matching a regular expression."
  },
  "925": {
    "name": "jiffies",
    "module": "torch.storage.np.testing",
    "fullName": "torch.storage.np.testing.jiffies",
    "signature": "(_proc_pid_stat='/proc/16328/stat', _load_time=[])",
    "description": "Return number of jiffies elapsed."
  },
  "926": {
    "name": "measure",
    "module": "torch.storage.np.testing",
    "fullName": "torch.storage.np.testing.measure",
    "signature": "(code_str, times=1, label=None)",
    "description": "Return elapsed time for executing code in the namespace of the caller."
  },
  "927": {
    "name": "memusage",
    "module": "torch.storage.np.testing",
    "fullName": "torch.storage.np.testing.memusage",
    "signature": "(_proc_pid_stat='/proc/16328/stat')",
    "description": "Return virtual memory size in bytes of the running python."
  },
  "928": {
    "name": "print_assert_equal",
    "module": "torch.storage.np.testing",
    "fullName": "torch.storage.np.testing.print_assert_equal",
    "signature": "(test_string, actual, desired)",
    "description": "Test if two objects are equal, and print an error message if test fails."
  },
  "929": {
    "name": "raises",
    "module": "torch.storage.np.testing",
    "fullName": "torch.storage.np.testing.raises",
    "signature": "(*args)",
    "description": "Decorator to check for raised exceptions."
  },
  "930": {
    "name": "run_module_suite",
    "module": "torch.storage.np.testing",
    "fullName": "torch.storage.np.testing.run_module_suite",
    "signature": "(file_to_run=None, argv=None)",
    "description": "Run a test module."
  },
  "931": {
    "name": "rundocs",
    "module": "torch.storage.np.testing",
    "fullName": "torch.storage.np.testing.rundocs",
    "signature": "(filename=None, raise_on_error=True)",
    "description": "Run doctests found in the given file."
  },
  "932": {
    "name": "runstring",
    "module": "torch.storage.np.testing",
    "fullName": "torch.storage.np.testing.runstring",
    "signature": "(astr, dict)",
    "description": "No description available."
  },
  "933": {
    "name": "tempdir",
    "module": "torch.storage.np.testing",
    "fullName": "torch.storage.np.testing.tempdir",
    "signature": "(*args, **kwargs)",
    "description": "Context manager to provide a temporary test folder."
  },
  "934": {
    "name": "temppath",
    "module": "torch.storage.np.testing",
    "fullName": "torch.storage.np.testing.temppath",
    "signature": "(*args, **kwargs)",
    "description": "Context manager for temporary files."
  },
  "935": {
    "name": "build",
    "module": "torch.storage.np.testing.extbuild",
    "fullName": "torch.storage.np.testing.extbuild.build",
    "signature": "(cfile, outputfilename, compile_extra, link_extra, include_dirs, libraries, library_dirs)",
    "description": "cd into the directory where the cfile is, use distutils to build"
  },
  "936": {
    "name": "build_and_import_extension",
    "module": "torch.storage.np.testing.extbuild",
    "fullName": "torch.storage.np.testing.extbuild.build_and_import_extension",
    "signature": "(modname, functions, *, prologue='', build_dir=None, include_dirs=[], more_init='')",
    "description": "Build and imports a c-extension module `modname` from a list of function"
  },
  "937": {
    "name": "compile_extension_module",
    "module": "torch.storage.np.testing.extbuild",
    "fullName": "torch.storage.np.testing.extbuild.compile_extension_module",
    "signature": "(name, builddir, include_dirs, source_string, libraries=[], library_dirs=[])",
    "description": "Build an extension module and return the filename of the resulting"
  },
  "938": {
    "name": "get_so_suffix",
    "module": "torch.storage.np.testing.extbuild",
    "fullName": "torch.storage.np.testing.extbuild.get_so_suffix",
    "signature": "()",
    "description": "No description available."
  },
  "939": {
    "name": "get_config_h_filename",
    "module": "torch.storage.np.testing.extbuild.sysconfig",
    "fullName": "torch.storage.np.testing.extbuild.sysconfig.get_config_h_filename",
    "signature": "()",
    "description": "Return the path of pyconfig.h."
  },
  "940": {
    "name": "get_config_var",
    "module": "torch.storage.np.testing.extbuild.sysconfig",
    "fullName": "torch.storage.np.testing.extbuild.sysconfig.get_config_var",
    "signature": "(name)",
    "description": "Return the value of a single variable using the dictionary returned by"
  },
  "941": {
    "name": "get_config_vars",
    "module": "torch.storage.np.testing.extbuild.sysconfig",
    "fullName": "torch.storage.np.testing.extbuild.sysconfig.get_config_vars",
    "signature": "(*args)",
    "description": "With no arguments, return a dictionary of all configuration"
  },
  "942": {
    "name": "get_makefile_filename",
    "module": "torch.storage.np.testing.extbuild.sysconfig",
    "fullName": "torch.storage.np.testing.extbuild.sysconfig.get_makefile_filename",
    "signature": "()",
    "description": "Return the path of the Makefile."
  },
  "943": {
    "name": "get_path",
    "module": "torch.storage.np.testing.extbuild.sysconfig",
    "fullName": "torch.storage.np.testing.extbuild.sysconfig.get_path",
    "signature": "(name, scheme='posix_prefix', vars=None, expand=True)",
    "description": "Return a path corresponding to the scheme."
  },
  "944": {
    "name": "get_path_names",
    "module": "torch.storage.np.testing.extbuild.sysconfig",
    "fullName": "torch.storage.np.testing.extbuild.sysconfig.get_path_names",
    "signature": "()",
    "description": "Return a tuple containing the paths names."
  },
  "945": {
    "name": "get_paths",
    "module": "torch.storage.np.testing.extbuild.sysconfig",
    "fullName": "torch.storage.np.testing.extbuild.sysconfig.get_paths",
    "signature": "(scheme='posix_prefix', vars=None, expand=True)",
    "description": "Return a mapping containing an install scheme."
  },
  "946": {
    "name": "get_platform",
    "module": "torch.storage.np.testing.extbuild.sysconfig",
    "fullName": "torch.storage.np.testing.extbuild.sysconfig.get_platform",
    "signature": "()",
    "description": "Return a string that identifies the current platform."
  },
  "947": {
    "name": "get_python_version",
    "module": "torch.storage.np.testing.extbuild.sysconfig",
    "fullName": "torch.storage.np.testing.extbuild.sysconfig.get_python_version",
    "signature": "()",
    "description": "No description available."
  },
  "948": {
    "name": "get_scheme_names",
    "module": "torch.storage.np.testing.extbuild.sysconfig",
    "fullName": "torch.storage.np.testing.extbuild.sysconfig.get_scheme_names",
    "signature": "()",
    "description": "Return a tuple containing the schemes names."
  },
  "949": {
    "name": "is_python_build",
    "module": "torch.storage.np.testing.extbuild.sysconfig",
    "fullName": "torch.storage.np.testing.extbuild.sysconfig.is_python_build",
    "signature": "(check_home=False)",
    "description": "No description available."
  },
  "950": {
    "name": "parse_config_h",
    "module": "torch.storage.np.testing.extbuild.sysconfig",
    "fullName": "torch.storage.np.testing.extbuild.sysconfig.parse_config_h",
    "signature": "(fp, vars=None)",
    "description": "Parse a config.h-style file."
  },
  "951": {
    "name": "realpath",
    "module": "torch.storage.np.testing.extbuild.sysconfig",
    "fullName": "torch.storage.np.testing.extbuild.sysconfig.realpath",
    "signature": "(filename)",
    "description": "Return the canonical path of the specified filename, eliminating any"
  },
  "952": {
    "name": "urlquote_from_bytes",
    "module": "torch.storage.np.testing.extbuild.pathlib",
    "fullName": "torch.storage.np.testing.extbuild.pathlib.urlquote_from_bytes",
    "signature": "(bs, safe='/')",
    "description": "Like quote(), but accepts a bytes object rather than a str, and does"
  },
  "953": {
    "name": "abspath",
    "module": "torch.storage.np.testing.extbuild.pathlib.ntpath",
    "fullName": "torch.storage.np.testing.extbuild.pathlib.ntpath.abspath",
    "signature": "(path)",
    "description": "Return the absolute version of a path as a fallback function in case"
  },
  "954": {
    "name": "basename",
    "module": "torch.storage.np.testing.extbuild.pathlib.ntpath",
    "fullName": "torch.storage.np.testing.extbuild.pathlib.ntpath.basename",
    "signature": "(p)",
    "description": "Returns the final component of a pathname"
  },
  "955": {
    "name": "commonpath",
    "module": "torch.storage.np.testing.extbuild.pathlib.ntpath",
    "fullName": "torch.storage.np.testing.extbuild.pathlib.ntpath.commonpath",
    "signature": "(paths)",
    "description": "Given a sequence of path names, returns the longest common sub-path."
  },
  "956": {
    "name": "commonprefix",
    "module": "torch.storage.np.testing.extbuild.pathlib.ntpath",
    "fullName": "torch.storage.np.testing.extbuild.pathlib.ntpath.commonprefix",
    "signature": "(m)",
    "description": "Given a list of pathnames, returns the longest common leading component"
  },
  "957": {
    "name": "dirname",
    "module": "torch.storage.np.testing.extbuild.pathlib.ntpath",
    "fullName": "torch.storage.np.testing.extbuild.pathlib.ntpath.dirname",
    "signature": "(p)",
    "description": "Returns the directory component of a pathname"
  },
  "958": {
    "name": "exists",
    "module": "torch.storage.np.testing.extbuild.pathlib.ntpath",
    "fullName": "torch.storage.np.testing.extbuild.pathlib.ntpath.exists",
    "signature": "(path)",
    "description": "Test whether a path exists.  Returns False for broken symbolic links"
  },
  "959": {
    "name": "expanduser",
    "module": "torch.storage.np.testing.extbuild.pathlib.ntpath",
    "fullName": "torch.storage.np.testing.extbuild.pathlib.ntpath.expanduser",
    "signature": "(path)",
    "description": "Expand ~ and ~user constructs."
  },
  "960": {
    "name": "expandvars",
    "module": "torch.storage.np.testing.extbuild.pathlib.ntpath",
    "fullName": "torch.storage.np.testing.extbuild.pathlib.ntpath.expandvars",
    "signature": "(path)",
    "description": "Expand shell variables of the forms $var, ${var} and %var%."
  },
  "961": {
    "name": "getatime",
    "module": "torch.storage.np.testing.extbuild.pathlib.ntpath",
    "fullName": "torch.storage.np.testing.extbuild.pathlib.ntpath.getatime",
    "signature": "(filename)",
    "description": "Return the last access time of a file, reported by os.stat()."
  },
  "962": {
    "name": "getctime",
    "module": "torch.storage.np.testing.extbuild.pathlib.ntpath",
    "fullName": "torch.storage.np.testing.extbuild.pathlib.ntpath.getctime",
    "signature": "(filename)",
    "description": "Return the metadata change time of a file, reported by os.stat()."
  },
  "963": {
    "name": "getmtime",
    "module": "torch.storage.np.testing.extbuild.pathlib.ntpath",
    "fullName": "torch.storage.np.testing.extbuild.pathlib.ntpath.getmtime",
    "signature": "(filename)",
    "description": "Return the last modification time of a file, reported by os.stat()."
  },
  "964": {
    "name": "getsize",
    "module": "torch.storage.np.testing.extbuild.pathlib.ntpath",
    "fullName": "torch.storage.np.testing.extbuild.pathlib.ntpath.getsize",
    "signature": "(filename)",
    "description": "Return the size of a file, reported by os.stat()."
  },
  "965": {
    "name": "isabs",
    "module": "torch.storage.np.testing.extbuild.pathlib.ntpath",
    "fullName": "torch.storage.np.testing.extbuild.pathlib.ntpath.isabs",
    "signature": "(s)",
    "description": "Test whether a path is absolute"
  },
  "966": {
    "name": "isdir",
    "module": "torch.storage.np.testing.extbuild.pathlib.ntpath",
    "fullName": "torch.storage.np.testing.extbuild.pathlib.ntpath.isdir",
    "signature": "(s)",
    "description": "Return true if the pathname refers to an existing directory."
  },
  "967": {
    "name": "isfile",
    "module": "torch.storage.np.testing.extbuild.pathlib.ntpath",
    "fullName": "torch.storage.np.testing.extbuild.pathlib.ntpath.isfile",
    "signature": "(path)",
    "description": "Test whether a path is a regular file"
  },
  "968": {
    "name": "islink",
    "module": "torch.storage.np.testing.extbuild.pathlib.ntpath",
    "fullName": "torch.storage.np.testing.extbuild.pathlib.ntpath.islink",
    "signature": "(path)",
    "description": "Test whether a path is a symbolic link."
  },
  "969": {
    "name": "ismount",
    "module": "torch.storage.np.testing.extbuild.pathlib.ntpath",
    "fullName": "torch.storage.np.testing.extbuild.pathlib.ntpath.ismount",
    "signature": "(path)",
    "description": "Test whether a path is a mount point (a drive root, the root of a"
  },
  "970": {
    "name": "join",
    "module": "torch.storage.np.testing.extbuild.pathlib.ntpath",
    "fullName": "torch.storage.np.testing.extbuild.pathlib.ntpath.join",
    "signature": "(path, *paths)",
    "description": "No description available."
  },
  "971": {
    "name": "lexists",
    "module": "torch.storage.np.testing.extbuild.pathlib.ntpath",
    "fullName": "torch.storage.np.testing.extbuild.pathlib.ntpath.lexists",
    "signature": "(path)",
    "description": "Test whether a path exists.  Returns True for broken symbolic links"
  },
  "972": {
    "name": "normcase",
    "module": "torch.storage.np.testing.extbuild.pathlib.ntpath",
    "fullName": "torch.storage.np.testing.extbuild.pathlib.ntpath.normcase",
    "signature": "(s)",
    "description": "Normalize case of pathname."
  },
  "973": {
    "name": "normpath",
    "module": "torch.storage.np.testing.extbuild.pathlib.ntpath",
    "fullName": "torch.storage.np.testing.extbuild.pathlib.ntpath.normpath",
    "signature": "(path)",
    "description": "Normalize path, eliminating double slashes, etc."
  },
  "974": {
    "name": "realpath",
    "module": "torch.storage.np.testing.extbuild.pathlib.ntpath",
    "fullName": "torch.storage.np.testing.extbuild.pathlib.ntpath.realpath",
    "signature": "(path)",
    "description": "Return the absolute version of a path as a fallback function in case"
  },
  "975": {
    "name": "relpath",
    "module": "torch.storage.np.testing.extbuild.pathlib.ntpath",
    "fullName": "torch.storage.np.testing.extbuild.pathlib.ntpath.relpath",
    "signature": "(path, start=None)",
    "description": "Return a relative version of a path"
  },
  "976": {
    "name": "samefile",
    "module": "torch.storage.np.testing.extbuild.pathlib.ntpath",
    "fullName": "torch.storage.np.testing.extbuild.pathlib.ntpath.samefile",
    "signature": "(f1, f2)",
    "description": "Test whether two pathnames reference the same actual file or directory"
  },
  "977": {
    "name": "sameopenfile",
    "module": "torch.storage.np.testing.extbuild.pathlib.ntpath",
    "fullName": "torch.storage.np.testing.extbuild.pathlib.ntpath.sameopenfile",
    "signature": "(fp1, fp2)",
    "description": "Test whether two open file objects reference the same file"
  },
  "978": {
    "name": "samestat",
    "module": "torch.storage.np.testing.extbuild.pathlib.ntpath",
    "fullName": "torch.storage.np.testing.extbuild.pathlib.ntpath.samestat",
    "signature": "(s1, s2)",
    "description": "Test whether two stat buffers reference the same file"
  },
  "979": {
    "name": "split",
    "module": "torch.storage.np.testing.extbuild.pathlib.ntpath",
    "fullName": "torch.storage.np.testing.extbuild.pathlib.ntpath.split",
    "signature": "(p)",
    "description": "Split a pathname."
  },
  "980": {
    "name": "splitdrive",
    "module": "torch.storage.np.testing.extbuild.pathlib.ntpath",
    "fullName": "torch.storage.np.testing.extbuild.pathlib.ntpath.splitdrive",
    "signature": "(p)",
    "description": "Split a pathname into drive/UNC sharepoint and relative path specifiers."
  },
  "981": {
    "name": "splitext",
    "module": "torch.storage.np.testing.extbuild.pathlib.ntpath",
    "fullName": "torch.storage.np.testing.extbuild.pathlib.ntpath.splitext",
    "signature": "(p)",
    "description": "Split the extension from a pathname."
  },
  "982": {
    "name": "array",
    "module": "torch.storage.np.rec",
    "fullName": "torch.storage.np.rec.array",
    "signature": "(obj, dtype=None, shape=None, offset=0, strides=None, formats=None, names=None, titles=None, aligned=False, byteorder=None, copy=True)",
    "description": "Construct a record array from a wide-variety of objects."
  },
  "983": {
    "name": "find_duplicate",
    "module": "torch.storage.np.rec",
    "fullName": "torch.storage.np.rec.find_duplicate",
    "signature": "(list)",
    "description": "Find duplication in a list, return a list of duplicated elements"
  },
  "984": {
    "name": "fromarrays",
    "module": "torch.storage.np.rec",
    "fullName": "torch.storage.np.rec.fromarrays",
    "signature": "(arrayList, dtype=None, shape=None, formats=None, names=None, titles=None, aligned=False, byteorder=None)",
    "description": "Create a record array from a (flat) list of arrays"
  },
  "985": {
    "name": "fromfile",
    "module": "torch.storage.np.rec",
    "fullName": "torch.storage.np.rec.fromfile",
    "signature": "(fd, dtype=None, shape=None, offset=0, formats=None, names=None, titles=None, aligned=False, byteorder=None)",
    "description": "Create an array from binary file data"
  },
  "986": {
    "name": "fromrecords",
    "module": "torch.storage.np.rec",
    "fullName": "torch.storage.np.rec.fromrecords",
    "signature": "(recList, dtype=None, shape=None, formats=None, names=None, titles=None, aligned=False, byteorder=None)",
    "description": "Create a recarray from a list of records in text form."
  },
  "987": {
    "name": "fromstring",
    "module": "torch.storage.np.rec",
    "fullName": "torch.storage.np.rec.fromstring",
    "signature": "(datastring, dtype=None, shape=None, offset=0, formats=None, names=None, titles=None, aligned=False, byteorder=None)",
    "description": "Create a record array from binary data"
  },
  "988": {
    "name": "get_remaining_size",
    "module": "torch.storage.np.rec",
    "fullName": "torch.storage.np.rec.get_remaining_size",
    "signature": "(fd)",
    "description": "No description available."
  },
  "989": {
    "name": "set_module",
    "module": "torch.storage.np.rec",
    "fullName": "torch.storage.np.rec.set_module",
    "signature": "(module)",
    "description": "Decorator for overriding __module__ on a function or class."
  },
  "990": {
    "name": "all",
    "module": "torch.storage.np.rec.sb",
    "fullName": "torch.storage.np.rec.sb.all",
    "signature": "(a, axis=None, out=None, keepdims=<no value>, *, where=<no value>)",
    "description": "Test whether all array elements along a given axis evaluate to True."
  },
  "991": {
    "name": "allclose",
    "module": "torch.storage.np.rec.sb",
    "fullName": "torch.storage.np.rec.sb.allclose",
    "signature": "(a, b, rtol=1e-05, atol=1e-08, equal_nan=False)",
    "description": "Returns True if two arrays are element-wise equal within a tolerance."
  },
  "992": {
    "name": "alltrue",
    "module": "torch.storage.np.rec.sb",
    "fullName": "torch.storage.np.rec.sb.alltrue",
    "signature": "(*args, **kwargs)",
    "description": "Check if all elements of input array are true."
  },
  "993": {
    "name": "amax",
    "module": "torch.storage.np.rec.sb",
    "fullName": "torch.storage.np.rec.sb.amax",
    "signature": "(a, axis=None, out=None, keepdims=<no value>, initial=<no value>, where=<no value>)",
    "description": "Return the maximum of an array or maximum along an axis."
  },
  "994": {
    "name": "amin",
    "module": "torch.storage.np.rec.sb",
    "fullName": "torch.storage.np.rec.sb.amin",
    "signature": "(a, axis=None, out=None, keepdims=<no value>, initial=<no value>, where=<no value>)",
    "description": "Return the minimum of an array or minimum along an axis."
  },
  "995": {
    "name": "any",
    "module": "torch.storage.np.rec.sb",
    "fullName": "torch.storage.np.rec.sb.any",
    "signature": "(a, axis=None, out=None, keepdims=<no value>, *, where=<no value>)",
    "description": "Test whether any array element along a given axis evaluates to True."
  },
  "996": {
    "name": "argmax",
    "module": "torch.storage.np.rec.sb",
    "fullName": "torch.storage.np.rec.sb.argmax",
    "signature": "(a, axis=None, out=None, *, keepdims=<no value>)",
    "description": "Returns the indices of the maximum values along an axis."
  },
  "997": {
    "name": "argmin",
    "module": "torch.storage.np.rec.sb",
    "fullName": "torch.storage.np.rec.sb.argmin",
    "signature": "(a, axis=None, out=None, *, keepdims=<no value>)",
    "description": "Returns the indices of the minimum values along an axis."
  },
  "998": {
    "name": "argsort",
    "module": "torch.storage.np.rec.sb",
    "fullName": "torch.storage.np.rec.sb.argsort",
    "signature": "(a, axis=-1, kind=None, order=None)",
    "description": "Returns the indices that would sort an array."
  },
  "999": {
    "name": "argwhere",
    "module": "torch.storage.np.rec.sb",
    "fullName": "torch.storage.np.rec.sb.argwhere",
    "signature": "(a)",
    "description": "Find the indices of array elements that are non-zero, grouped by element."
  },
  "1000": {
    "name": "around",
    "module": "torch.storage.np.rec.sb",
    "fullName": "torch.storage.np.rec.sb.around",
    "signature": "(a, decimals=0, out=None)",
    "description": "Evenly round to the given number of decimals."
  },
  "1001": {
    "name": "array_equal",
    "module": "torch.storage.np.rec.sb",
    "fullName": "torch.storage.np.rec.sb.array_equal",
    "signature": "(a1, a2, equal_nan=False)",
    "description": "True if two arrays have the same shape and elements, False otherwise."
  },
  "1002": {
    "name": "array_equiv",
    "module": "torch.storage.np.rec.sb",
    "fullName": "torch.storage.np.rec.sb.array_equiv",
    "signature": "(a1, a2)",
    "description": "Returns True if input arrays are shape consistent and all elements equal."
  },
  "1003": {
    "name": "array_repr",
    "module": "torch.storage.np.rec.sb",
    "fullName": "torch.storage.np.rec.sb.array_repr",
    "signature": "(arr, max_line_width=None, precision=None, suppress_small=None)",
    "description": "Return the string representation of an array."
  },
  "1004": {
    "name": "array_str",
    "module": "torch.storage.np.rec.sb",
    "fullName": "torch.storage.np.rec.sb.array_str",
    "signature": "(a, max_line_width=None, precision=None, suppress_small=None)",
    "description": "Return a string representation of the data in an array."
  },
  "1005": {
    "name": "base_repr",
    "module": "torch.storage.np.rec.sb",
    "fullName": "torch.storage.np.rec.sb.base_repr",
    "signature": "(number, base=2, padding=0)",
    "description": "Return a string representation of a number in the given base system."
  },
  "1006": {
    "name": "busday_count",
    "module": "torch.storage.np.rec.sb",
    "fullName": "torch.storage.np.rec.sb.busday_count",
    "signature": "N/A",
    "description": "busday_count(begindates, enddates, weekmask='1111100', holidays=[], busdaycal=None, out=None)"
  },
  "1007": {
    "name": "busday_offset",
    "module": "torch.storage.np.rec.sb",
    "fullName": "torch.storage.np.rec.sb.busday_offset",
    "signature": "N/A",
    "description": "busday_offset(dates, offsets, roll='raise', weekmask='1111100', holidays=None, busdaycal=None, out=None)"
  },
  "1008": {
    "name": "can_cast",
    "module": "torch.storage.np.rec.sb",
    "fullName": "torch.storage.np.rec.sb.can_cast",
    "signature": "N/A",
    "description": "can_cast(from_, to, casting='safe')"
  },
  "1009": {
    "name": "choose",
    "module": "torch.storage.np.rec.sb",
    "fullName": "torch.storage.np.rec.sb.choose",
    "signature": "(a, choices, out=None, mode='raise')",
    "description": "Construct an array from an index array and a list of arrays to choose from."
  },
  "1010": {
    "name": "clip",
    "module": "torch.storage.np.rec.sb",
    "fullName": "torch.storage.np.rec.sb.clip",
    "signature": "(a, a_min, a_max, out=None, **kwargs)",
    "description": "Clip (limit) the values in an array."
  },
  "1011": {
    "name": "compress",
    "module": "torch.storage.np.rec.sb",
    "fullName": "torch.storage.np.rec.sb.compress",
    "signature": "(condition, a, axis=None, out=None)",
    "description": "Return selected slices of an array along given axis."
  },
  "1012": {
    "name": "concatenate",
    "module": "torch.storage.np.rec.sb",
    "fullName": "torch.storage.np.rec.sb.concatenate",
    "signature": "N/A",
    "description": "concatenate((a1, a2, ...), axis=0, out=None, dtype=None, casting=\"same_kind\")"
  },
  "1013": {
    "name": "convolve",
    "module": "torch.storage.np.rec.sb",
    "fullName": "torch.storage.np.rec.sb.convolve",
    "signature": "(a, v, mode='full')",
    "description": "Returns the discrete, linear convolution of two one-dimensional sequences."
  },
  "1014": {
    "name": "copyto",
    "module": "torch.storage.np.rec.sb",
    "fullName": "torch.storage.np.rec.sb.copyto",
    "signature": "N/A",
    "description": "copyto(dst, src, casting='same_kind', where=True)"
  },
  "1015": {
    "name": "correlate",
    "module": "torch.storage.np.rec.sb",
    "fullName": "torch.storage.np.rec.sb.correlate",
    "signature": "(a, v, mode='valid')",
    "description": "Cross-correlation of two 1-dimensional sequences."
  },
  "1016": {
    "name": "count_nonzero",
    "module": "torch.storage.np.rec.sb",
    "fullName": "torch.storage.np.rec.sb.count_nonzero",
    "signature": "(a, axis=None, *, keepdims=False)",
    "description": "Counts the number of non-zero values in the array ``a``."
  },
  "1017": {
    "name": "cross",
    "module": "torch.storage.np.rec.sb",
    "fullName": "torch.storage.np.rec.sb.cross",
    "signature": "(a, b, axisa=-1, axisb=-1, axisc=-1, axis=None)",
    "description": "Return the cross product of two (arrays of) vectors."
  },
  "1018": {
    "name": "cumprod",
    "module": "torch.storage.np.rec.sb",
    "fullName": "torch.storage.np.rec.sb.cumprod",
    "signature": "(a, axis=None, dtype=None, out=None)",
    "description": "Return the cumulative product of elements along a given axis."
  },
  "1019": {
    "name": "cumproduct",
    "module": "torch.storage.np.rec.sb",
    "fullName": "torch.storage.np.rec.sb.cumproduct",
    "signature": "(*args, **kwargs)",
    "description": "Return the cumulative product over the given axis."
  },
  "1020": {
    "name": "cumsum",
    "module": "torch.storage.np.rec.sb",
    "fullName": "torch.storage.np.rec.sb.cumsum",
    "signature": "(a, axis=None, dtype=None, out=None)",
    "description": "Return the cumulative sum of the elements along a given axis."
  },
  "1021": {
    "name": "datetime_as_string",
    "module": "torch.storage.np.rec.sb",
    "fullName": "torch.storage.np.rec.sb.datetime_as_string",
    "signature": "N/A",
    "description": "datetime_as_string(arr, unit=None, timezone='naive', casting='same_kind')"
  },
  "1022": {
    "name": "dot",
    "module": "torch.storage.np.rec.sb",
    "fullName": "torch.storage.np.rec.sb.dot",
    "signature": "N/A",
    "description": "dot(a, b, out=None)"
  },
  "1023": {
    "name": "empty_like",
    "module": "torch.storage.np.rec.sb",
    "fullName": "torch.storage.np.rec.sb.empty_like",
    "signature": "N/A",
    "description": "empty_like(prototype, dtype=None, order='K', subok=True, shape=None)"
  },
  "1024": {
    "name": "extend_all",
    "module": "torch.storage.np.rec.sb",
    "fullName": "torch.storage.np.rec.sb.extend_all",
    "signature": "(module)",
    "description": "No description available."
  },
  "1025": {
    "name": "find_common_type",
    "module": "torch.storage.np.rec.sb",
    "fullName": "torch.storage.np.rec.sb.find_common_type",
    "signature": "(array_types, scalar_types)",
    "description": "Determine common type following standard coercion rules."
  },
  "1026": {
    "name": "flatnonzero",
    "module": "torch.storage.np.rec.sb",
    "fullName": "torch.storage.np.rec.sb.flatnonzero",
    "signature": "(a)",
    "description": "Return indices that are non-zero in the flattened version of a."
  },
  "1027": {
    "name": "format_float_positional",
    "module": "torch.storage.np.rec.sb",
    "fullName": "torch.storage.np.rec.sb.format_float_positional",
    "signature": "(x, precision=None, unique=True, fractional=True, trim='k', sign=False, pad_left=None, pad_right=None, min_digits=None)",
    "description": "Format a floating-point scalar as a decimal string in positional notation."
  },
  "1028": {
    "name": "format_float_scientific",
    "module": "torch.storage.np.rec.sb",
    "fullName": "torch.storage.np.rec.sb.format_float_scientific",
    "signature": "(x, precision=None, unique=True, trim='k', sign=False, pad_left=None, exp_digits=None, min_digits=None)",
    "description": "Format a floating-point scalar as a decimal string in scientific notation."
  },
  "1029": {
    "name": "fromfunction",
    "module": "torch.storage.np.rec.sb",
    "fullName": "torch.storage.np.rec.sb.fromfunction",
    "signature": "(function, shape, *, dtype=<class 'float'>, like=None, **kwargs)",
    "description": "Construct an array by executing a function over each coordinate."
  },
  "1030": {
    "name": "full",
    "module": "torch.storage.np.rec.sb",
    "fullName": "torch.storage.np.rec.sb.full",
    "signature": "(shape, fill_value, dtype=None, order='C', *, like=None)",
    "description": "Return a new array of given shape and type, filled with `fill_value`."
  },
  "1031": {
    "name": "full_like",
    "module": "torch.storage.np.rec.sb",
    "fullName": "torch.storage.np.rec.sb.full_like",
    "signature": "(a, fill_value, dtype=None, order='K', subok=True, shape=None)",
    "description": "Return a full array with the same shape and type as a given array."
  },
  "1032": {
    "name": "get_printoptions",
    "module": "torch.storage.np.rec.sb",
    "fullName": "torch.storage.np.rec.sb.get_printoptions",
    "signature": "()",
    "description": "Return the current print options."
  },
  "1033": {
    "name": "getbufsize",
    "module": "torch.storage.np.rec.sb",
    "fullName": "torch.storage.np.rec.sb.getbufsize",
    "signature": "()",
    "description": "Return the size of the buffer used in ufuncs."
  },
  "1034": {
    "name": "geterr",
    "module": "torch.storage.np.rec.sb",
    "fullName": "torch.storage.np.rec.sb.geterr",
    "signature": "()",
    "description": "Get the current way of handling floating-point errors."
  },
  "1035": {
    "name": "geterrcall",
    "module": "torch.storage.np.rec.sb",
    "fullName": "torch.storage.np.rec.sb.geterrcall",
    "signature": "()",
    "description": "Return the current callback function used on floating-point errors."
  },
  "1036": {
    "name": "identity",
    "module": "torch.storage.np.rec.sb",
    "fullName": "torch.storage.np.rec.sb.identity",
    "signature": "(n, dtype=None, *, like=None)",
    "description": "Return the identity array."
  },
  "1037": {
    "name": "indices",
    "module": "torch.storage.np.rec.sb",
    "fullName": "torch.storage.np.rec.sb.indices",
    "signature": "(dimensions, dtype=<class 'int'>, sparse=False)",
    "description": "Return an array representing the indices of a grid."
  },
  "1038": {
    "name": "inner",
    "module": "torch.storage.np.rec.sb",
    "fullName": "torch.storage.np.rec.sb.inner",
    "signature": "N/A",
    "description": "inner(a, b, /)"
  },
  "1039": {
    "name": "is_busday",
    "module": "torch.storage.np.rec.sb",
    "fullName": "torch.storage.np.rec.sb.is_busday",
    "signature": "N/A",
    "description": "is_busday(dates, weekmask='1111100', holidays=None, busdaycal=None, out=None)"
  },
  "1040": {
    "name": "isclose",
    "module": "torch.storage.np.rec.sb",
    "fullName": "torch.storage.np.rec.sb.isclose",
    "signature": "(a, b, rtol=1e-05, atol=1e-08, equal_nan=False)",
    "description": "Returns a boolean array where two arrays are element-wise equal within a"
  },
  "1041": {
    "name": "isfortran",
    "module": "torch.storage.np.rec.sb",
    "fullName": "torch.storage.np.rec.sb.isfortran",
    "signature": "(a)",
    "description": "Check if the array is Fortran contiguous but *not* C contiguous."
  },
  "1042": {
    "name": "isscalar",
    "module": "torch.storage.np.rec.sb",
    "fullName": "torch.storage.np.rec.sb.isscalar",
    "signature": "(element)",
    "description": "Returns True if the type of `element` is a scalar type."
  },
  "1043": {
    "name": "issctype",
    "module": "torch.storage.np.rec.sb",
    "fullName": "torch.storage.np.rec.sb.issctype",
    "signature": "(rep)",
    "description": "Determines whether the given object represents a scalar data-type."
  },
  "1044": {
    "name": "issubdtype",
    "module": "torch.storage.np.rec.sb",
    "fullName": "torch.storage.np.rec.sb.issubdtype",
    "signature": "(arg1, arg2)",
    "description": "Returns True if first argument is a typecode lower/equal in type hierarchy."
  },
  "1045": {
    "name": "lexsort",
    "module": "torch.storage.np.rec.sb",
    "fullName": "torch.storage.np.rec.sb.lexsort",
    "signature": "N/A",
    "description": "lexsort(keys, axis=-1)"
  },
  "1046": {
    "name": "maximum_sctype",
    "module": "torch.storage.np.rec.sb",
    "fullName": "torch.storage.np.rec.sb.maximum_sctype",
    "signature": "(t)",
    "description": "Return the scalar type of highest precision of the same kind as the input."
  },
  "1047": {
    "name": "may_share_memory",
    "module": "torch.storage.np.rec.sb",
    "fullName": "torch.storage.np.rec.sb.may_share_memory",
    "signature": "N/A",
    "description": "may_share_memory(a, b, /, max_work=None)"
  },
  "1048": {
    "name": "mean",
    "module": "torch.storage.np.rec.sb",
    "fullName": "torch.storage.np.rec.sb.mean",
    "signature": "(a, axis=None, dtype=None, out=None, keepdims=<no value>, *, where=<no value>)",
    "description": "Compute the arithmetic mean along the specified axis."
  },
  "1049": {
    "name": "min_scalar_type",
    "module": "torch.storage.np.rec.sb",
    "fullName": "torch.storage.np.rec.sb.min_scalar_type",
    "signature": "N/A",
    "description": "min_scalar_type(a, /)"
  },
  "1050": {
    "name": "moveaxis",
    "module": "torch.storage.np.rec.sb",
    "fullName": "torch.storage.np.rec.sb.moveaxis",
    "signature": "(a, source, destination)",
    "description": "Move axes of an array to new positions."
  },
  "1051": {
    "name": "ndim",
    "module": "torch.storage.np.rec.sb",
    "fullName": "torch.storage.np.rec.sb.ndim",
    "signature": "(a)",
    "description": "Return the number of dimensions of an array."
  },
  "1052": {
    "name": "normalize_axis_tuple",
    "module": "torch.storage.np.rec.sb",
    "fullName": "torch.storage.np.rec.sb.normalize_axis_tuple",
    "signature": "(axis, ndim, argname=None, allow_duplicate=False)",
    "description": "Normalizes an axis argument into a tuple of non-negative integer axes."
  },
  "1053": {
    "name": "obj2sctype",
    "module": "torch.storage.np.rec.sb",
    "fullName": "torch.storage.np.rec.sb.obj2sctype",
    "signature": "(rep, default=None)",
    "description": "Return the scalar dtype or NumPy equivalent of Python type of an object."
  },
  "1054": {
    "name": "ones",
    "module": "torch.storage.np.rec.sb",
    "fullName": "torch.storage.np.rec.sb.ones",
    "signature": "(shape, dtype=None, order='C', *, like=None)",
    "description": "Return a new array of given shape and type, filled with ones."
  },
  "1055": {
    "name": "ones_like",
    "module": "torch.storage.np.rec.sb",
    "fullName": "torch.storage.np.rec.sb.ones_like",
    "signature": "(a, dtype=None, order='K', subok=True, shape=None)",
    "description": "Return an array of ones with the same shape and type as a given array."
  },
  "1056": {
    "name": "outer",
    "module": "torch.storage.np.rec.sb",
    "fullName": "torch.storage.np.rec.sb.outer",
    "signature": "(a, b, out=None)",
    "description": "Compute the outer product of two vectors."
  },
  "1057": {
    "name": "printoptions",
    "module": "torch.storage.np.rec.sb",
    "fullName": "torch.storage.np.rec.sb.printoptions",
    "signature": "(*args, **kwargs)",
    "description": "Context manager for setting print options."
  },
  "1058": {
    "name": "prod",
    "module": "torch.storage.np.rec.sb",
    "fullName": "torch.storage.np.rec.sb.prod",
    "signature": "(a, axis=None, dtype=None, out=None, keepdims=<no value>, initial=<no value>, where=<no value>)",
    "description": "Return the product of array elements over a given axis."
  },
  "1059": {
    "name": "product",
    "module": "torch.storage.np.rec.sb",
    "fullName": "torch.storage.np.rec.sb.product",
    "signature": "(*args, **kwargs)",
    "description": "Return the product of array elements over a given axis."
  },
  "1060": {
    "name": "ptp",
    "module": "torch.storage.np.rec.sb",
    "fullName": "torch.storage.np.rec.sb.ptp",
    "signature": "(a, axis=None, out=None, keepdims=<no value>)",
    "description": "Range of values (maximum - minimum) along an axis."
  },
  "1061": {
    "name": "put",
    "module": "torch.storage.np.rec.sb",
    "fullName": "torch.storage.np.rec.sb.put",
    "signature": "(a, ind, v, mode='raise')",
    "description": "Replaces specified elements of an array with given values."
  },
  "1062": {
    "name": "putmask",
    "module": "torch.storage.np.rec.sb",
    "fullName": "torch.storage.np.rec.sb.putmask",
    "signature": "N/A",
    "description": "putmask(a, mask, values)"
  },
  "1063": {
    "name": "ravel",
    "module": "torch.storage.np.rec.sb",
    "fullName": "torch.storage.np.rec.sb.ravel",
    "signature": "(a, order='C')",
    "description": "Return a contiguous flattened array."
  },
  "1064": {
    "name": "repeat",
    "module": "torch.storage.np.rec.sb",
    "fullName": "torch.storage.np.rec.sb.repeat",
    "signature": "(a, repeats, axis=None)",
    "description": "Repeat elements of an array."
  },
  "1065": {
    "name": "require",
    "module": "torch.storage.np.rec.sb",
    "fullName": "torch.storage.np.rec.sb.require",
    "signature": "(a, dtype=None, requirements=None, *, like=None)",
    "description": "Return an ndarray of the provided type that satisfies requirements."
  },
  "1066": {
    "name": "reshape",
    "module": "torch.storage.np.rec.sb",
    "fullName": "torch.storage.np.rec.sb.reshape",
    "signature": "(a, newshape, order='C')",
    "description": "Gives a new shape to an array without changing its data."
  },
  "1067": {
    "name": "resize",
    "module": "torch.storage.np.rec.sb",
    "fullName": "torch.storage.np.rec.sb.resize",
    "signature": "(a, new_shape)",
    "description": "Return a new array with the specified shape."
  },
  "1068": {
    "name": "result_type",
    "module": "torch.storage.np.rec.sb",
    "fullName": "torch.storage.np.rec.sb.result_type",
    "signature": "N/A",
    "description": "result_type(*arrays_and_dtypes)"
  },
  "1069": {
    "name": "roll",
    "module": "torch.storage.np.rec.sb",
    "fullName": "torch.storage.np.rec.sb.roll",
    "signature": "(a, shift, axis=None)",
    "description": "Roll array elements along a given axis."
  },
  "1070": {
    "name": "rollaxis",
    "module": "torch.storage.np.rec.sb",
    "fullName": "torch.storage.np.rec.sb.rollaxis",
    "signature": "(a, axis, start=0)",
    "description": "Roll the specified axis backwards, until it lies in a given position."
  },
  "1071": {
    "name": "round_",
    "module": "torch.storage.np.rec.sb",
    "fullName": "torch.storage.np.rec.sb.round_",
    "signature": "(a, decimals=0, out=None)",
    "description": "Round an array to the given number of decimals."
  },
  "1072": {
    "name": "sctype2char",
    "module": "torch.storage.np.rec.sb",
    "fullName": "torch.storage.np.rec.sb.sctype2char",
    "signature": "(sctype)",
    "description": "Return the string representation of a scalar dtype."
  },
  "1073": {
    "name": "searchsorted",
    "module": "torch.storage.np.rec.sb",
    "fullName": "torch.storage.np.rec.sb.searchsorted",
    "signature": "(a, v, side='left', sorter=None)",
    "description": "Find indices where elements should be inserted to maintain order."
  },
  "1074": {
    "name": "set_array_function_like_doc",
    "module": "torch.storage.np.rec.sb",
    "fullName": "torch.storage.np.rec.sb.set_array_function_like_doc",
    "signature": "(public_api)",
    "description": "No description available."
  },
  "1075": {
    "name": "set_module",
    "module": "torch.storage.np.rec.sb",
    "fullName": "torch.storage.np.rec.sb.set_module",
    "signature": "(module)",
    "description": "Decorator for overriding __module__ on a function or class."
  },
  "1076": {
    "name": "set_printoptions",
    "module": "torch.storage.np.rec.sb",
    "fullName": "torch.storage.np.rec.sb.set_printoptions",
    "signature": "(precision=None, threshold=None, edgeitems=None, linewidth=None, suppress=None, nanstr=None, infstr=None, formatter=None, sign=None, floatmode=None, *, legacy=None)",
    "description": "Set printing options."
  },
  "1077": {
    "name": "set_string_function",
    "module": "torch.storage.np.rec.sb",
    "fullName": "torch.storage.np.rec.sb.set_string_function",
    "signature": "(f, repr=True)",
    "description": "Set a Python function to be used when pretty printing arrays."
  },
  "1078": {
    "name": "setbufsize",
    "module": "torch.storage.np.rec.sb",
    "fullName": "torch.storage.np.rec.sb.setbufsize",
    "signature": "(size)",
    "description": "Set the size of the buffer used in ufuncs."
  },
  "1079": {
    "name": "seterr",
    "module": "torch.storage.np.rec.sb",
    "fullName": "torch.storage.np.rec.sb.seterr",
    "signature": "(all=None, divide=None, over=None, under=None, invalid=None)",
    "description": "Set how floating-point errors are handled."
  },
  "1080": {
    "name": "seterrcall",
    "module": "torch.storage.np.rec.sb",
    "fullName": "torch.storage.np.rec.sb.seterrcall",
    "signature": "(func)",
    "description": "Set the floating-point error callback function or log object."
  },
  "1081": {
    "name": "shape",
    "module": "torch.storage.np.rec.sb",
    "fullName": "torch.storage.np.rec.sb.shape",
    "signature": "(a)",
    "description": "Return the shape of an array."
  },
  "1082": {
    "name": "shares_memory",
    "module": "torch.storage.np.rec.sb",
    "fullName": "torch.storage.np.rec.sb.shares_memory",
    "signature": "N/A",
    "description": "shares_memory(a, b, /, max_work=None)"
  },
  "1083": {
    "name": "size",
    "module": "torch.storage.np.rec.sb",
    "fullName": "torch.storage.np.rec.sb.size",
    "signature": "(a, axis=None)",
    "description": "Return the number of elements along a given axis."
  },
  "1084": {
    "name": "sometrue",
    "module": "torch.storage.np.rec.sb",
    "fullName": "torch.storage.np.rec.sb.sometrue",
    "signature": "(*args, **kwargs)",
    "description": "Check whether some values are true."
  },
  "1085": {
    "name": "sort",
    "module": "torch.storage.np.rec.sb",
    "fullName": "torch.storage.np.rec.sb.sort",
    "signature": "(a, axis=-1, kind=None, order=None)",
    "description": "Return a sorted copy of an array."
  },
  "1086": {
    "name": "squeeze",
    "module": "torch.storage.np.rec.sb",
    "fullName": "torch.storage.np.rec.sb.squeeze",
    "signature": "(a, axis=None)",
    "description": "Remove axes of length one from `a`."
  },
  "1087": {
    "name": "std",
    "module": "torch.storage.np.rec.sb",
    "fullName": "torch.storage.np.rec.sb.std",
    "signature": "(a, axis=None, dtype=None, out=None, ddof=0, keepdims=<no value>, *, where=<no value>)",
    "description": "Compute the standard deviation along the specified axis."
  },
  "1088": {
    "name": "sum",
    "module": "torch.storage.np.rec.sb",
    "fullName": "torch.storage.np.rec.sb.sum",
    "signature": "(a, axis=None, dtype=None, out=None, keepdims=<no value>, initial=<no value>, where=<no value>)",
    "description": "Sum of array elements over a given axis."
  },
  "1089": {
    "name": "swapaxes",
    "module": "torch.storage.np.rec.sb",
    "fullName": "torch.storage.np.rec.sb.swapaxes",
    "signature": "(a, axis1, axis2)",
    "description": "Interchange two axes of an array."
  },
  "1090": {
    "name": "take",
    "module": "torch.storage.np.rec.sb",
    "fullName": "torch.storage.np.rec.sb.take",
    "signature": "(a, indices, axis=None, out=None, mode='raise')",
    "description": "Take elements from an array along an axis."
  },
  "1091": {
    "name": "tensordot",
    "module": "torch.storage.np.rec.sb",
    "fullName": "torch.storage.np.rec.sb.tensordot",
    "signature": "(a, b, axes=2)",
    "description": "Compute tensor dot product along specified axes."
  },
  "1092": {
    "name": "trace",
    "module": "torch.storage.np.rec.sb",
    "fullName": "torch.storage.np.rec.sb.trace",
    "signature": "(a, offset=0, axis1=0, axis2=1, dtype=None, out=None)",
    "description": "Return the sum along diagonals of the array."
  },
  "1093": {
    "name": "transpose",
    "module": "torch.storage.np.rec.sb",
    "fullName": "torch.storage.np.rec.sb.transpose",
    "signature": "(a, axes=None)",
    "description": "Returns an array with axes transposed."
  },
  "1094": {
    "name": "var",
    "module": "torch.storage.np.rec.sb",
    "fullName": "torch.storage.np.rec.sb.var",
    "signature": "(a, axis=None, dtype=None, out=None, ddof=0, keepdims=<no value>, *, where=<no value>)",
    "description": "Compute the variance along the specified axis."
  },
  "1095": {
    "name": "vdot",
    "module": "torch.storage.np.rec.sb",
    "fullName": "torch.storage.np.rec.sb.vdot",
    "signature": "N/A",
    "description": "vdot(a, b, /)"
  },
  "1096": {
    "name": "where",
    "module": "torch.storage.np.rec.sb",
    "fullName": "torch.storage.np.rec.sb.where",
    "signature": "N/A",
    "description": "where(condition, [x, y], /)"
  },
  "1097": {
    "name": "zeros_like",
    "module": "torch.storage.np.rec.sb",
    "fullName": "torch.storage.np.rec.sb.zeros_like",
    "signature": "(a, dtype=None, order='K', subok=True, shape=None)",
    "description": "Return an array of zeros with the same shape and type as a given array."
  },
  "1098": {
    "name": "atleast_1d",
    "module": "torch.storage.np.rec.sb.shape_base",
    "fullName": "torch.storage.np.rec.sb.shape_base.atleast_1d",
    "signature": "(*arys)",
    "description": "Convert inputs to arrays with at least one dimension."
  },
  "1099": {
    "name": "atleast_2d",
    "module": "torch.storage.np.rec.sb.shape_base",
    "fullName": "torch.storage.np.rec.sb.shape_base.atleast_2d",
    "signature": "(*arys)",
    "description": "View inputs as arrays with at least two dimensions."
  },
  "1100": {
    "name": "atleast_3d",
    "module": "torch.storage.np.rec.sb.shape_base",
    "fullName": "torch.storage.np.rec.sb.shape_base.atleast_3d",
    "signature": "(*arys)",
    "description": "View inputs as arrays with at least three dimensions."
  },
  "1101": {
    "name": "block",
    "module": "torch.storage.np.rec.sb.shape_base",
    "fullName": "torch.storage.np.rec.sb.shape_base.block",
    "signature": "(arrays)",
    "description": "Assemble an nd-array from nested lists of blocks."
  },
  "1102": {
    "name": "hstack",
    "module": "torch.storage.np.rec.sb.shape_base",
    "fullName": "torch.storage.np.rec.sb.shape_base.hstack",
    "signature": "(tup, *, dtype=None, casting='same_kind')",
    "description": "Stack arrays in sequence horizontally (column wise)."
  },
  "1103": {
    "name": "stack",
    "module": "torch.storage.np.rec.sb.shape_base",
    "fullName": "torch.storage.np.rec.sb.shape_base.stack",
    "signature": "(arrays, axis=0, out=None, *, dtype=None, casting='same_kind')",
    "description": "Join a sequence of arrays along a new axis."
  },
  "1104": {
    "name": "vstack",
    "module": "torch.storage.np.rec.sb.shape_base",
    "fullName": "torch.storage.np.rec.sb.shape_base.vstack",
    "signature": "(tup, *, dtype=None, casting='same_kind')",
    "description": "Stack arrays in sequence vertically (row wise)."
  },
  "1105": {
    "name": "array_function_dispatch",
    "module": "torch.storage.np.rec.sb.shape_base.overrides",
    "fullName": "torch.storage.np.rec.sb.shape_base.overrides.array_function_dispatch",
    "signature": "(dispatcher, module=None, verify=True, docs_from_dispatcher=False, use_like=False)",
    "description": "Decorator for adding dispatch with the __array_function__ protocol."
  },
  "1106": {
    "name": "array_function_from_dispatcher",
    "module": "torch.storage.np.rec.sb.shape_base.overrides",
    "fullName": "torch.storage.np.rec.sb.shape_base.overrides.array_function_from_dispatcher",
    "signature": "(implementation, module=None, verify=True, docs_from_dispatcher=True)",
    "description": "Like array_function_dispatcher, but with function arguments flipped."
  },
  "1107": {
    "name": "getargspec",
    "module": "torch.storage.np.rec.sb.shape_base.overrides",
    "fullName": "torch.storage.np.rec.sb.shape_base.overrides.getargspec",
    "signature": "(func)",
    "description": "Get the names and default values of a function's arguments."
  },
  "1108": {
    "name": "set_array_function_like_doc",
    "module": "torch.storage.np.rec.sb.shape_base.overrides",
    "fullName": "torch.storage.np.rec.sb.shape_base.overrides.set_array_function_like_doc",
    "signature": "(public_api)",
    "description": "No description available."
  },
  "1109": {
    "name": "set_module",
    "module": "torch.storage.np.rec.sb.shape_base.overrides",
    "fullName": "torch.storage.np.rec.sb.shape_base.overrides.set_module",
    "signature": "(module)",
    "description": "Decorator for overriding __module__ on a function or class."
  },
  "1110": {
    "name": "verify_matching_signatures",
    "module": "torch.storage.np.rec.sb.shape_base.overrides",
    "fullName": "torch.storage.np.rec.sb.shape_base.overrides.verify_matching_signatures",
    "signature": "(implementation, dispatcher)",
    "description": "Verify that a dispatcher function has the right signature."
  },
  "1111": {
    "name": "bitname",
    "module": "torch.storage.np.rec.sb.numerictypes",
    "fullName": "torch.storage.np.rec.sb.numerictypes.bitname",
    "signature": "(obj)",
    "description": "Return a bit-width name for a given type object"
  },
  "1112": {
    "name": "busday_count",
    "module": "torch.storage.np.rec.sb.numerictypes",
    "fullName": "torch.storage.np.rec.sb.numerictypes.busday_count",
    "signature": "N/A",
    "description": "busday_count(begindates, enddates, weekmask='1111100', holidays=[], busdaycal=None, out=None)"
  },
  "1113": {
    "name": "busday_offset",
    "module": "torch.storage.np.rec.sb.numerictypes",
    "fullName": "torch.storage.np.rec.sb.numerictypes.busday_offset",
    "signature": "N/A",
    "description": "busday_offset(dates, offsets, roll='raise', weekmask='1111100', holidays=None, busdaycal=None, out=None)"
  },
  "1114": {
    "name": "datetime_as_string",
    "module": "torch.storage.np.rec.sb.numerictypes",
    "fullName": "torch.storage.np.rec.sb.numerictypes.datetime_as_string",
    "signature": "N/A",
    "description": "datetime_as_string(arr, unit=None, timezone='naive', casting='same_kind')"
  },
  "1115": {
    "name": "english_capitalize",
    "module": "torch.storage.np.rec.sb.numerictypes",
    "fullName": "torch.storage.np.rec.sb.numerictypes.english_capitalize",
    "signature": "(s)",
    "description": "Apply English case rules to convert the first character of an ASCII"
  },
  "1116": {
    "name": "english_lower",
    "module": "torch.storage.np.rec.sb.numerictypes",
    "fullName": "torch.storage.np.rec.sb.numerictypes.english_lower",
    "signature": "(s)",
    "description": "Apply English case rules to convert ASCII strings to all lower case."
  },
  "1117": {
    "name": "english_upper",
    "module": "torch.storage.np.rec.sb.numerictypes",
    "fullName": "torch.storage.np.rec.sb.numerictypes.english_upper",
    "signature": "(s)",
    "description": "Apply English case rules to convert ASCII strings to all upper case."
  },
  "1118": {
    "name": "find_common_type",
    "module": "torch.storage.np.rec.sb.numerictypes",
    "fullName": "torch.storage.np.rec.sb.numerictypes.find_common_type",
    "signature": "(array_types, scalar_types)",
    "description": "Determine common type following standard coercion rules."
  },
  "1119": {
    "name": "is_busday",
    "module": "torch.storage.np.rec.sb.numerictypes",
    "fullName": "torch.storage.np.rec.sb.numerictypes.is_busday",
    "signature": "N/A",
    "description": "is_busday(dates, weekmask='1111100', holidays=None, busdaycal=None, out=None)"
  },
  "1120": {
    "name": "issctype",
    "module": "torch.storage.np.rec.sb.numerictypes",
    "fullName": "torch.storage.np.rec.sb.numerictypes.issctype",
    "signature": "(rep)",
    "description": "Determines whether the given object represents a scalar data-type."
  },
  "1121": {
    "name": "issubclass_",
    "module": "torch.storage.np.rec.sb.numerictypes",
    "fullName": "torch.storage.np.rec.sb.numerictypes.issubclass_",
    "signature": "(arg1, arg2)",
    "description": "Determine if a class is a subclass of a second class."
  },
  "1122": {
    "name": "issubdtype",
    "module": "torch.storage.np.rec.sb.numerictypes",
    "fullName": "torch.storage.np.rec.sb.numerictypes.issubdtype",
    "signature": "(arg1, arg2)",
    "description": "Returns True if first argument is a typecode lower/equal in type hierarchy."
  },
  "1123": {
    "name": "issubsctype",
    "module": "torch.storage.np.rec.sb.numerictypes",
    "fullName": "torch.storage.np.rec.sb.numerictypes.issubsctype",
    "signature": "(arg1, arg2)",
    "description": "Determine if the first argument is a subclass of the second argument."
  },
  "1124": {
    "name": "maximum_sctype",
    "module": "torch.storage.np.rec.sb.numerictypes",
    "fullName": "torch.storage.np.rec.sb.numerictypes.maximum_sctype",
    "signature": "(t)",
    "description": "Return the scalar type of highest precision of the same kind as the input."
  },
  "1125": {
    "name": "obj2sctype",
    "module": "torch.storage.np.rec.sb.numerictypes",
    "fullName": "torch.storage.np.rec.sb.numerictypes.obj2sctype",
    "signature": "(rep, default=None)",
    "description": "Return the scalar dtype or NumPy equivalent of Python type of an object."
  },
  "1126": {
    "name": "sctype2char",
    "module": "torch.storage.np.rec.sb.numerictypes",
    "fullName": "torch.storage.np.rec.sb.numerictypes.sctype2char",
    "signature": "(sctype)",
    "description": "Return the string representation of a scalar dtype."
  },
  "1127": {
    "name": "set_module",
    "module": "torch.storage.np.rec.sb.numerictypes",
    "fullName": "torch.storage.np.rec.sb.numerictypes.set_module",
    "signature": "(module)",
    "description": "Decorator for overriding __module__ on a function or class."
  },
  "1128": {
    "name": "bincount",
    "module": "torch.storage.np.rec.sb.multiarray",
    "fullName": "torch.storage.np.rec.sb.multiarray.bincount",
    "signature": "N/A",
    "description": "bincount(x, /, weights=None, minlength=0)"
  },
  "1129": {
    "name": "busday_count",
    "module": "torch.storage.np.rec.sb.multiarray",
    "fullName": "torch.storage.np.rec.sb.multiarray.busday_count",
    "signature": "N/A",
    "description": "busday_count(begindates, enddates, weekmask='1111100', holidays=[], busdaycal=None, out=None)"
  },
  "1130": {
    "name": "busday_offset",
    "module": "torch.storage.np.rec.sb.multiarray",
    "fullName": "torch.storage.np.rec.sb.multiarray.busday_offset",
    "signature": "N/A",
    "description": "busday_offset(dates, offsets, roll='raise', weekmask='1111100', holidays=None, busdaycal=None, out=None)"
  },
  "1131": {
    "name": "can_cast",
    "module": "torch.storage.np.rec.sb.multiarray",
    "fullName": "torch.storage.np.rec.sb.multiarray.can_cast",
    "signature": "N/A",
    "description": "can_cast(from_, to, casting='safe')"
  },
  "1132": {
    "name": "concatenate",
    "module": "torch.storage.np.rec.sb.multiarray",
    "fullName": "torch.storage.np.rec.sb.multiarray.concatenate",
    "signature": "N/A",
    "description": "concatenate((a1, a2, ...), axis=0, out=None, dtype=None, casting=\"same_kind\")"
  },
  "1133": {
    "name": "copyto",
    "module": "torch.storage.np.rec.sb.multiarray",
    "fullName": "torch.storage.np.rec.sb.multiarray.copyto",
    "signature": "N/A",
    "description": "copyto(dst, src, casting='same_kind', where=True)"
  },
  "1134": {
    "name": "datetime_as_string",
    "module": "torch.storage.np.rec.sb.multiarray",
    "fullName": "torch.storage.np.rec.sb.multiarray.datetime_as_string",
    "signature": "N/A",
    "description": "datetime_as_string(arr, unit=None, timezone='naive', casting='same_kind')"
  },
  "1135": {
    "name": "dot",
    "module": "torch.storage.np.rec.sb.multiarray",
    "fullName": "torch.storage.np.rec.sb.multiarray.dot",
    "signature": "N/A",
    "description": "dot(a, b, out=None)"
  },
  "1136": {
    "name": "empty_like",
    "module": "torch.storage.np.rec.sb.multiarray",
    "fullName": "torch.storage.np.rec.sb.multiarray.empty_like",
    "signature": "N/A",
    "description": "empty_like(prototype, dtype=None, order='K', subok=True, shape=None)"
  },
  "1137": {
    "name": "inner",
    "module": "torch.storage.np.rec.sb.multiarray",
    "fullName": "torch.storage.np.rec.sb.multiarray.inner",
    "signature": "N/A",
    "description": "inner(a, b, /)"
  },
  "1138": {
    "name": "is_busday",
    "module": "torch.storage.np.rec.sb.multiarray",
    "fullName": "torch.storage.np.rec.sb.multiarray.is_busday",
    "signature": "N/A",
    "description": "is_busday(dates, weekmask='1111100', holidays=None, busdaycal=None, out=None)"
  },
  "1139": {
    "name": "lexsort",
    "module": "torch.storage.np.rec.sb.multiarray",
    "fullName": "torch.storage.np.rec.sb.multiarray.lexsort",
    "signature": "N/A",
    "description": "lexsort(keys, axis=-1)"
  },
  "1140": {
    "name": "may_share_memory",
    "module": "torch.storage.np.rec.sb.multiarray",
    "fullName": "torch.storage.np.rec.sb.multiarray.may_share_memory",
    "signature": "N/A",
    "description": "may_share_memory(a, b, /, max_work=None)"
  },
  "1141": {
    "name": "min_scalar_type",
    "module": "torch.storage.np.rec.sb.multiarray",
    "fullName": "torch.storage.np.rec.sb.multiarray.min_scalar_type",
    "signature": "N/A",
    "description": "min_scalar_type(a, /)"
  },
  "1142": {
    "name": "packbits",
    "module": "torch.storage.np.rec.sb.multiarray",
    "fullName": "torch.storage.np.rec.sb.multiarray.packbits",
    "signature": "N/A",
    "description": "packbits(a, /, axis=None, bitorder='big')"
  },
  "1143": {
    "name": "putmask",
    "module": "torch.storage.np.rec.sb.multiarray",
    "fullName": "torch.storage.np.rec.sb.multiarray.putmask",
    "signature": "N/A",
    "description": "putmask(a, mask, values)"
  },
  "1144": {
    "name": "ravel_multi_index",
    "module": "torch.storage.np.rec.sb.multiarray",
    "fullName": "torch.storage.np.rec.sb.multiarray.ravel_multi_index",
    "signature": "N/A",
    "description": "ravel_multi_index(multi_index, dims, mode='raise', order='C')"
  },
  "1145": {
    "name": "result_type",
    "module": "torch.storage.np.rec.sb.multiarray",
    "fullName": "torch.storage.np.rec.sb.multiarray.result_type",
    "signature": "N/A",
    "description": "result_type(*arrays_and_dtypes)"
  },
  "1146": {
    "name": "shares_memory",
    "module": "torch.storage.np.rec.sb.multiarray",
    "fullName": "torch.storage.np.rec.sb.multiarray.shares_memory",
    "signature": "N/A",
    "description": "shares_memory(a, b, /, max_work=None)"
  },
  "1147": {
    "name": "unpackbits",
    "module": "torch.storage.np.rec.sb.multiarray",
    "fullName": "torch.storage.np.rec.sb.multiarray.unpackbits",
    "signature": "N/A",
    "description": "unpackbits(a, /, axis=None, count=None, bitorder='big')"
  },
  "1148": {
    "name": "unravel_index",
    "module": "torch.storage.np.rec.sb.multiarray",
    "fullName": "torch.storage.np.rec.sb.multiarray.unravel_index",
    "signature": "N/A",
    "description": "unravel_index(indices, shape, order='C')"
  },
  "1149": {
    "name": "vdot",
    "module": "torch.storage.np.rec.sb.multiarray",
    "fullName": "torch.storage.np.rec.sb.multiarray.vdot",
    "signature": "N/A",
    "description": "vdot(a, b, /)"
  },
  "1150": {
    "name": "where",
    "module": "torch.storage.np.rec.sb.multiarray",
    "fullName": "torch.storage.np.rec.sb.multiarray.where",
    "signature": "N/A",
    "description": "where(condition, [x, y], /)"
  },
  "1151": {
    "name": "all",
    "module": "torch.storage.np.rec.sb.fromnumeric",
    "fullName": "torch.storage.np.rec.sb.fromnumeric.all",
    "signature": "(a, axis=None, out=None, keepdims=<no value>, *, where=<no value>)",
    "description": "Test whether all array elements along a given axis evaluate to True."
  },
  "1152": {
    "name": "alltrue",
    "module": "torch.storage.np.rec.sb.fromnumeric",
    "fullName": "torch.storage.np.rec.sb.fromnumeric.alltrue",
    "signature": "(*args, **kwargs)",
    "description": "Check if all elements of input array are true."
  },
  "1153": {
    "name": "amax",
    "module": "torch.storage.np.rec.sb.fromnumeric",
    "fullName": "torch.storage.np.rec.sb.fromnumeric.amax",
    "signature": "(a, axis=None, out=None, keepdims=<no value>, initial=<no value>, where=<no value>)",
    "description": "Return the maximum of an array or maximum along an axis."
  },
  "1154": {
    "name": "amin",
    "module": "torch.storage.np.rec.sb.fromnumeric",
    "fullName": "torch.storage.np.rec.sb.fromnumeric.amin",
    "signature": "(a, axis=None, out=None, keepdims=<no value>, initial=<no value>, where=<no value>)",
    "description": "Return the minimum of an array or minimum along an axis."
  },
  "1155": {
    "name": "any",
    "module": "torch.storage.np.rec.sb.fromnumeric",
    "fullName": "torch.storage.np.rec.sb.fromnumeric.any",
    "signature": "(a, axis=None, out=None, keepdims=<no value>, *, where=<no value>)",
    "description": "Test whether any array element along a given axis evaluates to True."
  },
  "1156": {
    "name": "argmax",
    "module": "torch.storage.np.rec.sb.fromnumeric",
    "fullName": "torch.storage.np.rec.sb.fromnumeric.argmax",
    "signature": "(a, axis=None, out=None, *, keepdims=<no value>)",
    "description": "Returns the indices of the maximum values along an axis."
  },
  "1157": {
    "name": "argmin",
    "module": "torch.storage.np.rec.sb.fromnumeric",
    "fullName": "torch.storage.np.rec.sb.fromnumeric.argmin",
    "signature": "(a, axis=None, out=None, *, keepdims=<no value>)",
    "description": "Returns the indices of the minimum values along an axis."
  },
  "1158": {
    "name": "argsort",
    "module": "torch.storage.np.rec.sb.fromnumeric",
    "fullName": "torch.storage.np.rec.sb.fromnumeric.argsort",
    "signature": "(a, axis=-1, kind=None, order=None)",
    "description": "Returns the indices that would sort an array."
  },
  "1159": {
    "name": "around",
    "module": "torch.storage.np.rec.sb.fromnumeric",
    "fullName": "torch.storage.np.rec.sb.fromnumeric.around",
    "signature": "(a, decimals=0, out=None)",
    "description": "Evenly round to the given number of decimals."
  },
  "1160": {
    "name": "choose",
    "module": "torch.storage.np.rec.sb.fromnumeric",
    "fullName": "torch.storage.np.rec.sb.fromnumeric.choose",
    "signature": "(a, choices, out=None, mode='raise')",
    "description": "Construct an array from an index array and a list of arrays to choose from."
  },
  "1161": {
    "name": "clip",
    "module": "torch.storage.np.rec.sb.fromnumeric",
    "fullName": "torch.storage.np.rec.sb.fromnumeric.clip",
    "signature": "(a, a_min, a_max, out=None, **kwargs)",
    "description": "Clip (limit) the values in an array."
  },
  "1162": {
    "name": "compress",
    "module": "torch.storage.np.rec.sb.fromnumeric",
    "fullName": "torch.storage.np.rec.sb.fromnumeric.compress",
    "signature": "(condition, a, axis=None, out=None)",
    "description": "Return selected slices of an array along given axis."
  },
  "1163": {
    "name": "concatenate",
    "module": "torch.storage.np.rec.sb.fromnumeric",
    "fullName": "torch.storage.np.rec.sb.fromnumeric.concatenate",
    "signature": "N/A",
    "description": "concatenate((a1, a2, ...), axis=0, out=None, dtype=None, casting=\"same_kind\")"
  },
  "1164": {
    "name": "cumprod",
    "module": "torch.storage.np.rec.sb.fromnumeric",
    "fullName": "torch.storage.np.rec.sb.fromnumeric.cumprod",
    "signature": "(a, axis=None, dtype=None, out=None)",
    "description": "Return the cumulative product of elements along a given axis."
  },
  "1165": {
    "name": "cumproduct",
    "module": "torch.storage.np.rec.sb.fromnumeric",
    "fullName": "torch.storage.np.rec.sb.fromnumeric.cumproduct",
    "signature": "(*args, **kwargs)",
    "description": "Return the cumulative product over the given axis."
  },
  "1166": {
    "name": "cumsum",
    "module": "torch.storage.np.rec.sb.fromnumeric",
    "fullName": "torch.storage.np.rec.sb.fromnumeric.cumsum",
    "signature": "(a, axis=None, dtype=None, out=None)",
    "description": "Return the cumulative sum of the elements along a given axis."
  },
  "1167": {
    "name": "mean",
    "module": "torch.storage.np.rec.sb.fromnumeric",
    "fullName": "torch.storage.np.rec.sb.fromnumeric.mean",
    "signature": "(a, axis=None, dtype=None, out=None, keepdims=<no value>, *, where=<no value>)",
    "description": "Compute the arithmetic mean along the specified axis."
  },
  "1168": {
    "name": "ndim",
    "module": "torch.storage.np.rec.sb.fromnumeric",
    "fullName": "torch.storage.np.rec.sb.fromnumeric.ndim",
    "signature": "(a)",
    "description": "Return the number of dimensions of an array."
  },
  "1169": {
    "name": "prod",
    "module": "torch.storage.np.rec.sb.fromnumeric",
    "fullName": "torch.storage.np.rec.sb.fromnumeric.prod",
    "signature": "(a, axis=None, dtype=None, out=None, keepdims=<no value>, initial=<no value>, where=<no value>)",
    "description": "Return the product of array elements over a given axis."
  },
  "1170": {
    "name": "product",
    "module": "torch.storage.np.rec.sb.fromnumeric",
    "fullName": "torch.storage.np.rec.sb.fromnumeric.product",
    "signature": "(*args, **kwargs)",
    "description": "Return the product of array elements over a given axis."
  },
  "1171": {
    "name": "ptp",
    "module": "torch.storage.np.rec.sb.fromnumeric",
    "fullName": "torch.storage.np.rec.sb.fromnumeric.ptp",
    "signature": "(a, axis=None, out=None, keepdims=<no value>)",
    "description": "Range of values (maximum - minimum) along an axis."
  },
  "1172": {
    "name": "put",
    "module": "torch.storage.np.rec.sb.fromnumeric",
    "fullName": "torch.storage.np.rec.sb.fromnumeric.put",
    "signature": "(a, ind, v, mode='raise')",
    "description": "Replaces specified elements of an array with given values."
  },
  "1173": {
    "name": "ravel",
    "module": "torch.storage.np.rec.sb.fromnumeric",
    "fullName": "torch.storage.np.rec.sb.fromnumeric.ravel",
    "signature": "(a, order='C')",
    "description": "Return a contiguous flattened array."
  },
  "1174": {
    "name": "repeat",
    "module": "torch.storage.np.rec.sb.fromnumeric",
    "fullName": "torch.storage.np.rec.sb.fromnumeric.repeat",
    "signature": "(a, repeats, axis=None)",
    "description": "Repeat elements of an array."
  },
  "1175": {
    "name": "reshape",
    "module": "torch.storage.np.rec.sb.fromnumeric",
    "fullName": "torch.storage.np.rec.sb.fromnumeric.reshape",
    "signature": "(a, newshape, order='C')",
    "description": "Gives a new shape to an array without changing its data."
  },
  "1176": {
    "name": "resize",
    "module": "torch.storage.np.rec.sb.fromnumeric",
    "fullName": "torch.storage.np.rec.sb.fromnumeric.resize",
    "signature": "(a, new_shape)",
    "description": "Return a new array with the specified shape."
  },
  "1177": {
    "name": "round_",
    "module": "torch.storage.np.rec.sb.fromnumeric",
    "fullName": "torch.storage.np.rec.sb.fromnumeric.round_",
    "signature": "(a, decimals=0, out=None)",
    "description": "Round an array to the given number of decimals."
  },
  "1178": {
    "name": "searchsorted",
    "module": "torch.storage.np.rec.sb.fromnumeric",
    "fullName": "torch.storage.np.rec.sb.fromnumeric.searchsorted",
    "signature": "(a, v, side='left', sorter=None)",
    "description": "Find indices where elements should be inserted to maintain order."
  },
  "1179": {
    "name": "shape",
    "module": "torch.storage.np.rec.sb.fromnumeric",
    "fullName": "torch.storage.np.rec.sb.fromnumeric.shape",
    "signature": "(a)",
    "description": "Return the shape of an array."
  },
  "1180": {
    "name": "size",
    "module": "torch.storage.np.rec.sb.fromnumeric",
    "fullName": "torch.storage.np.rec.sb.fromnumeric.size",
    "signature": "(a, axis=None)",
    "description": "Return the number of elements along a given axis."
  },
  "1181": {
    "name": "sometrue",
    "module": "torch.storage.np.rec.sb.fromnumeric",
    "fullName": "torch.storage.np.rec.sb.fromnumeric.sometrue",
    "signature": "(*args, **kwargs)",
    "description": "Check whether some values are true."
  },
  "1182": {
    "name": "sort",
    "module": "torch.storage.np.rec.sb.fromnumeric",
    "fullName": "torch.storage.np.rec.sb.fromnumeric.sort",
    "signature": "(a, axis=-1, kind=None, order=None)",
    "description": "Return a sorted copy of an array."
  },
  "1183": {
    "name": "squeeze",
    "module": "torch.storage.np.rec.sb.fromnumeric",
    "fullName": "torch.storage.np.rec.sb.fromnumeric.squeeze",
    "signature": "(a, axis=None)",
    "description": "Remove axes of length one from `a`."
  },
  "1184": {
    "name": "std",
    "module": "torch.storage.np.rec.sb.fromnumeric",
    "fullName": "torch.storage.np.rec.sb.fromnumeric.std",
    "signature": "(a, axis=None, dtype=None, out=None, ddof=0, keepdims=<no value>, *, where=<no value>)",
    "description": "Compute the standard deviation along the specified axis."
  },
  "1185": {
    "name": "sum",
    "module": "torch.storage.np.rec.sb.fromnumeric",
    "fullName": "torch.storage.np.rec.sb.fromnumeric.sum",
    "signature": "(a, axis=None, dtype=None, out=None, keepdims=<no value>, initial=<no value>, where=<no value>)",
    "description": "Sum of array elements over a given axis."
  },
  "1186": {
    "name": "swapaxes",
    "module": "torch.storage.np.rec.sb.fromnumeric",
    "fullName": "torch.storage.np.rec.sb.fromnumeric.swapaxes",
    "signature": "(a, axis1, axis2)",
    "description": "Interchange two axes of an array."
  },
  "1187": {
    "name": "take",
    "module": "torch.storage.np.rec.sb.fromnumeric",
    "fullName": "torch.storage.np.rec.sb.fromnumeric.take",
    "signature": "(a, indices, axis=None, out=None, mode='raise')",
    "description": "Take elements from an array along an axis."
  },
  "1188": {
    "name": "trace",
    "module": "torch.storage.np.rec.sb.fromnumeric",
    "fullName": "torch.storage.np.rec.sb.fromnumeric.trace",
    "signature": "(a, offset=0, axis1=0, axis2=1, dtype=None, out=None)",
    "description": "Return the sum along diagonals of the array."
  },
  "1189": {
    "name": "transpose",
    "module": "torch.storage.np.rec.sb.fromnumeric",
    "fullName": "torch.storage.np.rec.sb.fromnumeric.transpose",
    "signature": "(a, axes=None)",
    "description": "Returns an array with axes transposed."
  },
  "1190": {
    "name": "var",
    "module": "torch.storage.np.rec.sb.fromnumeric",
    "fullName": "torch.storage.np.rec.sb.fromnumeric.var",
    "signature": "(a, axis=None, dtype=None, out=None, ddof=0, keepdims=<no value>, *, where=<no value>)",
    "description": "Compute the variance along the specified axis."
  },
  "1191": {
    "name": "any",
    "module": "torch.storage.np.rec.sb.arrayprint",
    "fullName": "torch.storage.np.rec.sb.arrayprint.any",
    "signature": "(a, axis=None, out=None, keepdims=<no value>, *, where=<no value>)",
    "description": "Test whether any array element along a given axis evaluates to True."
  },
  "1192": {
    "name": "array_function_dispatch",
    "module": "torch.storage.np.rec.sb.arrayprint",
    "fullName": "torch.storage.np.rec.sb.arrayprint.array_function_dispatch",
    "signature": "(dispatcher, module=None, verify=True, docs_from_dispatcher=False, use_like=False)",
    "description": "Decorator for adding dispatch with the __array_function__ protocol."
  },
  "1193": {
    "name": "array_repr",
    "module": "torch.storage.np.rec.sb.arrayprint",
    "fullName": "torch.storage.np.rec.sb.arrayprint.array_repr",
    "signature": "(arr, max_line_width=None, precision=None, suppress_small=None)",
    "description": "Return the string representation of an array."
  },
  "1194": {
    "name": "array_str",
    "module": "torch.storage.np.rec.sb.arrayprint",
    "fullName": "torch.storage.np.rec.sb.arrayprint.array_str",
    "signature": "(a, max_line_width=None, precision=None, suppress_small=None)",
    "description": "Return a string representation of the data in an array."
  },
  "1195": {
    "name": "concatenate",
    "module": "torch.storage.np.rec.sb.arrayprint",
    "fullName": "torch.storage.np.rec.sb.arrayprint.concatenate",
    "signature": "N/A",
    "description": "concatenate((a1, a2, ...), axis=0, out=None, dtype=None, casting=\"same_kind\")"
  },
  "1196": {
    "name": "datetime_as_string",
    "module": "torch.storage.np.rec.sb.arrayprint",
    "fullName": "torch.storage.np.rec.sb.arrayprint.datetime_as_string",
    "signature": "N/A",
    "description": "datetime_as_string(arr, unit=None, timezone='naive', casting='same_kind')"
  },
  "1197": {
    "name": "dtype_is_implied",
    "module": "torch.storage.np.rec.sb.arrayprint",
    "fullName": "torch.storage.np.rec.sb.arrayprint.dtype_is_implied",
    "signature": "(dtype)",
    "description": "Determine if the given dtype is implied by the representation of its values."
  },
  "1198": {
    "name": "dtype_short_repr",
    "module": "torch.storage.np.rec.sb.arrayprint",
    "fullName": "torch.storage.np.rec.sb.arrayprint.dtype_short_repr",
    "signature": "(dtype)",
    "description": "Convert a dtype to a short form which evaluates to the same dtype."
  },
  "1199": {
    "name": "format_float_positional",
    "module": "torch.storage.np.rec.sb.arrayprint",
    "fullName": "torch.storage.np.rec.sb.arrayprint.format_float_positional",
    "signature": "(x, precision=None, unique=True, fractional=True, trim='k', sign=False, pad_left=None, pad_right=None, min_digits=None)",
    "description": "Format a floating-point scalar as a decimal string in positional notation."
  },
  "1200": {
    "name": "format_float_scientific",
    "module": "torch.storage.np.rec.sb.arrayprint",
    "fullName": "torch.storage.np.rec.sb.arrayprint.format_float_scientific",
    "signature": "(x, precision=None, unique=True, trim='k', sign=False, pad_left=None, exp_digits=None, min_digits=None)",
    "description": "Format a floating-point scalar as a decimal string in scientific notation."
  },
  "1201": {
    "name": "get_printoptions",
    "module": "torch.storage.np.rec.sb.arrayprint",
    "fullName": "torch.storage.np.rec.sb.arrayprint.get_printoptions",
    "signature": "()",
    "description": "Return the current print options."
  },
  "1202": {
    "name": "printoptions",
    "module": "torch.storage.np.rec.sb.arrayprint",
    "fullName": "torch.storage.np.rec.sb.arrayprint.printoptions",
    "signature": "(*args, **kwargs)",
    "description": "Context manager for setting print options."
  },
  "1203": {
    "name": "repr_format",
    "module": "torch.storage.np.rec.sb.arrayprint",
    "fullName": "torch.storage.np.rec.sb.arrayprint.repr_format",
    "signature": "(x)",
    "description": "No description available."
  },
  "1204": {
    "name": "set_module",
    "module": "torch.storage.np.rec.sb.arrayprint",
    "fullName": "torch.storage.np.rec.sb.arrayprint.set_module",
    "signature": "(module)",
    "description": "Decorator for overriding __module__ on a function or class."
  },
  "1205": {
    "name": "set_printoptions",
    "module": "torch.storage.np.rec.sb.arrayprint",
    "fullName": "torch.storage.np.rec.sb.arrayprint.set_printoptions",
    "signature": "(precision=None, threshold=None, edgeitems=None, linewidth=None, suppress=None, nanstr=None, infstr=None, formatter=None, sign=None, floatmode=None, *, legacy=None)",
    "description": "Set printing options."
  },
  "1206": {
    "name": "set_string_function",
    "module": "torch.storage.np.rec.sb.arrayprint",
    "fullName": "torch.storage.np.rec.sb.arrayprint.set_string_function",
    "signature": "(f, repr=True)",
    "description": "Set a Python function to be used when pretty printing arrays."
  },
  "1207": {
    "name": "str_format",
    "module": "torch.storage.np.rec.sb.arrayprint",
    "fullName": "torch.storage.np.rec.sb.arrayprint.str_format",
    "signature": "(x)",
    "description": "No description available."
  },
  "1208": {
    "name": "set_default_printstyle",
    "module": "torch.storage.np.polynomial",
    "fullName": "torch.storage.np.polynomial.set_default_printstyle",
    "signature": "(style)",
    "description": "Set the default format for the string representation of polynomials."
  },
  "1209": {
    "name": "as_series",
    "module": "torch.storage.np.polynomial.polyutils",
    "fullName": "torch.storage.np.polynomial.polyutils.as_series",
    "signature": "(alist, trim=True)",
    "description": "Return argument as a list of 1-d arrays."
  },
  "1210": {
    "name": "format_float",
    "module": "torch.storage.np.polynomial.polyutils",
    "fullName": "torch.storage.np.polynomial.polyutils.format_float",
    "signature": "(x, parens=False)",
    "description": "No description available."
  },
  "1211": {
    "name": "getdomain",
    "module": "torch.storage.np.polynomial.polyutils",
    "fullName": "torch.storage.np.polynomial.polyutils.getdomain",
    "signature": "(x)",
    "description": "Return a domain suitable for given abscissae."
  },
  "1212": {
    "name": "mapdomain",
    "module": "torch.storage.np.polynomial.polyutils",
    "fullName": "torch.storage.np.polynomial.polyutils.mapdomain",
    "signature": "(x, old, new)",
    "description": "Apply linear map to input points."
  },
  "1213": {
    "name": "mapparms",
    "module": "torch.storage.np.polynomial.polyutils",
    "fullName": "torch.storage.np.polynomial.polyutils.mapparms",
    "signature": "(old, new)",
    "description": "Linear map parameters between domains."
  },
  "1214": {
    "name": "trimcoef",
    "module": "torch.storage.np.polynomial.polyutils",
    "fullName": "torch.storage.np.polynomial.polyutils.trimcoef",
    "signature": "(c, tol=0)",
    "description": "Remove \"small\" \"trailing\" coefficients from a polynomial."
  },
  "1215": {
    "name": "trimseq",
    "module": "torch.storage.np.polynomial.polyutils",
    "fullName": "torch.storage.np.polynomial.polyutils.trimseq",
    "signature": "(seq)",
    "description": "Remove small Poly series coefficients."
  },
  "1216": {
    "name": "polyadd",
    "module": "torch.storage.np.polynomial.polynomial",
    "fullName": "torch.storage.np.polynomial.polynomial.polyadd",
    "signature": "(c1, c2)",
    "description": "Add one polynomial to another."
  },
  "1217": {
    "name": "polycompanion",
    "module": "torch.storage.np.polynomial.polynomial",
    "fullName": "torch.storage.np.polynomial.polynomial.polycompanion",
    "signature": "(c)",
    "description": "Return the companion matrix of c."
  },
  "1218": {
    "name": "polyder",
    "module": "torch.storage.np.polynomial.polynomial",
    "fullName": "torch.storage.np.polynomial.polynomial.polyder",
    "signature": "(c, m=1, scl=1, axis=0)",
    "description": "Differentiate a polynomial."
  },
  "1219": {
    "name": "polydiv",
    "module": "torch.storage.np.polynomial.polynomial",
    "fullName": "torch.storage.np.polynomial.polynomial.polydiv",
    "signature": "(c1, c2)",
    "description": "Divide one polynomial by another."
  },
  "1220": {
    "name": "polyfit",
    "module": "torch.storage.np.polynomial.polynomial",
    "fullName": "torch.storage.np.polynomial.polynomial.polyfit",
    "signature": "(x, y, deg, rcond=None, full=False, w=None)",
    "description": "Least-squares fit of a polynomial to data."
  },
  "1221": {
    "name": "polyfromroots",
    "module": "torch.storage.np.polynomial.polynomial",
    "fullName": "torch.storage.np.polynomial.polynomial.polyfromroots",
    "signature": "(roots)",
    "description": "Generate a monic polynomial with given roots."
  },
  "1222": {
    "name": "polygrid2d",
    "module": "torch.storage.np.polynomial.polynomial",
    "fullName": "torch.storage.np.polynomial.polynomial.polygrid2d",
    "signature": "(x, y, c)",
    "description": "Evaluate a 2-D polynomial on the Cartesian product of x and y."
  },
  "1223": {
    "name": "polygrid3d",
    "module": "torch.storage.np.polynomial.polynomial",
    "fullName": "torch.storage.np.polynomial.polynomial.polygrid3d",
    "signature": "(x, y, z, c)",
    "description": "Evaluate a 3-D polynomial on the Cartesian product of x, y and z."
  },
  "1224": {
    "name": "polyint",
    "module": "torch.storage.np.polynomial.polynomial",
    "fullName": "torch.storage.np.polynomial.polynomial.polyint",
    "signature": "(c, m=1, k=[], lbnd=0, scl=1, axis=0)",
    "description": "Integrate a polynomial."
  },
  "1225": {
    "name": "polyline",
    "module": "torch.storage.np.polynomial.polynomial",
    "fullName": "torch.storage.np.polynomial.polynomial.polyline",
    "signature": "(off, scl)",
    "description": "Returns an array representing a linear polynomial."
  },
  "1226": {
    "name": "polymul",
    "module": "torch.storage.np.polynomial.polynomial",
    "fullName": "torch.storage.np.polynomial.polynomial.polymul",
    "signature": "(c1, c2)",
    "description": "Multiply one polynomial by another."
  },
  "1227": {
    "name": "polymulx",
    "module": "torch.storage.np.polynomial.polynomial",
    "fullName": "torch.storage.np.polynomial.polynomial.polymulx",
    "signature": "(c)",
    "description": "Multiply a polynomial by x."
  },
  "1228": {
    "name": "polypow",
    "module": "torch.storage.np.polynomial.polynomial",
    "fullName": "torch.storage.np.polynomial.polynomial.polypow",
    "signature": "(c, pow, maxpower=None)",
    "description": "Raise a polynomial to a power."
  },
  "1229": {
    "name": "polyroots",
    "module": "torch.storage.np.polynomial.polynomial",
    "fullName": "torch.storage.np.polynomial.polynomial.polyroots",
    "signature": "(c)",
    "description": "Compute the roots of a polynomial."
  },
  "1230": {
    "name": "polysub",
    "module": "torch.storage.np.polynomial.polynomial",
    "fullName": "torch.storage.np.polynomial.polynomial.polysub",
    "signature": "(c1, c2)",
    "description": "Subtract one polynomial from another."
  },
  "1231": {
    "name": "polytrim",
    "module": "torch.storage.np.polynomial.polynomial",
    "fullName": "torch.storage.np.polynomial.polynomial.polytrim",
    "signature": "(c, tol=0)",
    "description": "Remove \"small\" \"trailing\" coefficients from a polynomial."
  },
  "1232": {
    "name": "polyval",
    "module": "torch.storage.np.polynomial.polynomial",
    "fullName": "torch.storage.np.polynomial.polynomial.polyval",
    "signature": "(x, c, tensor=True)",
    "description": "Evaluate a polynomial at points x."
  },
  "1233": {
    "name": "polyval2d",
    "module": "torch.storage.np.polynomial.polynomial",
    "fullName": "torch.storage.np.polynomial.polynomial.polyval2d",
    "signature": "(x, y, c)",
    "description": "Evaluate a 2-D polynomial at points (x, y)."
  },
  "1234": {
    "name": "polyval3d",
    "module": "torch.storage.np.polynomial.polynomial",
    "fullName": "torch.storage.np.polynomial.polynomial.polyval3d",
    "signature": "(x, y, z, c)",
    "description": "Evaluate a 3-D polynomial at points (x, y, z)."
  },
  "1235": {
    "name": "polyvalfromroots",
    "module": "torch.storage.np.polynomial.polynomial",
    "fullName": "torch.storage.np.polynomial.polynomial.polyvalfromroots",
    "signature": "(x, r, tensor=True)",
    "description": "Evaluate a polynomial specified by its roots at points x."
  },
  "1236": {
    "name": "polyvander",
    "module": "torch.storage.np.polynomial.polynomial",
    "fullName": "torch.storage.np.polynomial.polynomial.polyvander",
    "signature": "(x, deg)",
    "description": "Vandermonde matrix of given degree."
  },
  "1237": {
    "name": "polyvander2d",
    "module": "torch.storage.np.polynomial.polynomial",
    "fullName": "torch.storage.np.polynomial.polynomial.polyvander2d",
    "signature": "(x, y, deg)",
    "description": "Pseudo-Vandermonde matrix of given degrees."
  },
  "1238": {
    "name": "polyvander3d",
    "module": "torch.storage.np.polynomial.polynomial",
    "fullName": "torch.storage.np.polynomial.polynomial.polyvander3d",
    "signature": "(x, y, z, deg)",
    "description": "Pseudo-Vandermonde matrix of given degrees."
  },
  "1239": {
    "name": "cholesky",
    "module": "torch.storage.np.polynomial.polynomial.la",
    "fullName": "torch.storage.np.polynomial.polynomial.la.cholesky",
    "signature": "(a)",
    "description": "Cholesky decomposition."
  },
  "1240": {
    "name": "cond",
    "module": "torch.storage.np.polynomial.polynomial.la",
    "fullName": "torch.storage.np.polynomial.polynomial.la.cond",
    "signature": "(x, p=None)",
    "description": "Compute the condition number of a matrix."
  },
  "1241": {
    "name": "det",
    "module": "torch.storage.np.polynomial.polynomial.la",
    "fullName": "torch.storage.np.polynomial.polynomial.la.det",
    "signature": "(a)",
    "description": "Compute the determinant of an array."
  },
  "1242": {
    "name": "eig",
    "module": "torch.storage.np.polynomial.polynomial.la",
    "fullName": "torch.storage.np.polynomial.polynomial.la.eig",
    "signature": "(a)",
    "description": "Compute the eigenvalues and right eigenvectors of a square array."
  },
  "1243": {
    "name": "eigh",
    "module": "torch.storage.np.polynomial.polynomial.la",
    "fullName": "torch.storage.np.polynomial.polynomial.la.eigh",
    "signature": "(a, UPLO='L')",
    "description": "Return the eigenvalues and eigenvectors of a complex Hermitian"
  },
  "1244": {
    "name": "eigvals",
    "module": "torch.storage.np.polynomial.polynomial.la",
    "fullName": "torch.storage.np.polynomial.polynomial.la.eigvals",
    "signature": "(a)",
    "description": "Compute the eigenvalues of a general matrix."
  },
  "1245": {
    "name": "eigvalsh",
    "module": "torch.storage.np.polynomial.polynomial.la",
    "fullName": "torch.storage.np.polynomial.polynomial.la.eigvalsh",
    "signature": "(a, UPLO='L')",
    "description": "Compute the eigenvalues of a complex Hermitian or real symmetric matrix."
  },
  "1246": {
    "name": "inv",
    "module": "torch.storage.np.polynomial.polynomial.la",
    "fullName": "torch.storage.np.polynomial.polynomial.la.inv",
    "signature": "(a)",
    "description": "Compute the (multiplicative) inverse of a matrix."
  },
  "1247": {
    "name": "lstsq",
    "module": "torch.storage.np.polynomial.polynomial.la",
    "fullName": "torch.storage.np.polynomial.polynomial.la.lstsq",
    "signature": "(a, b, rcond='warn')",
    "description": "Return the least-squares solution to a linear matrix equation."
  },
  "1248": {
    "name": "matrix_power",
    "module": "torch.storage.np.polynomial.polynomial.la",
    "fullName": "torch.storage.np.polynomial.polynomial.la.matrix_power",
    "signature": "(a, n)",
    "description": "Raise a square matrix to the (integer) power `n`."
  },
  "1249": {
    "name": "matrix_rank",
    "module": "torch.storage.np.polynomial.polynomial.la",
    "fullName": "torch.storage.np.polynomial.polynomial.la.matrix_rank",
    "signature": "(A, tol=None, hermitian=False)",
    "description": "Return matrix rank of array using SVD method"
  },
  "1250": {
    "name": "multi_dot",
    "module": "torch.storage.np.polynomial.polynomial.la",
    "fullName": "torch.storage.np.polynomial.polynomial.la.multi_dot",
    "signature": "(arrays, *, out=None)",
    "description": "Compute the dot product of two or more arrays in a single function call,"
  },
  "1251": {
    "name": "norm",
    "module": "torch.storage.np.polynomial.polynomial.la",
    "fullName": "torch.storage.np.polynomial.polynomial.la.norm",
    "signature": "(x, ord=None, axis=None, keepdims=False)",
    "description": "Matrix or vector norm."
  },
  "1252": {
    "name": "pinv",
    "module": "torch.storage.np.polynomial.polynomial.la",
    "fullName": "torch.storage.np.polynomial.polynomial.la.pinv",
    "signature": "(a, rcond=1e-15, hermitian=False)",
    "description": "Compute the (Moore-Penrose) pseudo-inverse of a matrix."
  },
  "1253": {
    "name": "slogdet",
    "module": "torch.storage.np.polynomial.polynomial.la",
    "fullName": "torch.storage.np.polynomial.polynomial.la.slogdet",
    "signature": "(a)",
    "description": "Compute the sign and (natural) logarithm of the determinant of an array."
  },
  "1254": {
    "name": "solve",
    "module": "torch.storage.np.polynomial.polynomial.la",
    "fullName": "torch.storage.np.polynomial.polynomial.la.solve",
    "signature": "(a, b)",
    "description": "Solve a linear matrix equation, or system of linear scalar equations."
  },
  "1255": {
    "name": "svd",
    "module": "torch.storage.np.polynomial.polynomial.la",
    "fullName": "torch.storage.np.polynomial.polynomial.la.svd",
    "signature": "(a, full_matrices=True, compute_uv=True, hermitian=False)",
    "description": "Singular Value Decomposition."
  },
  "1256": {
    "name": "tensorinv",
    "module": "torch.storage.np.polynomial.polynomial.la",
    "fullName": "torch.storage.np.polynomial.polynomial.la.tensorinv",
    "signature": "(a, ind=2)",
    "description": "Compute the 'inverse' of an N-dimensional array."
  },
  "1257": {
    "name": "tensorsolve",
    "module": "torch.storage.np.polynomial.polynomial.la",
    "fullName": "torch.storage.np.polynomial.polynomial.la.tensorsolve",
    "signature": "(a, b, axes=None)",
    "description": "Solve the tensor equation ``a x = b`` for x."
  },
  "1258": {
    "name": "all",
    "module": "torch.storage.np.polynomial.polynomial.la.linalg",
    "fullName": "torch.storage.np.polynomial.polynomial.la.linalg.all",
    "signature": "(a, axis=None, out=None, keepdims=<no value>, *, where=<no value>)",
    "description": "Test whether all array elements along a given axis evaluate to True."
  },
  "1259": {
    "name": "amax",
    "module": "torch.storage.np.polynomial.polynomial.la.linalg",
    "fullName": "torch.storage.np.polynomial.polynomial.la.linalg.amax",
    "signature": "(a, axis=None, out=None, keepdims=<no value>, initial=<no value>, where=<no value>)",
    "description": "Return the maximum of an array or maximum along an axis."
  },
  "1260": {
    "name": "amin",
    "module": "torch.storage.np.polynomial.polynomial.la.linalg",
    "fullName": "torch.storage.np.polynomial.polynomial.la.linalg.amin",
    "signature": "(a, axis=None, out=None, keepdims=<no value>, initial=<no value>, where=<no value>)",
    "description": "Return the minimum of an array or minimum along an axis."
  },
  "1261": {
    "name": "argsort",
    "module": "torch.storage.np.polynomial.polynomial.la.linalg",
    "fullName": "torch.storage.np.polynomial.polynomial.la.linalg.argsort",
    "signature": "(a, axis=-1, kind=None, order=None)",
    "description": "Returns the indices that would sort an array."
  },
  "1262": {
    "name": "atleast_2d",
    "module": "torch.storage.np.polynomial.polynomial.la.linalg",
    "fullName": "torch.storage.np.polynomial.polynomial.la.linalg.atleast_2d",
    "signature": "(*arys)",
    "description": "View inputs as arrays with at least two dimensions."
  },
  "1263": {
    "name": "cholesky",
    "module": "torch.storage.np.polynomial.polynomial.la.linalg",
    "fullName": "torch.storage.np.polynomial.polynomial.la.linalg.cholesky",
    "signature": "(a)",
    "description": "Cholesky decomposition."
  },
  "1264": {
    "name": "cond",
    "module": "torch.storage.np.polynomial.polynomial.la.linalg",
    "fullName": "torch.storage.np.polynomial.polynomial.la.linalg.cond",
    "signature": "(x, p=None)",
    "description": "Compute the condition number of a matrix."
  },
  "1265": {
    "name": "count_nonzero",
    "module": "torch.storage.np.polynomial.polynomial.la.linalg",
    "fullName": "torch.storage.np.polynomial.polynomial.la.linalg.count_nonzero",
    "signature": "(a, axis=None, *, keepdims=False)",
    "description": "Counts the number of non-zero values in the array ``a``."
  },
  "1266": {
    "name": "det",
    "module": "torch.storage.np.polynomial.polynomial.la.linalg",
    "fullName": "torch.storage.np.polynomial.polynomial.la.linalg.det",
    "signature": "(a)",
    "description": "Compute the determinant of an array."
  },
  "1267": {
    "name": "dot",
    "module": "torch.storage.np.polynomial.polynomial.la.linalg",
    "fullName": "torch.storage.np.polynomial.polynomial.la.linalg.dot",
    "signature": "N/A",
    "description": "dot(a, b, out=None)"
  },
  "1268": {
    "name": "eig",
    "module": "torch.storage.np.polynomial.polynomial.la.linalg",
    "fullName": "torch.storage.np.polynomial.polynomial.la.linalg.eig",
    "signature": "(a)",
    "description": "Compute the eigenvalues and right eigenvectors of a square array."
  },
  "1269": {
    "name": "eigh",
    "module": "torch.storage.np.polynomial.polynomial.la.linalg",
    "fullName": "torch.storage.np.polynomial.polynomial.la.linalg.eigh",
    "signature": "(a, UPLO='L')",
    "description": "Return the eigenvalues and eigenvectors of a complex Hermitian"
  },
  "1270": {
    "name": "eigvals",
    "module": "torch.storage.np.polynomial.polynomial.la.linalg",
    "fullName": "torch.storage.np.polynomial.polynomial.la.linalg.eigvals",
    "signature": "(a)",
    "description": "Compute the eigenvalues of a general matrix."
  },
  "1271": {
    "name": "eigvalsh",
    "module": "torch.storage.np.polynomial.polynomial.la.linalg",
    "fullName": "torch.storage.np.polynomial.polynomial.la.linalg.eigvalsh",
    "signature": "(a, UPLO='L')",
    "description": "Compute the eigenvalues of a complex Hermitian or real symmetric matrix."
  },
  "1272": {
    "name": "empty_like",
    "module": "torch.storage.np.polynomial.polynomial.la.linalg",
    "fullName": "torch.storage.np.polynomial.polynomial.la.linalg.empty_like",
    "signature": "N/A",
    "description": "empty_like(prototype, dtype=None, order='K', subok=True, shape=None)"
  },
  "1273": {
    "name": "eye",
    "module": "torch.storage.np.polynomial.polynomial.la.linalg",
    "fullName": "torch.storage.np.polynomial.polynomial.la.linalg.eye",
    "signature": "(N, M=None, k=0, dtype=<class 'float'>, order='C', *, like=None)",
    "description": "Return a 2-D array with ones on the diagonal and zeros elsewhere."
  },
  "1274": {
    "name": "get_linalg_error_extobj",
    "module": "torch.storage.np.polynomial.polynomial.la.linalg",
    "fullName": "torch.storage.np.polynomial.polynomial.la.linalg.get_linalg_error_extobj",
    "signature": "(callback)",
    "description": "No description available."
  },
  "1275": {
    "name": "inv",
    "module": "torch.storage.np.polynomial.polynomial.la.linalg",
    "fullName": "torch.storage.np.polynomial.polynomial.la.linalg.inv",
    "signature": "(a)",
    "description": "Compute the (multiplicative) inverse of a matrix."
  },
  "1276": {
    "name": "isComplexType",
    "module": "torch.storage.np.polynomial.polynomial.la.linalg",
    "fullName": "torch.storage.np.polynomial.polynomial.la.linalg.isComplexType",
    "signature": "(t)",
    "description": "No description available."
  },
  "1277": {
    "name": "lstsq",
    "module": "torch.storage.np.polynomial.polynomial.la.linalg",
    "fullName": "torch.storage.np.polynomial.polynomial.la.linalg.lstsq",
    "signature": "(a, b, rcond='warn')",
    "description": "Return the least-squares solution to a linear matrix equation."
  },
  "1278": {
    "name": "matrix_power",
    "module": "torch.storage.np.polynomial.polynomial.la.linalg",
    "fullName": "torch.storage.np.polynomial.polynomial.la.linalg.matrix_power",
    "signature": "(a, n)",
    "description": "Raise a square matrix to the (integer) power `n`."
  },
  "1279": {
    "name": "matrix_rank",
    "module": "torch.storage.np.polynomial.polynomial.la.linalg",
    "fullName": "torch.storage.np.polynomial.polynomial.la.linalg.matrix_rank",
    "signature": "(A, tol=None, hermitian=False)",
    "description": "Return matrix rank of array using SVD method"
  },
  "1280": {
    "name": "moveaxis",
    "module": "torch.storage.np.polynomial.polynomial.la.linalg",
    "fullName": "torch.storage.np.polynomial.polynomial.la.linalg.moveaxis",
    "signature": "(a, source, destination)",
    "description": "Move axes of an array to new positions."
  },
  "1281": {
    "name": "multi_dot",
    "module": "torch.storage.np.polynomial.polynomial.la.linalg",
    "fullName": "torch.storage.np.polynomial.polynomial.la.linalg.multi_dot",
    "signature": "(arrays, *, out=None)",
    "description": "Compute the dot product of two or more arrays in a single function call,"
  },
  "1282": {
    "name": "norm",
    "module": "torch.storage.np.polynomial.polynomial.la.linalg",
    "fullName": "torch.storage.np.polynomial.polynomial.la.linalg.norm",
    "signature": "(x, ord=None, axis=None, keepdims=False)",
    "description": "Matrix or vector norm."
  },
  "1283": {
    "name": "pinv",
    "module": "torch.storage.np.polynomial.polynomial.la.linalg",
    "fullName": "torch.storage.np.polynomial.polynomial.la.linalg.pinv",
    "signature": "(a, rcond=1e-15, hermitian=False)",
    "description": "Compute the (Moore-Penrose) pseudo-inverse of a matrix."
  },
  "1284": {
    "name": "product",
    "module": "torch.storage.np.polynomial.polynomial.la.linalg",
    "fullName": "torch.storage.np.polynomial.polynomial.la.linalg.product",
    "signature": "(*args, **kwargs)",
    "description": "Return the product of array elements over a given axis."
  },
  "1285": {
    "name": "set_module",
    "module": "torch.storage.np.polynomial.polynomial.la.linalg",
    "fullName": "torch.storage.np.polynomial.polynomial.la.linalg.set_module",
    "signature": "(module)",
    "description": "Decorator for overriding __module__ on a function or class."
  },
  "1286": {
    "name": "slogdet",
    "module": "torch.storage.np.polynomial.polynomial.la.linalg",
    "fullName": "torch.storage.np.polynomial.polynomial.la.linalg.slogdet",
    "signature": "(a)",
    "description": "Compute the sign and (natural) logarithm of the determinant of an array."
  },
  "1287": {
    "name": "solve",
    "module": "torch.storage.np.polynomial.polynomial.la.linalg",
    "fullName": "torch.storage.np.polynomial.polynomial.la.linalg.solve",
    "signature": "(a, b)",
    "description": "Solve a linear matrix equation, or system of linear scalar equations."
  },
  "1288": {
    "name": "sort",
    "module": "torch.storage.np.polynomial.polynomial.la.linalg",
    "fullName": "torch.storage.np.polynomial.polynomial.la.linalg.sort",
    "signature": "(a, axis=-1, kind=None, order=None)",
    "description": "Return a sorted copy of an array."
  },
  "1289": {
    "name": "sum",
    "module": "torch.storage.np.polynomial.polynomial.la.linalg",
    "fullName": "torch.storage.np.polynomial.polynomial.la.linalg.sum",
    "signature": "(a, axis=None, dtype=None, out=None, keepdims=<no value>, initial=<no value>, where=<no value>)",
    "description": "Sum of array elements over a given axis."
  },
  "1290": {
    "name": "svd",
    "module": "torch.storage.np.polynomial.polynomial.la.linalg",
    "fullName": "torch.storage.np.polynomial.polynomial.la.linalg.svd",
    "signature": "(a, full_matrices=True, compute_uv=True, hermitian=False)",
    "description": "Singular Value Decomposition."
  },
  "1291": {
    "name": "swapaxes",
    "module": "torch.storage.np.polynomial.polynomial.la.linalg",
    "fullName": "torch.storage.np.polynomial.polynomial.la.linalg.swapaxes",
    "signature": "(a, axis1, axis2)",
    "description": "Interchange two axes of an array."
  },
  "1292": {
    "name": "tensorinv",
    "module": "torch.storage.np.polynomial.polynomial.la.linalg",
    "fullName": "torch.storage.np.polynomial.polynomial.la.linalg.tensorinv",
    "signature": "(a, ind=2)",
    "description": "Compute the 'inverse' of an N-dimensional array."
  },
  "1293": {
    "name": "tensorsolve",
    "module": "torch.storage.np.polynomial.polynomial.la.linalg",
    "fullName": "torch.storage.np.polynomial.polynomial.la.linalg.tensorsolve",
    "signature": "(a, b, axes=None)",
    "description": "Solve the tensor equation ``a x = b`` for x."
  },
  "1294": {
    "name": "transpose",
    "module": "torch.storage.np.polynomial.polynomial.la.linalg",
    "fullName": "torch.storage.np.polynomial.polynomial.la.linalg.transpose",
    "signature": "(a)",
    "description": "Transpose each matrix in a stack of matrices."
  },
  "1295": {
    "name": "triu",
    "module": "torch.storage.np.polynomial.polynomial.la.linalg",
    "fullName": "torch.storage.np.polynomial.polynomial.la.linalg.triu",
    "signature": "(m, k=0)",
    "description": "Upper triangle of an array."
  },
  "1296": {
    "name": "leg2poly",
    "module": "torch.storage.np.polynomial.legendre",
    "fullName": "torch.storage.np.polynomial.legendre.leg2poly",
    "signature": "(c)",
    "description": "Convert a Legendre series to a polynomial."
  },
  "1297": {
    "name": "legadd",
    "module": "torch.storage.np.polynomial.legendre",
    "fullName": "torch.storage.np.polynomial.legendre.legadd",
    "signature": "(c1, c2)",
    "description": "Add one Legendre series to another."
  },
  "1298": {
    "name": "legcompanion",
    "module": "torch.storage.np.polynomial.legendre",
    "fullName": "torch.storage.np.polynomial.legendre.legcompanion",
    "signature": "(c)",
    "description": "Return the scaled companion matrix of c."
  },
  "1299": {
    "name": "legder",
    "module": "torch.storage.np.polynomial.legendre",
    "fullName": "torch.storage.np.polynomial.legendre.legder",
    "signature": "(c, m=1, scl=1, axis=0)",
    "description": "Differentiate a Legendre series."
  },
  "1300": {
    "name": "legdiv",
    "module": "torch.storage.np.polynomial.legendre",
    "fullName": "torch.storage.np.polynomial.legendre.legdiv",
    "signature": "(c1, c2)",
    "description": "Divide one Legendre series by another."
  },
  "1301": {
    "name": "legfit",
    "module": "torch.storage.np.polynomial.legendre",
    "fullName": "torch.storage.np.polynomial.legendre.legfit",
    "signature": "(x, y, deg, rcond=None, full=False, w=None)",
    "description": "Least squares fit of Legendre series to data."
  },
  "1302": {
    "name": "legfromroots",
    "module": "torch.storage.np.polynomial.legendre",
    "fullName": "torch.storage.np.polynomial.legendre.legfromroots",
    "signature": "(roots)",
    "description": "Generate a Legendre series with given roots."
  },
  "1303": {
    "name": "leggauss",
    "module": "torch.storage.np.polynomial.legendre",
    "fullName": "torch.storage.np.polynomial.legendre.leggauss",
    "signature": "(deg)",
    "description": "Gauss-Legendre quadrature."
  },
  "1304": {
    "name": "leggrid2d",
    "module": "torch.storage.np.polynomial.legendre",
    "fullName": "torch.storage.np.polynomial.legendre.leggrid2d",
    "signature": "(x, y, c)",
    "description": "Evaluate a 2-D Legendre series on the Cartesian product of x and y."
  },
  "1305": {
    "name": "leggrid3d",
    "module": "torch.storage.np.polynomial.legendre",
    "fullName": "torch.storage.np.polynomial.legendre.leggrid3d",
    "signature": "(x, y, z, c)",
    "description": "Evaluate a 3-D Legendre series on the Cartesian product of x, y, and z."
  },
  "1306": {
    "name": "legint",
    "module": "torch.storage.np.polynomial.legendre",
    "fullName": "torch.storage.np.polynomial.legendre.legint",
    "signature": "(c, m=1, k=[], lbnd=0, scl=1, axis=0)",
    "description": "Integrate a Legendre series."
  },
  "1307": {
    "name": "legline",
    "module": "torch.storage.np.polynomial.legendre",
    "fullName": "torch.storage.np.polynomial.legendre.legline",
    "signature": "(off, scl)",
    "description": "Legendre series whose graph is a straight line."
  },
  "1308": {
    "name": "legmul",
    "module": "torch.storage.np.polynomial.legendre",
    "fullName": "torch.storage.np.polynomial.legendre.legmul",
    "signature": "(c1, c2)",
    "description": "Multiply one Legendre series by another."
  },
  "1309": {
    "name": "legmulx",
    "module": "torch.storage.np.polynomial.legendre",
    "fullName": "torch.storage.np.polynomial.legendre.legmulx",
    "signature": "(c)",
    "description": "Multiply a Legendre series by x."
  },
  "1310": {
    "name": "legpow",
    "module": "torch.storage.np.polynomial.legendre",
    "fullName": "torch.storage.np.polynomial.legendre.legpow",
    "signature": "(c, pow, maxpower=16)",
    "description": "Raise a Legendre series to a power."
  },
  "1311": {
    "name": "legroots",
    "module": "torch.storage.np.polynomial.legendre",
    "fullName": "torch.storage.np.polynomial.legendre.legroots",
    "signature": "(c)",
    "description": "Compute the roots of a Legendre series."
  },
  "1312": {
    "name": "legsub",
    "module": "torch.storage.np.polynomial.legendre",
    "fullName": "torch.storage.np.polynomial.legendre.legsub",
    "signature": "(c1, c2)",
    "description": "Subtract one Legendre series from another."
  },
  "1313": {
    "name": "legtrim",
    "module": "torch.storage.np.polynomial.legendre",
    "fullName": "torch.storage.np.polynomial.legendre.legtrim",
    "signature": "(c, tol=0)",
    "description": "Remove \"small\" \"trailing\" coefficients from a polynomial."
  },
  "1314": {
    "name": "legval",
    "module": "torch.storage.np.polynomial.legendre",
    "fullName": "torch.storage.np.polynomial.legendre.legval",
    "signature": "(x, c, tensor=True)",
    "description": "Evaluate a Legendre series at points x."
  },
  "1315": {
    "name": "legval2d",
    "module": "torch.storage.np.polynomial.legendre",
    "fullName": "torch.storage.np.polynomial.legendre.legval2d",
    "signature": "(x, y, c)",
    "description": "Evaluate a 2-D Legendre series at points (x, y)."
  },
  "1316": {
    "name": "legval3d",
    "module": "torch.storage.np.polynomial.legendre",
    "fullName": "torch.storage.np.polynomial.legendre.legval3d",
    "signature": "(x, y, z, c)",
    "description": "Evaluate a 3-D Legendre series at points (x, y, z)."
  },
  "1317": {
    "name": "legvander",
    "module": "torch.storage.np.polynomial.legendre",
    "fullName": "torch.storage.np.polynomial.legendre.legvander",
    "signature": "(x, deg)",
    "description": "Pseudo-Vandermonde matrix of given degree."
  },
  "1318": {
    "name": "legvander2d",
    "module": "torch.storage.np.polynomial.legendre",
    "fullName": "torch.storage.np.polynomial.legendre.legvander2d",
    "signature": "(x, y, deg)",
    "description": "Pseudo-Vandermonde matrix of given degrees."
  },
  "1319": {
    "name": "legvander3d",
    "module": "torch.storage.np.polynomial.legendre",
    "fullName": "torch.storage.np.polynomial.legendre.legvander3d",
    "signature": "(x, y, z, deg)",
    "description": "Pseudo-Vandermonde matrix of given degrees."
  },
  "1320": {
    "name": "legweight",
    "module": "torch.storage.np.polynomial.legendre",
    "fullName": "torch.storage.np.polynomial.legendre.legweight",
    "signature": "(x)",
    "description": "Weight function of the Legendre polynomials."
  },
  "1321": {
    "name": "poly2leg",
    "module": "torch.storage.np.polynomial.legendre",
    "fullName": "torch.storage.np.polynomial.legendre.poly2leg",
    "signature": "(pol)",
    "description": "Convert a polynomial to a Legendre series."
  },
  "1322": {
    "name": "lag2poly",
    "module": "torch.storage.np.polynomial.laguerre",
    "fullName": "torch.storage.np.polynomial.laguerre.lag2poly",
    "signature": "(c)",
    "description": "Convert a Laguerre series to a polynomial."
  },
  "1323": {
    "name": "lagadd",
    "module": "torch.storage.np.polynomial.laguerre",
    "fullName": "torch.storage.np.polynomial.laguerre.lagadd",
    "signature": "(c1, c2)",
    "description": "Add one Laguerre series to another."
  },
  "1324": {
    "name": "lagcompanion",
    "module": "torch.storage.np.polynomial.laguerre",
    "fullName": "torch.storage.np.polynomial.laguerre.lagcompanion",
    "signature": "(c)",
    "description": "Return the companion matrix of c."
  },
  "1325": {
    "name": "lagder",
    "module": "torch.storage.np.polynomial.laguerre",
    "fullName": "torch.storage.np.polynomial.laguerre.lagder",
    "signature": "(c, m=1, scl=1, axis=0)",
    "description": "Differentiate a Laguerre series."
  },
  "1326": {
    "name": "lagdiv",
    "module": "torch.storage.np.polynomial.laguerre",
    "fullName": "torch.storage.np.polynomial.laguerre.lagdiv",
    "signature": "(c1, c2)",
    "description": "Divide one Laguerre series by another."
  },
  "1327": {
    "name": "lagfit",
    "module": "torch.storage.np.polynomial.laguerre",
    "fullName": "torch.storage.np.polynomial.laguerre.lagfit",
    "signature": "(x, y, deg, rcond=None, full=False, w=None)",
    "description": "Least squares fit of Laguerre series to data."
  },
  "1328": {
    "name": "lagfromroots",
    "module": "torch.storage.np.polynomial.laguerre",
    "fullName": "torch.storage.np.polynomial.laguerre.lagfromroots",
    "signature": "(roots)",
    "description": "Generate a Laguerre series with given roots."
  },
  "1329": {
    "name": "laggauss",
    "module": "torch.storage.np.polynomial.laguerre",
    "fullName": "torch.storage.np.polynomial.laguerre.laggauss",
    "signature": "(deg)",
    "description": "Gauss-Laguerre quadrature."
  },
  "1330": {
    "name": "laggrid2d",
    "module": "torch.storage.np.polynomial.laguerre",
    "fullName": "torch.storage.np.polynomial.laguerre.laggrid2d",
    "signature": "(x, y, c)",
    "description": "Evaluate a 2-D Laguerre series on the Cartesian product of x and y."
  },
  "1331": {
    "name": "laggrid3d",
    "module": "torch.storage.np.polynomial.laguerre",
    "fullName": "torch.storage.np.polynomial.laguerre.laggrid3d",
    "signature": "(x, y, z, c)",
    "description": "Evaluate a 3-D Laguerre series on the Cartesian product of x, y, and z."
  },
  "1332": {
    "name": "lagint",
    "module": "torch.storage.np.polynomial.laguerre",
    "fullName": "torch.storage.np.polynomial.laguerre.lagint",
    "signature": "(c, m=1, k=[], lbnd=0, scl=1, axis=0)",
    "description": "Integrate a Laguerre series."
  },
  "1333": {
    "name": "lagline",
    "module": "torch.storage.np.polynomial.laguerre",
    "fullName": "torch.storage.np.polynomial.laguerre.lagline",
    "signature": "(off, scl)",
    "description": "Laguerre series whose graph is a straight line."
  },
  "1334": {
    "name": "lagmul",
    "module": "torch.storage.np.polynomial.laguerre",
    "fullName": "torch.storage.np.polynomial.laguerre.lagmul",
    "signature": "(c1, c2)",
    "description": "Multiply one Laguerre series by another."
  },
  "1335": {
    "name": "lagmulx",
    "module": "torch.storage.np.polynomial.laguerre",
    "fullName": "torch.storage.np.polynomial.laguerre.lagmulx",
    "signature": "(c)",
    "description": "Multiply a Laguerre series by x."
  },
  "1336": {
    "name": "lagpow",
    "module": "torch.storage.np.polynomial.laguerre",
    "fullName": "torch.storage.np.polynomial.laguerre.lagpow",
    "signature": "(c, pow, maxpower=16)",
    "description": "Raise a Laguerre series to a power."
  },
  "1337": {
    "name": "lagroots",
    "module": "torch.storage.np.polynomial.laguerre",
    "fullName": "torch.storage.np.polynomial.laguerre.lagroots",
    "signature": "(c)",
    "description": "Compute the roots of a Laguerre series."
  },
  "1338": {
    "name": "lagsub",
    "module": "torch.storage.np.polynomial.laguerre",
    "fullName": "torch.storage.np.polynomial.laguerre.lagsub",
    "signature": "(c1, c2)",
    "description": "Subtract one Laguerre series from another."
  },
  "1339": {
    "name": "lagtrim",
    "module": "torch.storage.np.polynomial.laguerre",
    "fullName": "torch.storage.np.polynomial.laguerre.lagtrim",
    "signature": "(c, tol=0)",
    "description": "Remove \"small\" \"trailing\" coefficients from a polynomial."
  },
  "1340": {
    "name": "lagval",
    "module": "torch.storage.np.polynomial.laguerre",
    "fullName": "torch.storage.np.polynomial.laguerre.lagval",
    "signature": "(x, c, tensor=True)",
    "description": "Evaluate a Laguerre series at points x."
  },
  "1341": {
    "name": "lagval2d",
    "module": "torch.storage.np.polynomial.laguerre",
    "fullName": "torch.storage.np.polynomial.laguerre.lagval2d",
    "signature": "(x, y, c)",
    "description": "Evaluate a 2-D Laguerre series at points (x, y)."
  },
  "1342": {
    "name": "lagval3d",
    "module": "torch.storage.np.polynomial.laguerre",
    "fullName": "torch.storage.np.polynomial.laguerre.lagval3d",
    "signature": "(x, y, z, c)",
    "description": "Evaluate a 3-D Laguerre series at points (x, y, z)."
  },
  "1343": {
    "name": "lagvander",
    "module": "torch.storage.np.polynomial.laguerre",
    "fullName": "torch.storage.np.polynomial.laguerre.lagvander",
    "signature": "(x, deg)",
    "description": "Pseudo-Vandermonde matrix of given degree."
  },
  "1344": {
    "name": "lagvander2d",
    "module": "torch.storage.np.polynomial.laguerre",
    "fullName": "torch.storage.np.polynomial.laguerre.lagvander2d",
    "signature": "(x, y, deg)",
    "description": "Pseudo-Vandermonde matrix of given degrees."
  },
  "1345": {
    "name": "lagvander3d",
    "module": "torch.storage.np.polynomial.laguerre",
    "fullName": "torch.storage.np.polynomial.laguerre.lagvander3d",
    "signature": "(x, y, z, deg)",
    "description": "Pseudo-Vandermonde matrix of given degrees."
  },
  "1346": {
    "name": "lagweight",
    "module": "torch.storage.np.polynomial.laguerre",
    "fullName": "torch.storage.np.polynomial.laguerre.lagweight",
    "signature": "(x)",
    "description": "Weight function of the Laguerre polynomials."
  },
  "1347": {
    "name": "poly2lag",
    "module": "torch.storage.np.polynomial.laguerre",
    "fullName": "torch.storage.np.polynomial.laguerre.poly2lag",
    "signature": "(pol)",
    "description": "poly2lag(pol)"
  },
  "1348": {
    "name": "herme2poly",
    "module": "torch.storage.np.polynomial.hermite_e",
    "fullName": "torch.storage.np.polynomial.hermite_e.herme2poly",
    "signature": "(c)",
    "description": "Convert a Hermite series to a polynomial."
  },
  "1349": {
    "name": "hermeadd",
    "module": "torch.storage.np.polynomial.hermite_e",
    "fullName": "torch.storage.np.polynomial.hermite_e.hermeadd",
    "signature": "(c1, c2)",
    "description": "Add one Hermite series to another."
  },
  "1350": {
    "name": "hermecompanion",
    "module": "torch.storage.np.polynomial.hermite_e",
    "fullName": "torch.storage.np.polynomial.hermite_e.hermecompanion",
    "signature": "(c)",
    "description": "Return the scaled companion matrix of c."
  },
  "1351": {
    "name": "hermeder",
    "module": "torch.storage.np.polynomial.hermite_e",
    "fullName": "torch.storage.np.polynomial.hermite_e.hermeder",
    "signature": "(c, m=1, scl=1, axis=0)",
    "description": "Differentiate a Hermite_e series."
  },
  "1352": {
    "name": "hermediv",
    "module": "torch.storage.np.polynomial.hermite_e",
    "fullName": "torch.storage.np.polynomial.hermite_e.hermediv",
    "signature": "(c1, c2)",
    "description": "Divide one Hermite series by another."
  },
  "1353": {
    "name": "hermefit",
    "module": "torch.storage.np.polynomial.hermite_e",
    "fullName": "torch.storage.np.polynomial.hermite_e.hermefit",
    "signature": "(x, y, deg, rcond=None, full=False, w=None)",
    "description": "Least squares fit of Hermite series to data."
  },
  "1354": {
    "name": "hermefromroots",
    "module": "torch.storage.np.polynomial.hermite_e",
    "fullName": "torch.storage.np.polynomial.hermite_e.hermefromroots",
    "signature": "(roots)",
    "description": "Generate a HermiteE series with given roots."
  },
  "1355": {
    "name": "hermegauss",
    "module": "torch.storage.np.polynomial.hermite_e",
    "fullName": "torch.storage.np.polynomial.hermite_e.hermegauss",
    "signature": "(deg)",
    "description": "Gauss-HermiteE quadrature."
  },
  "1356": {
    "name": "hermegrid2d",
    "module": "torch.storage.np.polynomial.hermite_e",
    "fullName": "torch.storage.np.polynomial.hermite_e.hermegrid2d",
    "signature": "(x, y, c)",
    "description": "Evaluate a 2-D HermiteE series on the Cartesian product of x and y."
  },
  "1357": {
    "name": "hermegrid3d",
    "module": "torch.storage.np.polynomial.hermite_e",
    "fullName": "torch.storage.np.polynomial.hermite_e.hermegrid3d",
    "signature": "(x, y, z, c)",
    "description": "Evaluate a 3-D HermiteE series on the Cartesian product of x, y, and z."
  },
  "1358": {
    "name": "hermeint",
    "module": "torch.storage.np.polynomial.hermite_e",
    "fullName": "torch.storage.np.polynomial.hermite_e.hermeint",
    "signature": "(c, m=1, k=[], lbnd=0, scl=1, axis=0)",
    "description": "Integrate a Hermite_e series."
  },
  "1359": {
    "name": "hermeline",
    "module": "torch.storage.np.polynomial.hermite_e",
    "fullName": "torch.storage.np.polynomial.hermite_e.hermeline",
    "signature": "(off, scl)",
    "description": "Hermite series whose graph is a straight line."
  },
  "1360": {
    "name": "hermemul",
    "module": "torch.storage.np.polynomial.hermite_e",
    "fullName": "torch.storage.np.polynomial.hermite_e.hermemul",
    "signature": "(c1, c2)",
    "description": "Multiply one Hermite series by another."
  },
  "1361": {
    "name": "hermemulx",
    "module": "torch.storage.np.polynomial.hermite_e",
    "fullName": "torch.storage.np.polynomial.hermite_e.hermemulx",
    "signature": "(c)",
    "description": "Multiply a Hermite series by x."
  },
  "1362": {
    "name": "hermepow",
    "module": "torch.storage.np.polynomial.hermite_e",
    "fullName": "torch.storage.np.polynomial.hermite_e.hermepow",
    "signature": "(c, pow, maxpower=16)",
    "description": "Raise a Hermite series to a power."
  },
  "1363": {
    "name": "hermeroots",
    "module": "torch.storage.np.polynomial.hermite_e",
    "fullName": "torch.storage.np.polynomial.hermite_e.hermeroots",
    "signature": "(c)",
    "description": "Compute the roots of a HermiteE series."
  },
  "1364": {
    "name": "hermesub",
    "module": "torch.storage.np.polynomial.hermite_e",
    "fullName": "torch.storage.np.polynomial.hermite_e.hermesub",
    "signature": "(c1, c2)",
    "description": "Subtract one Hermite series from another."
  },
  "1365": {
    "name": "hermetrim",
    "module": "torch.storage.np.polynomial.hermite_e",
    "fullName": "torch.storage.np.polynomial.hermite_e.hermetrim",
    "signature": "(c, tol=0)",
    "description": "Remove \"small\" \"trailing\" coefficients from a polynomial."
  },
  "1366": {
    "name": "hermeval",
    "module": "torch.storage.np.polynomial.hermite_e",
    "fullName": "torch.storage.np.polynomial.hermite_e.hermeval",
    "signature": "(x, c, tensor=True)",
    "description": "Evaluate an HermiteE series at points x."
  },
  "1367": {
    "name": "hermeval2d",
    "module": "torch.storage.np.polynomial.hermite_e",
    "fullName": "torch.storage.np.polynomial.hermite_e.hermeval2d",
    "signature": "(x, y, c)",
    "description": "Evaluate a 2-D HermiteE series at points (x, y)."
  },
  "1368": {
    "name": "hermeval3d",
    "module": "torch.storage.np.polynomial.hermite_e",
    "fullName": "torch.storage.np.polynomial.hermite_e.hermeval3d",
    "signature": "(x, y, z, c)",
    "description": "Evaluate a 3-D Hermite_e series at points (x, y, z)."
  },
  "1369": {
    "name": "hermevander",
    "module": "torch.storage.np.polynomial.hermite_e",
    "fullName": "torch.storage.np.polynomial.hermite_e.hermevander",
    "signature": "(x, deg)",
    "description": "Pseudo-Vandermonde matrix of given degree."
  },
  "1370": {
    "name": "hermevander2d",
    "module": "torch.storage.np.polynomial.hermite_e",
    "fullName": "torch.storage.np.polynomial.hermite_e.hermevander2d",
    "signature": "(x, y, deg)",
    "description": "Pseudo-Vandermonde matrix of given degrees."
  },
  "1371": {
    "name": "hermevander3d",
    "module": "torch.storage.np.polynomial.hermite_e",
    "fullName": "torch.storage.np.polynomial.hermite_e.hermevander3d",
    "signature": "(x, y, z, deg)",
    "description": "Pseudo-Vandermonde matrix of given degrees."
  },
  "1372": {
    "name": "hermeweight",
    "module": "torch.storage.np.polynomial.hermite_e",
    "fullName": "torch.storage.np.polynomial.hermite_e.hermeweight",
    "signature": "(x)",
    "description": "Weight function of the Hermite_e polynomials."
  },
  "1373": {
    "name": "poly2herme",
    "module": "torch.storage.np.polynomial.hermite_e",
    "fullName": "torch.storage.np.polynomial.hermite_e.poly2herme",
    "signature": "(pol)",
    "description": "poly2herme(pol)"
  },
  "1374": {
    "name": "herm2poly",
    "module": "torch.storage.np.polynomial.hermite",
    "fullName": "torch.storage.np.polynomial.hermite.herm2poly",
    "signature": "(c)",
    "description": "Convert a Hermite series to a polynomial."
  },
  "1375": {
    "name": "hermadd",
    "module": "torch.storage.np.polynomial.hermite",
    "fullName": "torch.storage.np.polynomial.hermite.hermadd",
    "signature": "(c1, c2)",
    "description": "Add one Hermite series to another."
  },
  "1376": {
    "name": "hermcompanion",
    "module": "torch.storage.np.polynomial.hermite",
    "fullName": "torch.storage.np.polynomial.hermite.hermcompanion",
    "signature": "(c)",
    "description": "Return the scaled companion matrix of c."
  },
  "1377": {
    "name": "hermder",
    "module": "torch.storage.np.polynomial.hermite",
    "fullName": "torch.storage.np.polynomial.hermite.hermder",
    "signature": "(c, m=1, scl=1, axis=0)",
    "description": "Differentiate a Hermite series."
  },
  "1378": {
    "name": "hermdiv",
    "module": "torch.storage.np.polynomial.hermite",
    "fullName": "torch.storage.np.polynomial.hermite.hermdiv",
    "signature": "(c1, c2)",
    "description": "Divide one Hermite series by another."
  },
  "1379": {
    "name": "hermfit",
    "module": "torch.storage.np.polynomial.hermite",
    "fullName": "torch.storage.np.polynomial.hermite.hermfit",
    "signature": "(x, y, deg, rcond=None, full=False, w=None)",
    "description": "Least squares fit of Hermite series to data."
  },
  "1380": {
    "name": "hermfromroots",
    "module": "torch.storage.np.polynomial.hermite",
    "fullName": "torch.storage.np.polynomial.hermite.hermfromroots",
    "signature": "(roots)",
    "description": "Generate a Hermite series with given roots."
  },
  "1381": {
    "name": "hermgauss",
    "module": "torch.storage.np.polynomial.hermite",
    "fullName": "torch.storage.np.polynomial.hermite.hermgauss",
    "signature": "(deg)",
    "description": "Gauss-Hermite quadrature."
  },
  "1382": {
    "name": "hermgrid2d",
    "module": "torch.storage.np.polynomial.hermite",
    "fullName": "torch.storage.np.polynomial.hermite.hermgrid2d",
    "signature": "(x, y, c)",
    "description": "Evaluate a 2-D Hermite series on the Cartesian product of x and y."
  },
  "1383": {
    "name": "hermgrid3d",
    "module": "torch.storage.np.polynomial.hermite",
    "fullName": "torch.storage.np.polynomial.hermite.hermgrid3d",
    "signature": "(x, y, z, c)",
    "description": "Evaluate a 3-D Hermite series on the Cartesian product of x, y, and z."
  },
  "1384": {
    "name": "hermint",
    "module": "torch.storage.np.polynomial.hermite",
    "fullName": "torch.storage.np.polynomial.hermite.hermint",
    "signature": "(c, m=1, k=[], lbnd=0, scl=1, axis=0)",
    "description": "Integrate a Hermite series."
  },
  "1385": {
    "name": "hermline",
    "module": "torch.storage.np.polynomial.hermite",
    "fullName": "torch.storage.np.polynomial.hermite.hermline",
    "signature": "(off, scl)",
    "description": "Hermite series whose graph is a straight line."
  },
  "1386": {
    "name": "hermmul",
    "module": "torch.storage.np.polynomial.hermite",
    "fullName": "torch.storage.np.polynomial.hermite.hermmul",
    "signature": "(c1, c2)",
    "description": "Multiply one Hermite series by another."
  },
  "1387": {
    "name": "hermmulx",
    "module": "torch.storage.np.polynomial.hermite",
    "fullName": "torch.storage.np.polynomial.hermite.hermmulx",
    "signature": "(c)",
    "description": "Multiply a Hermite series by x."
  },
  "1388": {
    "name": "hermpow",
    "module": "torch.storage.np.polynomial.hermite",
    "fullName": "torch.storage.np.polynomial.hermite.hermpow",
    "signature": "(c, pow, maxpower=16)",
    "description": "Raise a Hermite series to a power."
  },
  "1389": {
    "name": "hermroots",
    "module": "torch.storage.np.polynomial.hermite",
    "fullName": "torch.storage.np.polynomial.hermite.hermroots",
    "signature": "(c)",
    "description": "Compute the roots of a Hermite series."
  },
  "1390": {
    "name": "hermsub",
    "module": "torch.storage.np.polynomial.hermite",
    "fullName": "torch.storage.np.polynomial.hermite.hermsub",
    "signature": "(c1, c2)",
    "description": "Subtract one Hermite series from another."
  },
  "1391": {
    "name": "hermtrim",
    "module": "torch.storage.np.polynomial.hermite",
    "fullName": "torch.storage.np.polynomial.hermite.hermtrim",
    "signature": "(c, tol=0)",
    "description": "Remove \"small\" \"trailing\" coefficients from a polynomial."
  },
  "1392": {
    "name": "hermval",
    "module": "torch.storage.np.polynomial.hermite",
    "fullName": "torch.storage.np.polynomial.hermite.hermval",
    "signature": "(x, c, tensor=True)",
    "description": "Evaluate an Hermite series at points x."
  },
  "1393": {
    "name": "hermval2d",
    "module": "torch.storage.np.polynomial.hermite",
    "fullName": "torch.storage.np.polynomial.hermite.hermval2d",
    "signature": "(x, y, c)",
    "description": "Evaluate a 2-D Hermite series at points (x, y)."
  },
  "1394": {
    "name": "hermval3d",
    "module": "torch.storage.np.polynomial.hermite",
    "fullName": "torch.storage.np.polynomial.hermite.hermval3d",
    "signature": "(x, y, z, c)",
    "description": "Evaluate a 3-D Hermite series at points (x, y, z)."
  },
  "1395": {
    "name": "hermvander",
    "module": "torch.storage.np.polynomial.hermite",
    "fullName": "torch.storage.np.polynomial.hermite.hermvander",
    "signature": "(x, deg)",
    "description": "Pseudo-Vandermonde matrix of given degree."
  },
  "1396": {
    "name": "hermvander2d",
    "module": "torch.storage.np.polynomial.hermite",
    "fullName": "torch.storage.np.polynomial.hermite.hermvander2d",
    "signature": "(x, y, deg)",
    "description": "Pseudo-Vandermonde matrix of given degrees."
  },
  "1397": {
    "name": "hermvander3d",
    "module": "torch.storage.np.polynomial.hermite",
    "fullName": "torch.storage.np.polynomial.hermite.hermvander3d",
    "signature": "(x, y, z, deg)",
    "description": "Pseudo-Vandermonde matrix of given degrees."
  },
  "1398": {
    "name": "hermweight",
    "module": "torch.storage.np.polynomial.hermite",
    "fullName": "torch.storage.np.polynomial.hermite.hermweight",
    "signature": "(x)",
    "description": "Weight function of the Hermite polynomials."
  },
  "1399": {
    "name": "poly2herm",
    "module": "torch.storage.np.polynomial.hermite",
    "fullName": "torch.storage.np.polynomial.hermite.poly2herm",
    "signature": "(pol)",
    "description": "poly2herm(pol)"
  },
  "1400": {
    "name": "cheb2poly",
    "module": "torch.storage.np.polynomial.chebyshev",
    "fullName": "torch.storage.np.polynomial.chebyshev.cheb2poly",
    "signature": "(c)",
    "description": "Convert a Chebyshev series to a polynomial."
  },
  "1401": {
    "name": "chebadd",
    "module": "torch.storage.np.polynomial.chebyshev",
    "fullName": "torch.storage.np.polynomial.chebyshev.chebadd",
    "signature": "(c1, c2)",
    "description": "Add one Chebyshev series to another."
  },
  "1402": {
    "name": "chebcompanion",
    "module": "torch.storage.np.polynomial.chebyshev",
    "fullName": "torch.storage.np.polynomial.chebyshev.chebcompanion",
    "signature": "(c)",
    "description": "Return the scaled companion matrix of c."
  },
  "1403": {
    "name": "chebder",
    "module": "torch.storage.np.polynomial.chebyshev",
    "fullName": "torch.storage.np.polynomial.chebyshev.chebder",
    "signature": "(c, m=1, scl=1, axis=0)",
    "description": "Differentiate a Chebyshev series."
  },
  "1404": {
    "name": "chebdiv",
    "module": "torch.storage.np.polynomial.chebyshev",
    "fullName": "torch.storage.np.polynomial.chebyshev.chebdiv",
    "signature": "(c1, c2)",
    "description": "Divide one Chebyshev series by another."
  },
  "1405": {
    "name": "chebfit",
    "module": "torch.storage.np.polynomial.chebyshev",
    "fullName": "torch.storage.np.polynomial.chebyshev.chebfit",
    "signature": "(x, y, deg, rcond=None, full=False, w=None)",
    "description": "Least squares fit of Chebyshev series to data."
  },
  "1406": {
    "name": "chebfromroots",
    "module": "torch.storage.np.polynomial.chebyshev",
    "fullName": "torch.storage.np.polynomial.chebyshev.chebfromroots",
    "signature": "(roots)",
    "description": "Generate a Chebyshev series with given roots."
  },
  "1407": {
    "name": "chebgauss",
    "module": "torch.storage.np.polynomial.chebyshev",
    "fullName": "torch.storage.np.polynomial.chebyshev.chebgauss",
    "signature": "(deg)",
    "description": "Gauss-Chebyshev quadrature."
  },
  "1408": {
    "name": "chebgrid2d",
    "module": "torch.storage.np.polynomial.chebyshev",
    "fullName": "torch.storage.np.polynomial.chebyshev.chebgrid2d",
    "signature": "(x, y, c)",
    "description": "Evaluate a 2-D Chebyshev series on the Cartesian product of x and y."
  },
  "1409": {
    "name": "chebgrid3d",
    "module": "torch.storage.np.polynomial.chebyshev",
    "fullName": "torch.storage.np.polynomial.chebyshev.chebgrid3d",
    "signature": "(x, y, z, c)",
    "description": "Evaluate a 3-D Chebyshev series on the Cartesian product of x, y, and z."
  },
  "1410": {
    "name": "chebint",
    "module": "torch.storage.np.polynomial.chebyshev",
    "fullName": "torch.storage.np.polynomial.chebyshev.chebint",
    "signature": "(c, m=1, k=[], lbnd=0, scl=1, axis=0)",
    "description": "Integrate a Chebyshev series."
  },
  "1411": {
    "name": "chebinterpolate",
    "module": "torch.storage.np.polynomial.chebyshev",
    "fullName": "torch.storage.np.polynomial.chebyshev.chebinterpolate",
    "signature": "(func, deg, args=())",
    "description": "Interpolate a function at the Chebyshev points of the first kind."
  },
  "1412": {
    "name": "chebline",
    "module": "torch.storage.np.polynomial.chebyshev",
    "fullName": "torch.storage.np.polynomial.chebyshev.chebline",
    "signature": "(off, scl)",
    "description": "Chebyshev series whose graph is a straight line."
  },
  "1413": {
    "name": "chebmul",
    "module": "torch.storage.np.polynomial.chebyshev",
    "fullName": "torch.storage.np.polynomial.chebyshev.chebmul",
    "signature": "(c1, c2)",
    "description": "Multiply one Chebyshev series by another."
  },
  "1414": {
    "name": "chebmulx",
    "module": "torch.storage.np.polynomial.chebyshev",
    "fullName": "torch.storage.np.polynomial.chebyshev.chebmulx",
    "signature": "(c)",
    "description": "Multiply a Chebyshev series by x."
  },
  "1415": {
    "name": "chebpow",
    "module": "torch.storage.np.polynomial.chebyshev",
    "fullName": "torch.storage.np.polynomial.chebyshev.chebpow",
    "signature": "(c, pow, maxpower=16)",
    "description": "Raise a Chebyshev series to a power."
  },
  "1416": {
    "name": "chebpts1",
    "module": "torch.storage.np.polynomial.chebyshev",
    "fullName": "torch.storage.np.polynomial.chebyshev.chebpts1",
    "signature": "(npts)",
    "description": "Chebyshev points of the first kind."
  },
  "1417": {
    "name": "chebpts2",
    "module": "torch.storage.np.polynomial.chebyshev",
    "fullName": "torch.storage.np.polynomial.chebyshev.chebpts2",
    "signature": "(npts)",
    "description": "Chebyshev points of the second kind."
  },
  "1418": {
    "name": "chebroots",
    "module": "torch.storage.np.polynomial.chebyshev",
    "fullName": "torch.storage.np.polynomial.chebyshev.chebroots",
    "signature": "(c)",
    "description": "Compute the roots of a Chebyshev series."
  },
  "1419": {
    "name": "chebsub",
    "module": "torch.storage.np.polynomial.chebyshev",
    "fullName": "torch.storage.np.polynomial.chebyshev.chebsub",
    "signature": "(c1, c2)",
    "description": "Subtract one Chebyshev series from another."
  },
  "1420": {
    "name": "chebtrim",
    "module": "torch.storage.np.polynomial.chebyshev",
    "fullName": "torch.storage.np.polynomial.chebyshev.chebtrim",
    "signature": "(c, tol=0)",
    "description": "Remove \"small\" \"trailing\" coefficients from a polynomial."
  },
  "1421": {
    "name": "chebval",
    "module": "torch.storage.np.polynomial.chebyshev",
    "fullName": "torch.storage.np.polynomial.chebyshev.chebval",
    "signature": "(x, c, tensor=True)",
    "description": "Evaluate a Chebyshev series at points x."
  },
  "1422": {
    "name": "chebval2d",
    "module": "torch.storage.np.polynomial.chebyshev",
    "fullName": "torch.storage.np.polynomial.chebyshev.chebval2d",
    "signature": "(x, y, c)",
    "description": "Evaluate a 2-D Chebyshev series at points (x, y)."
  },
  "1423": {
    "name": "chebval3d",
    "module": "torch.storage.np.polynomial.chebyshev",
    "fullName": "torch.storage.np.polynomial.chebyshev.chebval3d",
    "signature": "(x, y, z, c)",
    "description": "Evaluate a 3-D Chebyshev series at points (x, y, z)."
  },
  "1424": {
    "name": "chebvander",
    "module": "torch.storage.np.polynomial.chebyshev",
    "fullName": "torch.storage.np.polynomial.chebyshev.chebvander",
    "signature": "(x, deg)",
    "description": "Pseudo-Vandermonde matrix of given degree."
  },
  "1425": {
    "name": "chebvander2d",
    "module": "torch.storage.np.polynomial.chebyshev",
    "fullName": "torch.storage.np.polynomial.chebyshev.chebvander2d",
    "signature": "(x, y, deg)",
    "description": "Pseudo-Vandermonde matrix of given degrees."
  },
  "1426": {
    "name": "chebvander3d",
    "module": "torch.storage.np.polynomial.chebyshev",
    "fullName": "torch.storage.np.polynomial.chebyshev.chebvander3d",
    "signature": "(x, y, z, deg)",
    "description": "Pseudo-Vandermonde matrix of given degrees."
  },
  "1427": {
    "name": "chebweight",
    "module": "torch.storage.np.polynomial.chebyshev",
    "fullName": "torch.storage.np.polynomial.chebyshev.chebweight",
    "signature": "(x)",
    "description": "The weight function of the Chebyshev polynomials."
  },
  "1428": {
    "name": "poly2cheb",
    "module": "torch.storage.np.polynomial.chebyshev",
    "fullName": "torch.storage.np.polynomial.chebyshev.poly2cheb",
    "signature": "(pol)",
    "description": "Convert a polynomial to a Chebyshev series."
  },
  "1429": {
    "name": "allclose",
    "module": "torch.storage.np.ma",
    "fullName": "torch.storage.np.ma.allclose",
    "signature": "(a, b, masked_equal=True, rtol=1e-05, atol=1e-08)",
    "description": "Returns True if two arrays are element-wise equal within a tolerance."
  },
  "1430": {
    "name": "allequal",
    "module": "torch.storage.np.ma",
    "fullName": "torch.storage.np.ma.allequal",
    "signature": "(a, b, fill_value=True)",
    "description": "Return True if all entries of a and b are equal, using"
  },
  "1431": {
    "name": "amax",
    "module": "torch.storage.np.ma",
    "fullName": "torch.storage.np.ma.amax",
    "signature": "(a, axis=None, out=None, keepdims=<no value>, initial=<no value>, where=<no value>)",
    "description": "Return the maximum of an array or maximum along an axis."
  },
  "1432": {
    "name": "amin",
    "module": "torch.storage.np.ma",
    "fullName": "torch.storage.np.ma.amin",
    "signature": "(a, axis=None, out=None, keepdims=<no value>, initial=<no value>, where=<no value>)",
    "description": "Return the minimum of an array or minimum along an axis."
  },
  "1433": {
    "name": "append",
    "module": "torch.storage.np.ma",
    "fullName": "torch.storage.np.ma.append",
    "signature": "(a, b, axis=None)",
    "description": "Append values to the end of an array."
  },
  "1434": {
    "name": "apply_along_axis",
    "module": "torch.storage.np.ma",
    "fullName": "torch.storage.np.ma.apply_along_axis",
    "signature": "(func1d, axis, arr, *args, **kwargs)",
    "description": "Apply a function to 1-D slices along the given axis."
  },
  "1435": {
    "name": "apply_over_axes",
    "module": "torch.storage.np.ma",
    "fullName": "torch.storage.np.ma.apply_over_axes",
    "signature": "(func, a, axes)",
    "description": "Apply a function repeatedly over multiple axes."
  },
  "1436": {
    "name": "argsort",
    "module": "torch.storage.np.ma",
    "fullName": "torch.storage.np.ma.argsort",
    "signature": "(a, axis=<no value>, kind=None, order=None, endwith=True, fill_value=None)",
    "description": "Return an ndarray of indices that sort the array along the"
  },
  "1437": {
    "name": "array",
    "module": "torch.storage.np.ma",
    "fullName": "torch.storage.np.ma.array",
    "signature": "(data, dtype=None, copy=False, order=None, mask=False, fill_value=None, keep_mask=True, hard_mask=False, shrink=True, subok=True, ndmin=0)",
    "description": "An array class with possibly masked values."
  },
  "1438": {
    "name": "asanyarray",
    "module": "torch.storage.np.ma",
    "fullName": "torch.storage.np.ma.asanyarray",
    "signature": "(a, dtype=None)",
    "description": "Convert the input to a masked array, conserving subclasses."
  },
  "1439": {
    "name": "asarray",
    "module": "torch.storage.np.ma",
    "fullName": "torch.storage.np.ma.asarray",
    "signature": "(a, dtype=None, order=None)",
    "description": "Convert the input to a masked array of the given data-type."
  },
  "1440": {
    "name": "average",
    "module": "torch.storage.np.ma",
    "fullName": "torch.storage.np.ma.average",
    "signature": "(a, axis=None, weights=None, returned=False, *, keepdims=<no value>)",
    "description": "Return the weighted average of array over the given axis."
  },
  "1441": {
    "name": "choose",
    "module": "torch.storage.np.ma",
    "fullName": "torch.storage.np.ma.choose",
    "signature": "(indices, choices, out=None, mode='raise')",
    "description": "Use an index array to construct a new array from a list of choices."
  },
  "1442": {
    "name": "clump_masked",
    "module": "torch.storage.np.ma",
    "fullName": "torch.storage.np.ma.clump_masked",
    "signature": "(a)",
    "description": "Returns a list of slices corresponding to the masked clumps of a 1-D array."
  },
  "1443": {
    "name": "clump_unmasked",
    "module": "torch.storage.np.ma",
    "fullName": "torch.storage.np.ma.clump_unmasked",
    "signature": "(a)",
    "description": "Return list of slices corresponding to the unmasked clumps of a 1-D array."
  },
  "1444": {
    "name": "common_fill_value",
    "module": "torch.storage.np.ma",
    "fullName": "torch.storage.np.ma.common_fill_value",
    "signature": "(a, b)",
    "description": "Return the common filling value of two masked arrays, if any."
  },
  "1445": {
    "name": "compress_cols",
    "module": "torch.storage.np.ma",
    "fullName": "torch.storage.np.ma.compress_cols",
    "signature": "(a)",
    "description": "Suppress whole columns of a 2-D array that contain masked values."
  },
  "1446": {
    "name": "compress_nd",
    "module": "torch.storage.np.ma",
    "fullName": "torch.storage.np.ma.compress_nd",
    "signature": "(x, axis=None)",
    "description": "Suppress slices from multiple dimensions which contain masked values."
  },
  "1447": {
    "name": "compress_rowcols",
    "module": "torch.storage.np.ma",
    "fullName": "torch.storage.np.ma.compress_rowcols",
    "signature": "(x, axis=None)",
    "description": "Suppress the rows and/or columns of a 2-D array that contain"
  },
  "1448": {
    "name": "compress_rows",
    "module": "torch.storage.np.ma",
    "fullName": "torch.storage.np.ma.compress_rows",
    "signature": "(a)",
    "description": "Suppress whole rows of a 2-D array that contain masked values."
  },
  "1449": {
    "name": "compressed",
    "module": "torch.storage.np.ma",
    "fullName": "torch.storage.np.ma.compressed",
    "signature": "(x)",
    "description": "Return all the non-masked data as a 1-D array."
  },
  "1450": {
    "name": "concatenate",
    "module": "torch.storage.np.ma",
    "fullName": "torch.storage.np.ma.concatenate",
    "signature": "(arrays, axis=0)",
    "description": "Concatenate a sequence of arrays along the given axis."
  },
  "1451": {
    "name": "convolve",
    "module": "torch.storage.np.ma",
    "fullName": "torch.storage.np.ma.convolve",
    "signature": "(a, v, mode='full', propagate_mask=True)",
    "description": "Returns the discrete, linear convolution of two one-dimensional sequences."
  },
  "1452": {
    "name": "correlate",
    "module": "torch.storage.np.ma",
    "fullName": "torch.storage.np.ma.correlate",
    "signature": "(a, v, mode='valid', propagate_mask=True)",
    "description": "Cross-correlation of two 1-dimensional sequences."
  },
  "1453": {
    "name": "count_masked",
    "module": "torch.storage.np.ma",
    "fullName": "torch.storage.np.ma.count_masked",
    "signature": "(arr, axis=None)",
    "description": "Count the number of masked elements along the given axis."
  },
  "1454": {
    "name": "cov",
    "module": "torch.storage.np.ma",
    "fullName": "torch.storage.np.ma.cov",
    "signature": "(x, y=None, rowvar=True, bias=False, allow_masked=True, ddof=None)",
    "description": "Estimate the covariance matrix."
  },
  "1455": {
    "name": "default_fill_value",
    "module": "torch.storage.np.ma",
    "fullName": "torch.storage.np.ma.default_fill_value",
    "signature": "(obj)",
    "description": "Return the default fill value for the argument object."
  },
  "1456": {
    "name": "diag",
    "module": "torch.storage.np.ma",
    "fullName": "torch.storage.np.ma.diag",
    "signature": "(v, k=0)",
    "description": "Extract a diagonal or construct a diagonal array."
  },
  "1457": {
    "name": "dot",
    "module": "torch.storage.np.ma",
    "fullName": "torch.storage.np.ma.dot",
    "signature": "(a, b, strict=False, out=None)",
    "description": "Return the dot product of two arrays."
  },
  "1458": {
    "name": "ediff1d",
    "module": "torch.storage.np.ma",
    "fullName": "torch.storage.np.ma.ediff1d",
    "signature": "(arr, to_end=None, to_begin=None)",
    "description": "Compute the differences between consecutive elements of an array."
  },
  "1459": {
    "name": "filled",
    "module": "torch.storage.np.ma",
    "fullName": "torch.storage.np.ma.filled",
    "signature": "(a, fill_value=None)",
    "description": "Return input as an array with masked data replaced by a fill value."
  },
  "1460": {
    "name": "fix_invalid",
    "module": "torch.storage.np.ma",
    "fullName": "torch.storage.np.ma.fix_invalid",
    "signature": "(a, mask=False, copy=True, fill_value=None)",
    "description": "Return input with invalid data masked and replaced by a fill value."
  },
  "1461": {
    "name": "flatnotmasked_contiguous",
    "module": "torch.storage.np.ma",
    "fullName": "torch.storage.np.ma.flatnotmasked_contiguous",
    "signature": "(a)",
    "description": "Find contiguous unmasked data in a masked array."
  },
  "1462": {
    "name": "flatnotmasked_edges",
    "module": "torch.storage.np.ma",
    "fullName": "torch.storage.np.ma.flatnotmasked_edges",
    "signature": "(a)",
    "description": "Find the indices of the first and last unmasked values."
  },
  "1463": {
    "name": "flatten_mask",
    "module": "torch.storage.np.ma",
    "fullName": "torch.storage.np.ma.flatten_mask",
    "signature": "(mask)",
    "description": "Returns a completely flattened version of the mask, where nested fields"
  },
  "1464": {
    "name": "flatten_structured_array",
    "module": "torch.storage.np.ma",
    "fullName": "torch.storage.np.ma.flatten_structured_array",
    "signature": "(a)",
    "description": "Flatten a structured array."
  },
  "1465": {
    "name": "fromflex",
    "module": "torch.storage.np.ma",
    "fullName": "torch.storage.np.ma.fromflex",
    "signature": "(fxarray)",
    "description": "Build a masked array from a suitable flexible-type array."
  },
  "1466": {
    "name": "getdata",
    "module": "torch.storage.np.ma",
    "fullName": "torch.storage.np.ma.getdata",
    "signature": "(a, subok=True)",
    "description": "Return the data of a masked array as an ndarray."
  },
  "1467": {
    "name": "getmask",
    "module": "torch.storage.np.ma",
    "fullName": "torch.storage.np.ma.getmask",
    "signature": "(a)",
    "description": "Return the mask of a masked array, or nomask."
  },
  "1468": {
    "name": "getmaskarray",
    "module": "torch.storage.np.ma",
    "fullName": "torch.storage.np.ma.getmaskarray",
    "signature": "(arr)",
    "description": "Return the mask of a masked array, or full boolean array of False."
  },
  "1469": {
    "name": "in1d",
    "module": "torch.storage.np.ma",
    "fullName": "torch.storage.np.ma.in1d",
    "signature": "(ar1, ar2, assume_unique=False, invert=False)",
    "description": "Test whether each element of an array is also present in a second"
  },
  "1470": {
    "name": "inner",
    "module": "torch.storage.np.ma",
    "fullName": "torch.storage.np.ma.inner",
    "signature": "(a, b)",
    "description": "inner(a, b, /)"
  },
  "1471": {
    "name": "innerproduct",
    "module": "torch.storage.np.ma",
    "fullName": "torch.storage.np.ma.innerproduct",
    "signature": "(a, b)",
    "description": "inner(a, b, /)"
  },
  "1472": {
    "name": "intersect1d",
    "module": "torch.storage.np.ma",
    "fullName": "torch.storage.np.ma.intersect1d",
    "signature": "(ar1, ar2, assume_unique=False)",
    "description": "Returns the unique elements common to both arrays."
  },
  "1473": {
    "name": "isMA",
    "module": "torch.storage.np.ma",
    "fullName": "torch.storage.np.ma.isMA",
    "signature": "(x)",
    "description": "Test whether input is an instance of MaskedArray."
  },
  "1474": {
    "name": "isMaskedArray",
    "module": "torch.storage.np.ma",
    "fullName": "torch.storage.np.ma.isMaskedArray",
    "signature": "(x)",
    "description": "Test whether input is an instance of MaskedArray."
  },
  "1475": {
    "name": "is_mask",
    "module": "torch.storage.np.ma",
    "fullName": "torch.storage.np.ma.is_mask",
    "signature": "(m)",
    "description": "Return True if m is a valid, standard mask."
  },
  "1476": {
    "name": "is_masked",
    "module": "torch.storage.np.ma",
    "fullName": "torch.storage.np.ma.is_masked",
    "signature": "(x)",
    "description": "Determine whether input has masked values."
  },
  "1477": {
    "name": "isarray",
    "module": "torch.storage.np.ma",
    "fullName": "torch.storage.np.ma.isarray",
    "signature": "(x)",
    "description": "Test whether input is an instance of MaskedArray."
  },
  "1478": {
    "name": "isin",
    "module": "torch.storage.np.ma",
    "fullName": "torch.storage.np.ma.isin",
    "signature": "(element, test_elements, assume_unique=False, invert=False)",
    "description": "Calculates `element in test_elements`, broadcasting over"
  },
  "1479": {
    "name": "left_shift",
    "module": "torch.storage.np.ma",
    "fullName": "torch.storage.np.ma.left_shift",
    "signature": "(a, n)",
    "description": "Shift the bits of an integer to the left."
  },
  "1480": {
    "name": "make_mask",
    "module": "torch.storage.np.ma",
    "fullName": "torch.storage.np.ma.make_mask",
    "signature": "(m, copy=False, shrink=True, dtype=<class 'numpy.bool_'>)",
    "description": "Create a boolean mask from an array."
  },
  "1481": {
    "name": "make_mask_descr",
    "module": "torch.storage.np.ma",
    "fullName": "torch.storage.np.ma.make_mask_descr",
    "signature": "(ndtype)",
    "description": "Construct a dtype description list from a given dtype."
  },
  "1482": {
    "name": "make_mask_none",
    "module": "torch.storage.np.ma",
    "fullName": "torch.storage.np.ma.make_mask_none",
    "signature": "(newshape, dtype=None)",
    "description": "Return a boolean mask of the given shape, filled with False."
  },
  "1483": {
    "name": "mask_cols",
    "module": "torch.storage.np.ma",
    "fullName": "torch.storage.np.ma.mask_cols",
    "signature": "(a, axis=<no value>)",
    "description": "Mask columns of a 2D array that contain masked values."
  },
  "1484": {
    "name": "mask_or",
    "module": "torch.storage.np.ma",
    "fullName": "torch.storage.np.ma.mask_or",
    "signature": "(m1, m2, copy=False, shrink=True)",
    "description": "Combine two masks with the ``logical_or`` operator."
  },
  "1485": {
    "name": "mask_rowcols",
    "module": "torch.storage.np.ma",
    "fullName": "torch.storage.np.ma.mask_rowcols",
    "signature": "(a, axis=None)",
    "description": "Mask rows and/or columns of a 2D array that contain masked values."
  },
  "1486": {
    "name": "mask_rows",
    "module": "torch.storage.np.ma",
    "fullName": "torch.storage.np.ma.mask_rows",
    "signature": "(a, axis=<no value>)",
    "description": "Mask rows of a 2D array that contain masked values."
  },
  "1487": {
    "name": "masked_all",
    "module": "torch.storage.np.ma",
    "fullName": "torch.storage.np.ma.masked_all",
    "signature": "(shape, dtype=<class 'float'>)",
    "description": "Empty masked array with all elements masked."
  },
  "1488": {
    "name": "masked_all_like",
    "module": "torch.storage.np.ma",
    "fullName": "torch.storage.np.ma.masked_all_like",
    "signature": "(arr)",
    "description": "Empty masked array with the properties of an existing array."
  },
  "1489": {
    "name": "masked_equal",
    "module": "torch.storage.np.ma",
    "fullName": "torch.storage.np.ma.masked_equal",
    "signature": "(x, value, copy=True)",
    "description": "Mask an array where equal to a given value."
  },
  "1490": {
    "name": "masked_greater",
    "module": "torch.storage.np.ma",
    "fullName": "torch.storage.np.ma.masked_greater",
    "signature": "(x, value, copy=True)",
    "description": "Mask an array where greater than a given value."
  },
  "1491": {
    "name": "masked_greater_equal",
    "module": "torch.storage.np.ma",
    "fullName": "torch.storage.np.ma.masked_greater_equal",
    "signature": "(x, value, copy=True)",
    "description": "Mask an array where greater than or equal to a given value."
  },
  "1492": {
    "name": "masked_inside",
    "module": "torch.storage.np.ma",
    "fullName": "torch.storage.np.ma.masked_inside",
    "signature": "(x, v1, v2, copy=True)",
    "description": "Mask an array inside a given interval."
  },
  "1493": {
    "name": "masked_invalid",
    "module": "torch.storage.np.ma",
    "fullName": "torch.storage.np.ma.masked_invalid",
    "signature": "(a, copy=True)",
    "description": "Mask an array where invalid values occur (NaNs or infs)."
  },
  "1494": {
    "name": "masked_less",
    "module": "torch.storage.np.ma",
    "fullName": "torch.storage.np.ma.masked_less",
    "signature": "(x, value, copy=True)",
    "description": "Mask an array where less than a given value."
  },
  "1495": {
    "name": "masked_less_equal",
    "module": "torch.storage.np.ma",
    "fullName": "torch.storage.np.ma.masked_less_equal",
    "signature": "(x, value, copy=True)",
    "description": "Mask an array where less than or equal to a given value."
  },
  "1496": {
    "name": "masked_not_equal",
    "module": "torch.storage.np.ma",
    "fullName": "torch.storage.np.ma.masked_not_equal",
    "signature": "(x, value, copy=True)",
    "description": "Mask an array where `not` equal to a given value."
  },
  "1497": {
    "name": "masked_object",
    "module": "torch.storage.np.ma",
    "fullName": "torch.storage.np.ma.masked_object",
    "signature": "(x, value, copy=True, shrink=True)",
    "description": "Mask the array `x` where the data are exactly equal to value."
  },
  "1498": {
    "name": "masked_outside",
    "module": "torch.storage.np.ma",
    "fullName": "torch.storage.np.ma.masked_outside",
    "signature": "(x, v1, v2, copy=True)",
    "description": "Mask an array outside a given interval."
  },
  "1499": {
    "name": "masked_values",
    "module": "torch.storage.np.ma",
    "fullName": "torch.storage.np.ma.masked_values",
    "signature": "(x, value, rtol=1e-05, atol=1e-08, copy=True, shrink=True)",
    "description": "Mask using floating point equality."
  },
  "1500": {
    "name": "masked_where",
    "module": "torch.storage.np.ma",
    "fullName": "torch.storage.np.ma.masked_where",
    "signature": "(condition, a, copy=True)",
    "description": "Mask an array where a condition is met."
  },
  "1501": {
    "name": "max",
    "module": "torch.storage.np.ma",
    "fullName": "torch.storage.np.ma.max",
    "signature": "(obj, axis=None, out=None, fill_value=None, keepdims=<no value>)",
    "description": "Return the maximum along a given axis."
  },
  "1502": {
    "name": "maximum_fill_value",
    "module": "torch.storage.np.ma",
    "fullName": "torch.storage.np.ma.maximum_fill_value",
    "signature": "(obj)",
    "description": "Return the minimum value that can be represented by the dtype of an object."
  },
  "1503": {
    "name": "median",
    "module": "torch.storage.np.ma",
    "fullName": "torch.storage.np.ma.median",
    "signature": "(a, axis=None, out=None, overwrite_input=False, keepdims=False)",
    "description": "Compute the median along the specified axis."
  },
  "1504": {
    "name": "min",
    "module": "torch.storage.np.ma",
    "fullName": "torch.storage.np.ma.min",
    "signature": "(obj, axis=None, out=None, fill_value=None, keepdims=<no value>)",
    "description": "Return the minimum along a given axis."
  },
  "1505": {
    "name": "minimum_fill_value",
    "module": "torch.storage.np.ma",
    "fullName": "torch.storage.np.ma.minimum_fill_value",
    "signature": "(obj)",
    "description": "Return the maximum value that can be represented by the dtype of an object."
  },
  "1506": {
    "name": "ndenumerate",
    "module": "torch.storage.np.ma",
    "fullName": "torch.storage.np.ma.ndenumerate",
    "signature": "(a, compressed=True)",
    "description": "Multidimensional index iterator."
  },
  "1507": {
    "name": "ndim",
    "module": "torch.storage.np.ma",
    "fullName": "torch.storage.np.ma.ndim",
    "signature": "(obj)",
    "description": "Return the number of dimensions of an array."
  },
  "1508": {
    "name": "notmasked_contiguous",
    "module": "torch.storage.np.ma",
    "fullName": "torch.storage.np.ma.notmasked_contiguous",
    "signature": "(a, axis=None)",
    "description": "Find contiguous unmasked data in a masked array along the given axis."
  },
  "1509": {
    "name": "notmasked_edges",
    "module": "torch.storage.np.ma",
    "fullName": "torch.storage.np.ma.notmasked_edges",
    "signature": "(a, axis=None)",
    "description": "Find the indices of the first and last unmasked values along an axis."
  },
  "1510": {
    "name": "outer",
    "module": "torch.storage.np.ma",
    "fullName": "torch.storage.np.ma.outer",
    "signature": "(a, b)",
    "description": "Compute the outer product of two vectors."
  },
  "1511": {
    "name": "outerproduct",
    "module": "torch.storage.np.ma",
    "fullName": "torch.storage.np.ma.outerproduct",
    "signature": "(a, b)",
    "description": "Compute the outer product of two vectors."
  },
  "1512": {
    "name": "polyfit",
    "module": "torch.storage.np.ma",
    "fullName": "torch.storage.np.ma.polyfit",
    "signature": "(x, y, deg, rcond=None, full=False, w=None, cov=False)",
    "description": "Least squares polynomial fit."
  },
  "1513": {
    "name": "power",
    "module": "torch.storage.np.ma",
    "fullName": "torch.storage.np.ma.power",
    "signature": "(a, b, third=None)",
    "description": "Returns element-wise base array raised to power from second array."
  },
  "1514": {
    "name": "ptp",
    "module": "torch.storage.np.ma",
    "fullName": "torch.storage.np.ma.ptp",
    "signature": "(obj, axis=None, out=None, fill_value=None, keepdims=<no value>)",
    "description": "Return (maximum - minimum) along the given dimension"
  },
  "1515": {
    "name": "put",
    "module": "torch.storage.np.ma",
    "fullName": "torch.storage.np.ma.put",
    "signature": "(a, indices, values, mode='raise')",
    "description": "Set storage-indexed locations to corresponding values."
  },
  "1516": {
    "name": "putmask",
    "module": "torch.storage.np.ma",
    "fullName": "torch.storage.np.ma.putmask",
    "signature": "(a, mask, values)",
    "description": "Changes elements of an array based on conditional and input values."
  },
  "1517": {
    "name": "reshape",
    "module": "torch.storage.np.ma",
    "fullName": "torch.storage.np.ma.reshape",
    "signature": "(a, new_shape, order='C')",
    "description": "Returns an array containing the same data with a new shape."
  },
  "1518": {
    "name": "resize",
    "module": "torch.storage.np.ma",
    "fullName": "torch.storage.np.ma.resize",
    "signature": "(x, new_shape)",
    "description": "Return a new masked array with the specified size and shape."
  },
  "1519": {
    "name": "right_shift",
    "module": "torch.storage.np.ma",
    "fullName": "torch.storage.np.ma.right_shift",
    "signature": "(a, n)",
    "description": "Shift the bits of an integer to the right."
  },
  "1520": {
    "name": "round",
    "module": "torch.storage.np.ma",
    "fullName": "torch.storage.np.ma.round",
    "signature": "(a, decimals=0, out=None)",
    "description": "Return a copy of a, rounded to 'decimals' places."
  },
  "1521": {
    "name": "round_",
    "module": "torch.storage.np.ma",
    "fullName": "torch.storage.np.ma.round_",
    "signature": "(a, decimals=0, out=None)",
    "description": "Return a copy of a, rounded to 'decimals' places."
  },
  "1522": {
    "name": "set_fill_value",
    "module": "torch.storage.np.ma",
    "fullName": "torch.storage.np.ma.set_fill_value",
    "signature": "(a, fill_value)",
    "description": "Set the filling value of a, if a is a masked array."
  },
  "1523": {
    "name": "setdiff1d",
    "module": "torch.storage.np.ma",
    "fullName": "torch.storage.np.ma.setdiff1d",
    "signature": "(ar1, ar2, assume_unique=False)",
    "description": "Set difference of 1D arrays with unique elements."
  },
  "1524": {
    "name": "setxor1d",
    "module": "torch.storage.np.ma",
    "fullName": "torch.storage.np.ma.setxor1d",
    "signature": "(ar1, ar2, assume_unique=False)",
    "description": "Set exclusive-or of 1-D arrays with unique elements."
  },
  "1525": {
    "name": "shape",
    "module": "torch.storage.np.ma",
    "fullName": "torch.storage.np.ma.shape",
    "signature": "(obj)",
    "description": "Return the shape of an array."
  },
  "1526": {
    "name": "size",
    "module": "torch.storage.np.ma",
    "fullName": "torch.storage.np.ma.size",
    "signature": "(obj, axis=None)",
    "description": "Return the number of elements along a given axis."
  },
  "1527": {
    "name": "sort",
    "module": "torch.storage.np.ma",
    "fullName": "torch.storage.np.ma.sort",
    "signature": "(a, axis=-1, kind=None, order=None, endwith=True, fill_value=None)",
    "description": "Return a sorted copy of the masked array."
  },
  "1528": {
    "name": "take",
    "module": "torch.storage.np.ma",
    "fullName": "torch.storage.np.ma.take",
    "signature": "(a, indices, axis=None, out=None, mode='raise')",
    "description": "    "
  },
  "1529": {
    "name": "transpose",
    "module": "torch.storage.np.ma",
    "fullName": "torch.storage.np.ma.transpose",
    "signature": "(a, axes=None)",
    "description": "Permute the dimensions of an array."
  },
  "1530": {
    "name": "union1d",
    "module": "torch.storage.np.ma",
    "fullName": "torch.storage.np.ma.union1d",
    "signature": "(ar1, ar2)",
    "description": "Union of two arrays."
  },
  "1531": {
    "name": "unique",
    "module": "torch.storage.np.ma",
    "fullName": "torch.storage.np.ma.unique",
    "signature": "(ar1, return_index=False, return_inverse=False)",
    "description": "Finds the unique elements of an array."
  },
  "1532": {
    "name": "vander",
    "module": "torch.storage.np.ma",
    "fullName": "torch.storage.np.ma.vander",
    "signature": "(x, n=None)",
    "description": "Generate a Vandermonde matrix."
  },
  "1533": {
    "name": "where",
    "module": "torch.storage.np.ma",
    "fullName": "torch.storage.np.ma.where",
    "signature": "(condition, x=<no value>, y=<no value>)",
    "description": "Return a masked array with elements from `x` or `y`, depending on condition."
  },
  "1534": {
    "name": "apply_along_axis",
    "module": "torch.storage.np.ma.extras",
    "fullName": "torch.storage.np.ma.extras.apply_along_axis",
    "signature": "(func1d, axis, arr, *args, **kwargs)",
    "description": "Apply a function to 1-D slices along the given axis."
  },
  "1535": {
    "name": "apply_over_axes",
    "module": "torch.storage.np.ma.extras",
    "fullName": "torch.storage.np.ma.extras.apply_over_axes",
    "signature": "(func, a, axes)",
    "description": "Apply a function repeatedly over multiple axes."
  },
  "1536": {
    "name": "array",
    "module": "torch.storage.np.ma.extras",
    "fullName": "torch.storage.np.ma.extras.array",
    "signature": "(data, dtype=None, copy=False, order=None, mask=False, fill_value=None, keep_mask=True, hard_mask=False, shrink=True, subok=True, ndmin=0)",
    "description": "An array class with possibly masked values."
  },
  "1537": {
    "name": "asarray",
    "module": "torch.storage.np.ma.extras",
    "fullName": "torch.storage.np.ma.extras.asarray",
    "signature": "(a, dtype=None, order=None)",
    "description": "Convert the input to a masked array of the given data-type."
  },
  "1538": {
    "name": "average",
    "module": "torch.storage.np.ma.extras",
    "fullName": "torch.storage.np.ma.extras.average",
    "signature": "(a, axis=None, weights=None, returned=False, *, keepdims=<no value>)",
    "description": "Return the weighted average of array over the given axis."
  },
  "1539": {
    "name": "clump_masked",
    "module": "torch.storage.np.ma.extras",
    "fullName": "torch.storage.np.ma.extras.clump_masked",
    "signature": "(a)",
    "description": "Returns a list of slices corresponding to the masked clumps of a 1-D array."
  },
  "1540": {
    "name": "clump_unmasked",
    "module": "torch.storage.np.ma.extras",
    "fullName": "torch.storage.np.ma.extras.clump_unmasked",
    "signature": "(a)",
    "description": "Return list of slices corresponding to the unmasked clumps of a 1-D array."
  },
  "1541": {
    "name": "compress_cols",
    "module": "torch.storage.np.ma.extras",
    "fullName": "torch.storage.np.ma.extras.compress_cols",
    "signature": "(a)",
    "description": "Suppress whole columns of a 2-D array that contain masked values."
  },
  "1542": {
    "name": "compress_nd",
    "module": "torch.storage.np.ma.extras",
    "fullName": "torch.storage.np.ma.extras.compress_nd",
    "signature": "(x, axis=None)",
    "description": "Suppress slices from multiple dimensions which contain masked values."
  },
  "1543": {
    "name": "compress_rowcols",
    "module": "torch.storage.np.ma.extras",
    "fullName": "torch.storage.np.ma.extras.compress_rowcols",
    "signature": "(x, axis=None)",
    "description": "Suppress the rows and/or columns of a 2-D array that contain"
  },
  "1544": {
    "name": "compress_rows",
    "module": "torch.storage.np.ma.extras",
    "fullName": "torch.storage.np.ma.extras.compress_rows",
    "signature": "(a)",
    "description": "Suppress whole rows of a 2-D array that contain masked values."
  },
  "1545": {
    "name": "concatenate",
    "module": "torch.storage.np.ma.extras",
    "fullName": "torch.storage.np.ma.extras.concatenate",
    "signature": "(arrays, axis=0)",
    "description": "Concatenate a sequence of arrays along the given axis."
  },
  "1546": {
    "name": "count_masked",
    "module": "torch.storage.np.ma.extras",
    "fullName": "torch.storage.np.ma.extras.count_masked",
    "signature": "(arr, axis=None)",
    "description": "Count the number of masked elements along the given axis."
  },
  "1547": {
    "name": "cov",
    "module": "torch.storage.np.ma.extras",
    "fullName": "torch.storage.np.ma.extras.cov",
    "signature": "(x, y=None, rowvar=True, bias=False, allow_masked=True, ddof=None)",
    "description": "Estimate the covariance matrix."
  },
  "1548": {
    "name": "dot",
    "module": "torch.storage.np.ma.extras",
    "fullName": "torch.storage.np.ma.extras.dot",
    "signature": "(a, b, strict=False, out=None)",
    "description": "Return the dot product of two arrays."
  },
  "1549": {
    "name": "ediff1d",
    "module": "torch.storage.np.ma.extras",
    "fullName": "torch.storage.np.ma.extras.ediff1d",
    "signature": "(arr, to_end=None, to_begin=None)",
    "description": "Compute the differences between consecutive elements of an array."
  },
  "1550": {
    "name": "filled",
    "module": "torch.storage.np.ma.extras",
    "fullName": "torch.storage.np.ma.extras.filled",
    "signature": "(a, fill_value=None)",
    "description": "Return input as an array with masked data replaced by a fill value."
  },
  "1551": {
    "name": "flatnotmasked_contiguous",
    "module": "torch.storage.np.ma.extras",
    "fullName": "torch.storage.np.ma.extras.flatnotmasked_contiguous",
    "signature": "(a)",
    "description": "Find contiguous unmasked data in a masked array."
  },
  "1552": {
    "name": "flatnotmasked_edges",
    "module": "torch.storage.np.ma.extras",
    "fullName": "torch.storage.np.ma.extras.flatnotmasked_edges",
    "signature": "(a)",
    "description": "Find the indices of the first and last unmasked values."
  },
  "1553": {
    "name": "flatten_inplace",
    "module": "torch.storage.np.ma.extras",
    "fullName": "torch.storage.np.ma.extras.flatten_inplace",
    "signature": "(seq)",
    "description": "Flatten a sequence in place."
  },
  "1554": {
    "name": "get_masked_subclass",
    "module": "torch.storage.np.ma.extras",
    "fullName": "torch.storage.np.ma.extras.get_masked_subclass",
    "signature": "(*arrays)",
    "description": "Return the youngest subclass of MaskedArray from a list of (masked) arrays."
  },
  "1555": {
    "name": "getdata",
    "module": "torch.storage.np.ma.extras",
    "fullName": "torch.storage.np.ma.extras.getdata",
    "signature": "(a, subok=True)",
    "description": "Return the data of a masked array as an ndarray."
  },
  "1556": {
    "name": "getmask",
    "module": "torch.storage.np.ma.extras",
    "fullName": "torch.storage.np.ma.extras.getmask",
    "signature": "(a)",
    "description": "Return the mask of a masked array, or nomask."
  },
  "1557": {
    "name": "getmaskarray",
    "module": "torch.storage.np.ma.extras",
    "fullName": "torch.storage.np.ma.extras.getmaskarray",
    "signature": "(arr)",
    "description": "Return the mask of a masked array, or full boolean array of False."
  },
  "1558": {
    "name": "in1d",
    "module": "torch.storage.np.ma.extras",
    "fullName": "torch.storage.np.ma.extras.in1d",
    "signature": "(ar1, ar2, assume_unique=False, invert=False)",
    "description": "Test whether each element of an array is also present in a second"
  },
  "1559": {
    "name": "intersect1d",
    "module": "torch.storage.np.ma.extras",
    "fullName": "torch.storage.np.ma.extras.intersect1d",
    "signature": "(ar1, ar2, assume_unique=False)",
    "description": "Returns the unique elements common to both arrays."
  },
  "1560": {
    "name": "isin",
    "module": "torch.storage.np.ma.extras",
    "fullName": "torch.storage.np.ma.extras.isin",
    "signature": "(element, test_elements, assume_unique=False, invert=False)",
    "description": "Calculates `element in test_elements`, broadcasting over"
  },
  "1561": {
    "name": "issequence",
    "module": "torch.storage.np.ma.extras",
    "fullName": "torch.storage.np.ma.extras.issequence",
    "signature": "(seq)",
    "description": "Is seq a sequence (ndarray, list or tuple)?"
  },
  "1562": {
    "name": "make_mask_descr",
    "module": "torch.storage.np.ma.extras",
    "fullName": "torch.storage.np.ma.extras.make_mask_descr",
    "signature": "(ndtype)",
    "description": "Construct a dtype description list from a given dtype."
  },
  "1563": {
    "name": "mask_cols",
    "module": "torch.storage.np.ma.extras",
    "fullName": "torch.storage.np.ma.extras.mask_cols",
    "signature": "(a, axis=<no value>)",
    "description": "Mask columns of a 2D array that contain masked values."
  },
  "1564": {
    "name": "mask_or",
    "module": "torch.storage.np.ma.extras",
    "fullName": "torch.storage.np.ma.extras.mask_or",
    "signature": "(m1, m2, copy=False, shrink=True)",
    "description": "Combine two masks with the ``logical_or`` operator."
  },
  "1565": {
    "name": "mask_rowcols",
    "module": "torch.storage.np.ma.extras",
    "fullName": "torch.storage.np.ma.extras.mask_rowcols",
    "signature": "(a, axis=None)",
    "description": "Mask rows and/or columns of a 2D array that contain masked values."
  },
  "1566": {
    "name": "mask_rows",
    "module": "torch.storage.np.ma.extras",
    "fullName": "torch.storage.np.ma.extras.mask_rows",
    "signature": "(a, axis=<no value>)",
    "description": "Mask rows of a 2D array that contain masked values."
  },
  "1567": {
    "name": "masked_all",
    "module": "torch.storage.np.ma.extras",
    "fullName": "torch.storage.np.ma.extras.masked_all",
    "signature": "(shape, dtype=<class 'float'>)",
    "description": "Empty masked array with all elements masked."
  },
  "1568": {
    "name": "masked_all_like",
    "module": "torch.storage.np.ma.extras",
    "fullName": "torch.storage.np.ma.extras.masked_all_like",
    "signature": "(arr)",
    "description": "Empty masked array with the properties of an existing array."
  },
  "1569": {
    "name": "median",
    "module": "torch.storage.np.ma.extras",
    "fullName": "torch.storage.np.ma.extras.median",
    "signature": "(a, axis=None, out=None, overwrite_input=False, keepdims=False)",
    "description": "Compute the median along the specified axis."
  },
  "1570": {
    "name": "ndenumerate",
    "module": "torch.storage.np.ma.extras",
    "fullName": "torch.storage.np.ma.extras.ndenumerate",
    "signature": "(a, compressed=True)",
    "description": "Multidimensional index iterator."
  },
  "1571": {
    "name": "normalize_axis_tuple",
    "module": "torch.storage.np.ma.extras",
    "fullName": "torch.storage.np.ma.extras.normalize_axis_tuple",
    "signature": "(axis, ndim, argname=None, allow_duplicate=False)",
    "description": "Normalizes an axis argument into a tuple of non-negative integer axes."
  },
  "1572": {
    "name": "notmasked_contiguous",
    "module": "torch.storage.np.ma.extras",
    "fullName": "torch.storage.np.ma.extras.notmasked_contiguous",
    "signature": "(a, axis=None)",
    "description": "Find contiguous unmasked data in a masked array along the given axis."
  },
  "1573": {
    "name": "notmasked_edges",
    "module": "torch.storage.np.ma.extras",
    "fullName": "torch.storage.np.ma.extras.notmasked_edges",
    "signature": "(a, axis=None)",
    "description": "Find the indices of the first and last unmasked values along an axis."
  },
  "1574": {
    "name": "polyfit",
    "module": "torch.storage.np.ma.extras",
    "fullName": "torch.storage.np.ma.extras.polyfit",
    "signature": "(x, y, deg, rcond=None, full=False, w=None, cov=False)",
    "description": "Least squares polynomial fit."
  },
  "1575": {
    "name": "setdiff1d",
    "module": "torch.storage.np.ma.extras",
    "fullName": "torch.storage.np.ma.extras.setdiff1d",
    "signature": "(ar1, ar2, assume_unique=False)",
    "description": "Set difference of 1D arrays with unique elements."
  },
  "1576": {
    "name": "setxor1d",
    "module": "torch.storage.np.ma.extras",
    "fullName": "torch.storage.np.ma.extras.setxor1d",
    "signature": "(ar1, ar2, assume_unique=False)",
    "description": "Set exclusive-or of 1-D arrays with unique elements."
  },
  "1577": {
    "name": "sort",
    "module": "torch.storage.np.ma.extras",
    "fullName": "torch.storage.np.ma.extras.sort",
    "signature": "(a, axis=-1, kind=None, order=None, endwith=True, fill_value=None)",
    "description": "Return a sorted copy of the masked array."
  },
  "1578": {
    "name": "union1d",
    "module": "torch.storage.np.ma.extras",
    "fullName": "torch.storage.np.ma.extras.union1d",
    "signature": "(ar1, ar2)",
    "description": "Union of two arrays."
  },
  "1579": {
    "name": "unique",
    "module": "torch.storage.np.ma.extras",
    "fullName": "torch.storage.np.ma.extras.unique",
    "signature": "(ar1, return_index=False, return_inverse=False)",
    "description": "Finds the unique elements of an array."
  },
  "1580": {
    "name": "vander",
    "module": "torch.storage.np.ma.extras",
    "fullName": "torch.storage.np.ma.extras.vander",
    "signature": "(x, n=None)",
    "description": "Generate a Vandermonde matrix."
  },
  "1581": {
    "name": "allclose",
    "module": "torch.storage.np.ma.extras.ma",
    "fullName": "torch.storage.np.ma.extras.ma.allclose",
    "signature": "(a, b, masked_equal=True, rtol=1e-05, atol=1e-08)",
    "description": "Returns True if two arrays are element-wise equal within a tolerance."
  },
  "1582": {
    "name": "allequal",
    "module": "torch.storage.np.ma.extras.ma",
    "fullName": "torch.storage.np.ma.extras.ma.allequal",
    "signature": "(a, b, fill_value=True)",
    "description": "Return True if all entries of a and b are equal, using"
  },
  "1583": {
    "name": "amax",
    "module": "torch.storage.np.ma.extras.ma",
    "fullName": "torch.storage.np.ma.extras.ma.amax",
    "signature": "(a, axis=None, out=None, keepdims=<no value>, initial=<no value>, where=<no value>)",
    "description": "Return the maximum of an array or maximum along an axis."
  },
  "1584": {
    "name": "amin",
    "module": "torch.storage.np.ma.extras.ma",
    "fullName": "torch.storage.np.ma.extras.ma.amin",
    "signature": "(a, axis=None, out=None, keepdims=<no value>, initial=<no value>, where=<no value>)",
    "description": "Return the minimum of an array or minimum along an axis."
  },
  "1585": {
    "name": "append",
    "module": "torch.storage.np.ma.extras.ma",
    "fullName": "torch.storage.np.ma.extras.ma.append",
    "signature": "(a, b, axis=None)",
    "description": "Append values to the end of an array."
  },
  "1586": {
    "name": "argsort",
    "module": "torch.storage.np.ma.extras.ma",
    "fullName": "torch.storage.np.ma.extras.ma.argsort",
    "signature": "(a, axis=<no value>, kind=None, order=None, endwith=True, fill_value=None)",
    "description": "Return an ndarray of indices that sort the array along the"
  },
  "1587": {
    "name": "array",
    "module": "torch.storage.np.ma.extras.ma",
    "fullName": "torch.storage.np.ma.extras.ma.array",
    "signature": "(data, dtype=None, copy=False, order=None, mask=False, fill_value=None, keep_mask=True, hard_mask=False, shrink=True, subok=True, ndmin=0)",
    "description": "An array class with possibly masked values."
  },
  "1588": {
    "name": "asanyarray",
    "module": "torch.storage.np.ma.extras.ma",
    "fullName": "torch.storage.np.ma.extras.ma.asanyarray",
    "signature": "(a, dtype=None)",
    "description": "Convert the input to a masked array, conserving subclasses."
  },
  "1589": {
    "name": "asarray",
    "module": "torch.storage.np.ma.extras.ma",
    "fullName": "torch.storage.np.ma.extras.ma.asarray",
    "signature": "(a, dtype=None, order=None)",
    "description": "Convert the input to a masked array of the given data-type."
  },
  "1590": {
    "name": "choose",
    "module": "torch.storage.np.ma.extras.ma",
    "fullName": "torch.storage.np.ma.extras.ma.choose",
    "signature": "(indices, choices, out=None, mode='raise')",
    "description": "Use an index array to construct a new array from a list of choices."
  },
  "1591": {
    "name": "common_fill_value",
    "module": "torch.storage.np.ma.extras.ma",
    "fullName": "torch.storage.np.ma.extras.ma.common_fill_value",
    "signature": "(a, b)",
    "description": "Return the common filling value of two masked arrays, if any."
  },
  "1592": {
    "name": "compressed",
    "module": "torch.storage.np.ma.extras.ma",
    "fullName": "torch.storage.np.ma.extras.ma.compressed",
    "signature": "(x)",
    "description": "Return all the non-masked data as a 1-D array."
  },
  "1593": {
    "name": "concatenate",
    "module": "torch.storage.np.ma.extras.ma",
    "fullName": "torch.storage.np.ma.extras.ma.concatenate",
    "signature": "(arrays, axis=0)",
    "description": "Concatenate a sequence of arrays along the given axis."
  },
  "1594": {
    "name": "convolve",
    "module": "torch.storage.np.ma.extras.ma",
    "fullName": "torch.storage.np.ma.extras.ma.convolve",
    "signature": "(a, v, mode='full', propagate_mask=True)",
    "description": "Returns the discrete, linear convolution of two one-dimensional sequences."
  },
  "1595": {
    "name": "correlate",
    "module": "torch.storage.np.ma.extras.ma",
    "fullName": "torch.storage.np.ma.extras.ma.correlate",
    "signature": "(a, v, mode='valid', propagate_mask=True)",
    "description": "Cross-correlation of two 1-dimensional sequences."
  },
  "1596": {
    "name": "default_fill_value",
    "module": "torch.storage.np.ma.extras.ma",
    "fullName": "torch.storage.np.ma.extras.ma.default_fill_value",
    "signature": "(obj)",
    "description": "Return the default fill value for the argument object."
  },
  "1597": {
    "name": "diag",
    "module": "torch.storage.np.ma.extras.ma",
    "fullName": "torch.storage.np.ma.extras.ma.diag",
    "signature": "(v, k=0)",
    "description": "Extract a diagonal or construct a diagonal array."
  },
  "1598": {
    "name": "doc_note",
    "module": "torch.storage.np.ma.extras.ma",
    "fullName": "torch.storage.np.ma.extras.ma.doc_note",
    "signature": "(initialdoc, note)",
    "description": "Adds a Notes section to an existing docstring."
  },
  "1599": {
    "name": "dot",
    "module": "torch.storage.np.ma.extras.ma",
    "fullName": "torch.storage.np.ma.extras.ma.dot",
    "signature": "(a, b, strict=False, out=None)",
    "description": "Return the dot product of two arrays."
  },
  "1600": {
    "name": "filled",
    "module": "torch.storage.np.ma.extras.ma",
    "fullName": "torch.storage.np.ma.extras.ma.filled",
    "signature": "(a, fill_value=None)",
    "description": "Return input as an array with masked data replaced by a fill value."
  },
  "1601": {
    "name": "fix_invalid",
    "module": "torch.storage.np.ma.extras.ma",
    "fullName": "torch.storage.np.ma.extras.ma.fix_invalid",
    "signature": "(a, mask=False, copy=True, fill_value=None)",
    "description": "Return input with invalid data masked and replaced by a fill value."
  },
  "1602": {
    "name": "flatten_mask",
    "module": "torch.storage.np.ma.extras.ma",
    "fullName": "torch.storage.np.ma.extras.ma.flatten_mask",
    "signature": "(mask)",
    "description": "Returns a completely flattened version of the mask, where nested fields"
  },
  "1603": {
    "name": "flatten_structured_array",
    "module": "torch.storage.np.ma.extras.ma",
    "fullName": "torch.storage.np.ma.extras.ma.flatten_structured_array",
    "signature": "(a)",
    "description": "Flatten a structured array."
  },
  "1604": {
    "name": "formatargspec",
    "module": "torch.storage.np.ma.extras.ma",
    "fullName": "torch.storage.np.ma.extras.ma.formatargspec",
    "signature": "(args, varargs=None, varkw=None, defaults=None, formatarg=<class 'str'>, formatvarargs=<function <lambda> at 0x7fce232ebf70>, formatvarkw=<function <lambda> at 0x7fce232ebca0>, formatvalue=<function <lambda> at 0x7fce232ebdc0>, join=<function joinseq at 0x7fce232eb3a0>)",
    "description": "Format an argument spec from the 4 values returned by getargspec."
  },
  "1605": {
    "name": "fromfile",
    "module": "torch.storage.np.ma.extras.ma",
    "fullName": "torch.storage.np.ma.extras.ma.fromfile",
    "signature": "(file, dtype=<class 'float'>, count=-1, sep='')",
    "description": "No description available."
  },
  "1606": {
    "name": "fromflex",
    "module": "torch.storage.np.ma.extras.ma",
    "fullName": "torch.storage.np.ma.extras.ma.fromflex",
    "signature": "(fxarray)",
    "description": "Build a masked array from a suitable flexible-type array."
  },
  "1607": {
    "name": "get_data",
    "module": "torch.storage.np.ma.extras.ma",
    "fullName": "torch.storage.np.ma.extras.ma.get_data",
    "signature": "(a, subok=True)",
    "description": "Return the data of a masked array as an ndarray."
  },
  "1608": {
    "name": "get_fill_value",
    "module": "torch.storage.np.ma.extras.ma",
    "fullName": "torch.storage.np.ma.extras.ma.get_fill_value",
    "signature": "(a)",
    "description": "Return the filling value of a, if any.  Otherwise, returns the"
  },
  "1609": {
    "name": "get_mask",
    "module": "torch.storage.np.ma.extras.ma",
    "fullName": "torch.storage.np.ma.extras.ma.get_mask",
    "signature": "(a)",
    "description": "Return the mask of a masked array, or nomask."
  },
  "1610": {
    "name": "get_masked_subclass",
    "module": "torch.storage.np.ma.extras.ma",
    "fullName": "torch.storage.np.ma.extras.ma.get_masked_subclass",
    "signature": "(*arrays)",
    "description": "Return the youngest subclass of MaskedArray from a list of (masked) arrays."
  },
  "1611": {
    "name": "get_object_signature",
    "module": "torch.storage.np.ma.extras.ma",
    "fullName": "torch.storage.np.ma.extras.ma.get_object_signature",
    "signature": "(obj)",
    "description": "Get the signature from obj"
  },
  "1612": {
    "name": "getargspec",
    "module": "torch.storage.np.ma.extras.ma",
    "fullName": "torch.storage.np.ma.extras.ma.getargspec",
    "signature": "(func)",
    "description": "Get the names and default values of a function's arguments."
  },
  "1613": {
    "name": "getdata",
    "module": "torch.storage.np.ma.extras.ma",
    "fullName": "torch.storage.np.ma.extras.ma.getdata",
    "signature": "(a, subok=True)",
    "description": "Return the data of a masked array as an ndarray."
  },
  "1614": {
    "name": "getmask",
    "module": "torch.storage.np.ma.extras.ma",
    "fullName": "torch.storage.np.ma.extras.ma.getmask",
    "signature": "(a)",
    "description": "Return the mask of a masked array, or nomask."
  },
  "1615": {
    "name": "getmaskarray",
    "module": "torch.storage.np.ma.extras.ma",
    "fullName": "torch.storage.np.ma.extras.ma.getmaskarray",
    "signature": "(arr)",
    "description": "Return the mask of a masked array, or full boolean array of False."
  },
  "1616": {
    "name": "inner",
    "module": "torch.storage.np.ma.extras.ma",
    "fullName": "torch.storage.np.ma.extras.ma.inner",
    "signature": "(a, b)",
    "description": "inner(a, b, /)"
  },
  "1617": {
    "name": "innerproduct",
    "module": "torch.storage.np.ma.extras.ma",
    "fullName": "torch.storage.np.ma.extras.ma.innerproduct",
    "signature": "(a, b)",
    "description": "inner(a, b, /)"
  },
  "1618": {
    "name": "isMA",
    "module": "torch.storage.np.ma.extras.ma",
    "fullName": "torch.storage.np.ma.extras.ma.isMA",
    "signature": "(x)",
    "description": "Test whether input is an instance of MaskedArray."
  },
  "1619": {
    "name": "isMaskedArray",
    "module": "torch.storage.np.ma.extras.ma",
    "fullName": "torch.storage.np.ma.extras.ma.isMaskedArray",
    "signature": "(x)",
    "description": "Test whether input is an instance of MaskedArray."
  },
  "1620": {
    "name": "is_mask",
    "module": "torch.storage.np.ma.extras.ma",
    "fullName": "torch.storage.np.ma.extras.ma.is_mask",
    "signature": "(m)",
    "description": "Return True if m is a valid, standard mask."
  },
  "1621": {
    "name": "is_masked",
    "module": "torch.storage.np.ma.extras.ma",
    "fullName": "torch.storage.np.ma.extras.ma.is_masked",
    "signature": "(x)",
    "description": "Determine whether input has masked values."
  },
  "1622": {
    "name": "is_string_or_list_of_strings",
    "module": "torch.storage.np.ma.extras.ma",
    "fullName": "torch.storage.np.ma.extras.ma.is_string_or_list_of_strings",
    "signature": "(val)",
    "description": "No description available."
  },
  "1623": {
    "name": "isarray",
    "module": "torch.storage.np.ma.extras.ma",
    "fullName": "torch.storage.np.ma.extras.ma.isarray",
    "signature": "(x)",
    "description": "Test whether input is an instance of MaskedArray."
  },
  "1624": {
    "name": "iscomplexobj",
    "module": "torch.storage.np.ma.extras.ma",
    "fullName": "torch.storage.np.ma.extras.ma.iscomplexobj",
    "signature": "(x)",
    "description": "Check for a complex type or an array of complex numbers."
  },
  "1625": {
    "name": "left_shift",
    "module": "torch.storage.np.ma.extras.ma",
    "fullName": "torch.storage.np.ma.extras.ma.left_shift",
    "signature": "(a, n)",
    "description": "Shift the bits of an integer to the left."
  },
  "1626": {
    "name": "make_mask",
    "module": "torch.storage.np.ma.extras.ma",
    "fullName": "torch.storage.np.ma.extras.ma.make_mask",
    "signature": "(m, copy=False, shrink=True, dtype=<class 'numpy.bool_'>)",
    "description": "Create a boolean mask from an array."
  },
  "1627": {
    "name": "make_mask_descr",
    "module": "torch.storage.np.ma.extras.ma",
    "fullName": "torch.storage.np.ma.extras.ma.make_mask_descr",
    "signature": "(ndtype)",
    "description": "Construct a dtype description list from a given dtype."
  },
  "1628": {
    "name": "make_mask_none",
    "module": "torch.storage.np.ma.extras.ma",
    "fullName": "torch.storage.np.ma.extras.ma.make_mask_none",
    "signature": "(newshape, dtype=None)",
    "description": "Return a boolean mask of the given shape, filled with False."
  },
  "1629": {
    "name": "mask_or",
    "module": "torch.storage.np.ma.extras.ma",
    "fullName": "torch.storage.np.ma.extras.ma.mask_or",
    "signature": "(m1, m2, copy=False, shrink=True)",
    "description": "Combine two masks with the ``logical_or`` operator."
  },
  "1630": {
    "name": "mask_rowcols",
    "module": "torch.storage.np.ma.extras.ma",
    "fullName": "torch.storage.np.ma.extras.ma.mask_rowcols",
    "signature": "(a, axis=None)",
    "description": "Mask rows and/or columns of a 2D array that contain masked values."
  },
  "1631": {
    "name": "masked_equal",
    "module": "torch.storage.np.ma.extras.ma",
    "fullName": "torch.storage.np.ma.extras.ma.masked_equal",
    "signature": "(x, value, copy=True)",
    "description": "Mask an array where equal to a given value."
  },
  "1632": {
    "name": "masked_greater",
    "module": "torch.storage.np.ma.extras.ma",
    "fullName": "torch.storage.np.ma.extras.ma.masked_greater",
    "signature": "(x, value, copy=True)",
    "description": "Mask an array where greater than a given value."
  },
  "1633": {
    "name": "masked_greater_equal",
    "module": "torch.storage.np.ma.extras.ma",
    "fullName": "torch.storage.np.ma.extras.ma.masked_greater_equal",
    "signature": "(x, value, copy=True)",
    "description": "Mask an array where greater than or equal to a given value."
  },
  "1634": {
    "name": "masked_inside",
    "module": "torch.storage.np.ma.extras.ma",
    "fullName": "torch.storage.np.ma.extras.ma.masked_inside",
    "signature": "(x, v1, v2, copy=True)",
    "description": "Mask an array inside a given interval."
  },
  "1635": {
    "name": "masked_invalid",
    "module": "torch.storage.np.ma.extras.ma",
    "fullName": "torch.storage.np.ma.extras.ma.masked_invalid",
    "signature": "(a, copy=True)",
    "description": "Mask an array where invalid values occur (NaNs or infs)."
  },
  "1636": {
    "name": "masked_less",
    "module": "torch.storage.np.ma.extras.ma",
    "fullName": "torch.storage.np.ma.extras.ma.masked_less",
    "signature": "(x, value, copy=True)",
    "description": "Mask an array where less than a given value."
  },
  "1637": {
    "name": "masked_less_equal",
    "module": "torch.storage.np.ma.extras.ma",
    "fullName": "torch.storage.np.ma.extras.ma.masked_less_equal",
    "signature": "(x, value, copy=True)",
    "description": "Mask an array where less than or equal to a given value."
  },
  "1638": {
    "name": "masked_not_equal",
    "module": "torch.storage.np.ma.extras.ma",
    "fullName": "torch.storage.np.ma.extras.ma.masked_not_equal",
    "signature": "(x, value, copy=True)",
    "description": "Mask an array where `not` equal to a given value."
  },
  "1639": {
    "name": "masked_object",
    "module": "torch.storage.np.ma.extras.ma",
    "fullName": "torch.storage.np.ma.extras.ma.masked_object",
    "signature": "(x, value, copy=True, shrink=True)",
    "description": "Mask the array `x` where the data are exactly equal to value."
  },
  "1640": {
    "name": "masked_outside",
    "module": "torch.storage.np.ma.extras.ma",
    "fullName": "torch.storage.np.ma.extras.ma.masked_outside",
    "signature": "(x, v1, v2, copy=True)",
    "description": "Mask an array outside a given interval."
  },
  "1641": {
    "name": "masked_values",
    "module": "torch.storage.np.ma.extras.ma",
    "fullName": "torch.storage.np.ma.extras.ma.masked_values",
    "signature": "(x, value, rtol=1e-05, atol=1e-08, copy=True, shrink=True)",
    "description": "Mask using floating point equality."
  },
  "1642": {
    "name": "masked_where",
    "module": "torch.storage.np.ma.extras.ma",
    "fullName": "torch.storage.np.ma.extras.ma.masked_where",
    "signature": "(condition, a, copy=True)",
    "description": "Mask an array where a condition is met."
  },
  "1643": {
    "name": "max",
    "module": "torch.storage.np.ma.extras.ma",
    "fullName": "torch.storage.np.ma.extras.ma.max",
    "signature": "(obj, axis=None, out=None, fill_value=None, keepdims=<no value>)",
    "description": "Return the maximum along a given axis."
  },
  "1644": {
    "name": "maximum_fill_value",
    "module": "torch.storage.np.ma.extras.ma",
    "fullName": "torch.storage.np.ma.extras.ma.maximum_fill_value",
    "signature": "(obj)",
    "description": "Return the minimum value that can be represented by the dtype of an object."
  },
  "1645": {
    "name": "min",
    "module": "torch.storage.np.ma.extras.ma",
    "fullName": "torch.storage.np.ma.extras.ma.min",
    "signature": "(obj, axis=None, out=None, fill_value=None, keepdims=<no value>)",
    "description": "Return the minimum along a given axis."
  },
  "1646": {
    "name": "minimum_fill_value",
    "module": "torch.storage.np.ma.extras.ma",
    "fullName": "torch.storage.np.ma.extras.ma.minimum_fill_value",
    "signature": "(obj)",
    "description": "Return the maximum value that can be represented by the dtype of an object."
  },
  "1647": {
    "name": "ndim",
    "module": "torch.storage.np.ma.extras.ma",
    "fullName": "torch.storage.np.ma.extras.ma.ndim",
    "signature": "(obj)",
    "description": "Return the number of dimensions of an array."
  },
  "1648": {
    "name": "normalize_axis_tuple",
    "module": "torch.storage.np.ma.extras.ma",
    "fullName": "torch.storage.np.ma.extras.ma.normalize_axis_tuple",
    "signature": "(axis, ndim, argname=None, allow_duplicate=False)",
    "description": "Normalizes an axis argument into a tuple of non-negative integer axes."
  },
  "1649": {
    "name": "outer",
    "module": "torch.storage.np.ma.extras.ma",
    "fullName": "torch.storage.np.ma.extras.ma.outer",
    "signature": "(a, b)",
    "description": "Compute the outer product of two vectors."
  },
  "1650": {
    "name": "outerproduct",
    "module": "torch.storage.np.ma.extras.ma",
    "fullName": "torch.storage.np.ma.extras.ma.outerproduct",
    "signature": "(a, b)",
    "description": "Compute the outer product of two vectors."
  },
  "1651": {
    "name": "power",
    "module": "torch.storage.np.ma.extras.ma",
    "fullName": "torch.storage.np.ma.extras.ma.power",
    "signature": "(a, b, third=None)",
    "description": "Returns element-wise base array raised to power from second array."
  },
  "1652": {
    "name": "ptp",
    "module": "torch.storage.np.ma.extras.ma",
    "fullName": "torch.storage.np.ma.extras.ma.ptp",
    "signature": "(obj, axis=None, out=None, fill_value=None, keepdims=<no value>)",
    "description": "Return (maximum - minimum) along the given dimension"
  },
  "1653": {
    "name": "put",
    "module": "torch.storage.np.ma.extras.ma",
    "fullName": "torch.storage.np.ma.extras.ma.put",
    "signature": "(a, indices, values, mode='raise')",
    "description": "Set storage-indexed locations to corresponding values."
  },
  "1654": {
    "name": "putmask",
    "module": "torch.storage.np.ma.extras.ma",
    "fullName": "torch.storage.np.ma.extras.ma.putmask",
    "signature": "(a, mask, values)",
    "description": "Changes elements of an array based on conditional and input values."
  },
  "1655": {
    "name": "reshape",
    "module": "torch.storage.np.ma.extras.ma",
    "fullName": "torch.storage.np.ma.extras.ma.reshape",
    "signature": "(a, new_shape, order='C')",
    "description": "Returns an array containing the same data with a new shape."
  },
  "1656": {
    "name": "resize",
    "module": "torch.storage.np.ma.extras.ma",
    "fullName": "torch.storage.np.ma.extras.ma.resize",
    "signature": "(x, new_shape)",
    "description": "Return a new masked array with the specified size and shape."
  },
  "1657": {
    "name": "right_shift",
    "module": "torch.storage.np.ma.extras.ma",
    "fullName": "torch.storage.np.ma.extras.ma.right_shift",
    "signature": "(a, n)",
    "description": "Shift the bits of an integer to the right."
  },
  "1658": {
    "name": "round",
    "module": "torch.storage.np.ma.extras.ma",
    "fullName": "torch.storage.np.ma.extras.ma.round",
    "signature": "(a, decimals=0, out=None)",
    "description": "Return a copy of a, rounded to 'decimals' places."
  },
  "1659": {
    "name": "round_",
    "module": "torch.storage.np.ma.extras.ma",
    "fullName": "torch.storage.np.ma.extras.ma.round_",
    "signature": "(a, decimals=0, out=None)",
    "description": "Return a copy of a, rounded to 'decimals' places."
  },
  "1660": {
    "name": "set_fill_value",
    "module": "torch.storage.np.ma.extras.ma",
    "fullName": "torch.storage.np.ma.extras.ma.set_fill_value",
    "signature": "(a, fill_value)",
    "description": "Set the filling value of a, if a is a masked array."
  },
  "1661": {
    "name": "shape",
    "module": "torch.storage.np.ma.extras.ma",
    "fullName": "torch.storage.np.ma.extras.ma.shape",
    "signature": "(obj)",
    "description": "Return the shape of an array."
  },
  "1662": {
    "name": "size",
    "module": "torch.storage.np.ma.extras.ma",
    "fullName": "torch.storage.np.ma.extras.ma.size",
    "signature": "(obj, axis=None)",
    "description": "Return the number of elements along a given axis."
  },
  "1663": {
    "name": "sort",
    "module": "torch.storage.np.ma.extras.ma",
    "fullName": "torch.storage.np.ma.extras.ma.sort",
    "signature": "(a, axis=-1, kind=None, order=None, endwith=True, fill_value=None)",
    "description": "Return a sorted copy of the masked array."
  },
  "1664": {
    "name": "take",
    "module": "torch.storage.np.ma.extras.ma",
    "fullName": "torch.storage.np.ma.extras.ma.take",
    "signature": "(a, indices, axis=None, out=None, mode='raise')",
    "description": "    "
  },
  "1665": {
    "name": "transpose",
    "module": "torch.storage.np.ma.extras.ma",
    "fullName": "torch.storage.np.ma.extras.ma.transpose",
    "signature": "(a, axes=None)",
    "description": "Permute the dimensions of an array."
  },
  "1666": {
    "name": "where",
    "module": "torch.storage.np.ma.extras.ma",
    "fullName": "torch.storage.np.ma.extras.ma.where",
    "signature": "(condition, x=<no value>, y=<no value>)",
    "description": "Return a masked array with elements from `x` or `y`, depending on condition."
  },
  "1667": {
    "name": "add_newdoc",
    "module": "torch.storage.np.lib",
    "fullName": "torch.storage.np.lib.add_newdoc",
    "signature": "(place, obj, doc, warn_on_python=True)",
    "description": "Add documentation to an existing object, typically one defined in C"
  },
  "1668": {
    "name": "angle",
    "module": "torch.storage.np.lib",
    "fullName": "torch.storage.np.lib.angle",
    "signature": "(z, deg=False)",
    "description": "Return the angle of the complex argument."
  },
  "1669": {
    "name": "append",
    "module": "torch.storage.np.lib",
    "fullName": "torch.storage.np.lib.append",
    "signature": "(arr, values, axis=None)",
    "description": "Append values to the end of an array."
  },
  "1670": {
    "name": "apply_along_axis",
    "module": "torch.storage.np.lib",
    "fullName": "torch.storage.np.lib.apply_along_axis",
    "signature": "(func1d, axis, arr, *args, **kwargs)",
    "description": "Apply a function to 1-D slices along the given axis."
  },
  "1671": {
    "name": "apply_over_axes",
    "module": "torch.storage.np.lib",
    "fullName": "torch.storage.np.lib.apply_over_axes",
    "signature": "(func, a, axes)",
    "description": "Apply a function repeatedly over multiple axes."
  },
  "1672": {
    "name": "array_split",
    "module": "torch.storage.np.lib",
    "fullName": "torch.storage.np.lib.array_split",
    "signature": "(ary, indices_or_sections, axis=0)",
    "description": "Split an array into multiple sub-arrays."
  },
  "1673": {
    "name": "asarray_chkfinite",
    "module": "torch.storage.np.lib",
    "fullName": "torch.storage.np.lib.asarray_chkfinite",
    "signature": "(a, dtype=None, order=None)",
    "description": "Convert the input to an array, checking for NaNs or Infs."
  },
  "1674": {
    "name": "asfarray",
    "module": "torch.storage.np.lib",
    "fullName": "torch.storage.np.lib.asfarray",
    "signature": "(a, dtype=<class 'numpy.float64'>)",
    "description": "Return an array converted to a float type."
  },
  "1675": {
    "name": "average",
    "module": "torch.storage.np.lib",
    "fullName": "torch.storage.np.lib.average",
    "signature": "(a, axis=None, weights=None, returned=False, *, keepdims=<no value>)",
    "description": "Compute the weighted average along the specified axis."
  },
  "1676": {
    "name": "bartlett",
    "module": "torch.storage.np.lib",
    "fullName": "torch.storage.np.lib.bartlett",
    "signature": "(M)",
    "description": "Return the Bartlett window."
  },
  "1677": {
    "name": "bincount",
    "module": "torch.storage.np.lib",
    "fullName": "torch.storage.np.lib.bincount",
    "signature": "N/A",
    "description": "bincount(x, /, weights=None, minlength=0)"
  },
  "1678": {
    "name": "blackman",
    "module": "torch.storage.np.lib",
    "fullName": "torch.storage.np.lib.blackman",
    "signature": "(M)",
    "description": "Return the Blackman window."
  },
  "1679": {
    "name": "broadcast_shapes",
    "module": "torch.storage.np.lib",
    "fullName": "torch.storage.np.lib.broadcast_shapes",
    "signature": "(*args)",
    "description": "Broadcast the input shapes into a single shape."
  },
  "1680": {
    "name": "broadcast_to",
    "module": "torch.storage.np.lib",
    "fullName": "torch.storage.np.lib.broadcast_to",
    "signature": "(array, shape, subok=False)",
    "description": "Broadcast an array to a new shape."
  },
  "1681": {
    "name": "byte_bounds",
    "module": "torch.storage.np.lib",
    "fullName": "torch.storage.np.lib.byte_bounds",
    "signature": "(a)",
    "description": "Returns pointers to the end-points of an array."
  },
  "1682": {
    "name": "column_stack",
    "module": "torch.storage.np.lib",
    "fullName": "torch.storage.np.lib.column_stack",
    "signature": "(tup)",
    "description": "Stack 1-D arrays as columns into a 2-D array."
  },
  "1683": {
    "name": "common_type",
    "module": "torch.storage.np.lib",
    "fullName": "torch.storage.np.lib.common_type",
    "signature": "(*arrays)",
    "description": "Return a scalar type which is common to the input arrays."
  },
  "1684": {
    "name": "copy",
    "module": "torch.storage.np.lib",
    "fullName": "torch.storage.np.lib.copy",
    "signature": "(a, order='K', subok=False)",
    "description": "Return an array copy of the given object."
  },
  "1685": {
    "name": "cov",
    "module": "torch.storage.np.lib",
    "fullName": "torch.storage.np.lib.cov",
    "signature": "(m, y=None, rowvar=True, bias=False, ddof=None, fweights=None, aweights=None, *, dtype=None)",
    "description": "Estimate a covariance matrix, given data and weights."
  },
  "1686": {
    "name": "delete",
    "module": "torch.storage.np.lib",
    "fullName": "torch.storage.np.lib.delete",
    "signature": "(arr, obj, axis=None)",
    "description": "Return a new array with sub-arrays along an axis deleted. For a one"
  },
  "1687": {
    "name": "deprecate_with_doc",
    "module": "torch.storage.np.lib",
    "fullName": "torch.storage.np.lib.deprecate_with_doc",
    "signature": "(msg)",
    "description": "Deprecates a function and includes the deprecation in its docstring."
  },
  "1688": {
    "name": "diag",
    "module": "torch.storage.np.lib",
    "fullName": "torch.storage.np.lib.diag",
    "signature": "(v, k=0)",
    "description": "Extract a diagonal or construct a diagonal array."
  },
  "1689": {
    "name": "diag_indices",
    "module": "torch.storage.np.lib",
    "fullName": "torch.storage.np.lib.diag_indices",
    "signature": "(n, ndim=2)",
    "description": "Return the indices to access the main diagonal of an array."
  },
  "1690": {
    "name": "diag_indices_from",
    "module": "torch.storage.np.lib",
    "fullName": "torch.storage.np.lib.diag_indices_from",
    "signature": "(arr)",
    "description": "Return the indices to access the main diagonal of an n-dimensional array."
  },
  "1691": {
    "name": "diagflat",
    "module": "torch.storage.np.lib",
    "fullName": "torch.storage.np.lib.diagflat",
    "signature": "(v, k=0)",
    "description": "Create a two-dimensional array with the flattened input as a diagonal."
  },
  "1692": {
    "name": "diff",
    "module": "torch.storage.np.lib",
    "fullName": "torch.storage.np.lib.diff",
    "signature": "(a, n=1, axis=-1, prepend=<no value>, append=<no value>)",
    "description": "Calculate the n-th discrete difference along the given axis."
  },
  "1693": {
    "name": "digitize",
    "module": "torch.storage.np.lib",
    "fullName": "torch.storage.np.lib.digitize",
    "signature": "(x, bins, right=False)",
    "description": "Return the indices of the bins to which each value in input array belongs."
  },
  "1694": {
    "name": "disp",
    "module": "torch.storage.np.lib",
    "fullName": "torch.storage.np.lib.disp",
    "signature": "(mesg, device=None, linefeed=True)",
    "description": "Display a message on a device."
  },
  "1695": {
    "name": "dsplit",
    "module": "torch.storage.np.lib",
    "fullName": "torch.storage.np.lib.dsplit",
    "signature": "(ary, indices_or_sections)",
    "description": "Split array into multiple sub-arrays along the 3rd axis (depth)."
  },
  "1696": {
    "name": "dstack",
    "module": "torch.storage.np.lib",
    "fullName": "torch.storage.np.lib.dstack",
    "signature": "(tup)",
    "description": "Stack arrays in sequence depth wise (along third axis)."
  },
  "1697": {
    "name": "ediff1d",
    "module": "torch.storage.np.lib",
    "fullName": "torch.storage.np.lib.ediff1d",
    "signature": "(ary, to_end=None, to_begin=None)",
    "description": "The differences between consecutive elements of an array."
  },
  "1698": {
    "name": "extract",
    "module": "torch.storage.np.lib",
    "fullName": "torch.storage.np.lib.extract",
    "signature": "(condition, arr)",
    "description": "Return the elements of an array that satisfy some condition."
  },
  "1699": {
    "name": "eye",
    "module": "torch.storage.np.lib",
    "fullName": "torch.storage.np.lib.eye",
    "signature": "(N, M=None, k=0, dtype=<class 'float'>, order='C', *, like=None)",
    "description": "Return a 2-D array with ones on the diagonal and zeros elsewhere."
  },
  "1700": {
    "name": "fill_diagonal",
    "module": "torch.storage.np.lib",
    "fullName": "torch.storage.np.lib.fill_diagonal",
    "signature": "(a, val, wrap=False)",
    "description": "Fill the main diagonal of the given array of any dimensionality."
  },
  "1701": {
    "name": "fix",
    "module": "torch.storage.np.lib",
    "fullName": "torch.storage.np.lib.fix",
    "signature": "(x, out=None)",
    "description": "Round to nearest integer towards zero."
  },
  "1702": {
    "name": "flip",
    "module": "torch.storage.np.lib",
    "fullName": "torch.storage.np.lib.flip",
    "signature": "(m, axis=None)",
    "description": "Reverse the order of elements in an array along the given axis."
  },
  "1703": {
    "name": "fliplr",
    "module": "torch.storage.np.lib",
    "fullName": "torch.storage.np.lib.fliplr",
    "signature": "(m)",
    "description": "Reverse the order of elements along axis 1 (left/right)."
  },
  "1704": {
    "name": "flipud",
    "module": "torch.storage.np.lib",
    "fullName": "torch.storage.np.lib.flipud",
    "signature": "(m)",
    "description": "Reverse the order of elements along axis 0 (up/down)."
  },
  "1705": {
    "name": "fromregex",
    "module": "torch.storage.np.lib",
    "fullName": "torch.storage.np.lib.fromregex",
    "signature": "(file, regexp, dtype, encoding=None)",
    "description": "Construct an array from a text file, using regular expression parsing."
  },
  "1706": {
    "name": "genfromtxt",
    "module": "torch.storage.np.lib",
    "fullName": "torch.storage.np.lib.genfromtxt",
    "signature": "(fname, dtype=<class 'float'>, comments='#', delimiter=None, skip_header=0, skip_footer=0, converters=None, missing_values=None, filling_values=None, usecols=None, names=None, excludelist=None, deletechars=\" !#$%&'()*+,-./:;<=>?@[\\\\]^{|}~\", replace_space='_', autostrip=False, case_sensitive=True, defaultfmt='f%i', unpack=None, usemask=False, loose=True, invalid_raise=True, max_rows=None, encoding='bytes', *, ndmin=0, like=None)",
    "description": "Load data from a text file, with missing values handled as specified."
  },
  "1707": {
    "name": "get_array_wrap",
    "module": "torch.storage.np.lib",
    "fullName": "torch.storage.np.lib.get_array_wrap",
    "signature": "(*args)",
    "description": "Find the wrapper for the array with the highest priority."
  },
  "1708": {
    "name": "get_include",
    "module": "torch.storage.np.lib",
    "fullName": "torch.storage.np.lib.get_include",
    "signature": "()",
    "description": "Return the directory that contains the NumPy \\*.h header files."
  },
  "1709": {
    "name": "gradient",
    "module": "torch.storage.np.lib",
    "fullName": "torch.storage.np.lib.gradient",
    "signature": "(f, *varargs, axis=None, edge_order=1)",
    "description": "Return the gradient of an N-dimensional array."
  },
  "1710": {
    "name": "hamming",
    "module": "torch.storage.np.lib",
    "fullName": "torch.storage.np.lib.hamming",
    "signature": "(M)",
    "description": "Return the Hamming window."
  },
  "1711": {
    "name": "hanning",
    "module": "torch.storage.np.lib",
    "fullName": "torch.storage.np.lib.hanning",
    "signature": "(M)",
    "description": "Return the Hanning window."
  },
  "1712": {
    "name": "histogram",
    "module": "torch.storage.np.lib",
    "fullName": "torch.storage.np.lib.histogram",
    "signature": "(a, bins=10, range=None, density=None, weights=None)",
    "description": "Compute the histogram of a dataset."
  },
  "1713": {
    "name": "histogram2d",
    "module": "torch.storage.np.lib",
    "fullName": "torch.storage.np.lib.histogram2d",
    "signature": "(x, y, bins=10, range=None, density=None, weights=None)",
    "description": "Compute the bi-dimensional histogram of two data samples."
  },
  "1714": {
    "name": "histogram_bin_edges",
    "module": "torch.storage.np.lib",
    "fullName": "torch.storage.np.lib.histogram_bin_edges",
    "signature": "(a, bins=10, range=None, weights=None)",
    "description": "Function to calculate only the edges of the bins used by the `histogram`"
  },
  "1715": {
    "name": "histogramdd",
    "module": "torch.storage.np.lib",
    "fullName": "torch.storage.np.lib.histogramdd",
    "signature": "(sample, bins=10, range=None, density=None, weights=None)",
    "description": "Compute the multidimensional histogram of some data."
  },
  "1716": {
    "name": "hsplit",
    "module": "torch.storage.np.lib",
    "fullName": "torch.storage.np.lib.hsplit",
    "signature": "(ary, indices_or_sections)",
    "description": "Split an array into multiple sub-arrays horizontally (column-wise)."
  },
  "1717": {
    "name": "i0",
    "module": "torch.storage.np.lib",
    "fullName": "torch.storage.np.lib.i0",
    "signature": "(x)",
    "description": "Modified Bessel function of the first kind, order 0."
  },
  "1718": {
    "name": "imag",
    "module": "torch.storage.np.lib",
    "fullName": "torch.storage.np.lib.imag",
    "signature": "(val)",
    "description": "Return the imaginary part of the complex argument."
  },
  "1719": {
    "name": "in1d",
    "module": "torch.storage.np.lib",
    "fullName": "torch.storage.np.lib.in1d",
    "signature": "(ar1, ar2, assume_unique=False, invert=False, *, kind=None)",
    "description": "Test whether each element of a 1-D array is also present in a second array."
  },
  "1720": {
    "name": "info",
    "module": "torch.storage.np.lib",
    "fullName": "torch.storage.np.lib.info",
    "signature": "(object=None, maxwidth=76, output=None, toplevel='numpy')",
    "description": "Get help information for a function, class, or module."
  },
  "1721": {
    "name": "insert",
    "module": "torch.storage.np.lib",
    "fullName": "torch.storage.np.lib.insert",
    "signature": "(arr, obj, values, axis=None)",
    "description": "Insert values along the given axis before the given indices."
  },
  "1722": {
    "name": "interp",
    "module": "torch.storage.np.lib",
    "fullName": "torch.storage.np.lib.interp",
    "signature": "(x, xp, fp, left=None, right=None, period=None)",
    "description": "One-dimensional linear interpolation for monotonically increasing sample points."
  },
  "1723": {
    "name": "intersect1d",
    "module": "torch.storage.np.lib",
    "fullName": "torch.storage.np.lib.intersect1d",
    "signature": "(ar1, ar2, assume_unique=False, return_indices=False)",
    "description": "Find the intersection of two arrays."
  },
  "1724": {
    "name": "iscomplex",
    "module": "torch.storage.np.lib",
    "fullName": "torch.storage.np.lib.iscomplex",
    "signature": "(x)",
    "description": "Returns a bool array, where True if input element is complex."
  },
  "1725": {
    "name": "iscomplexobj",
    "module": "torch.storage.np.lib",
    "fullName": "torch.storage.np.lib.iscomplexobj",
    "signature": "(x)",
    "description": "Check for a complex type or an array of complex numbers."
  },
  "1726": {
    "name": "isin",
    "module": "torch.storage.np.lib",
    "fullName": "torch.storage.np.lib.isin",
    "signature": "(element, test_elements, assume_unique=False, invert=False, *, kind=None)",
    "description": "Calculates ``element in test_elements``, broadcasting over `element` only."
  },
  "1727": {
    "name": "isneginf",
    "module": "torch.storage.np.lib",
    "fullName": "torch.storage.np.lib.isneginf",
    "signature": "(x, out=None)",
    "description": "Test element-wise for negative infinity, return result as bool array."
  },
  "1728": {
    "name": "isposinf",
    "module": "torch.storage.np.lib",
    "fullName": "torch.storage.np.lib.isposinf",
    "signature": "(x, out=None)",
    "description": "Test element-wise for positive infinity, return result as bool array."
  },
  "1729": {
    "name": "isreal",
    "module": "torch.storage.np.lib",
    "fullName": "torch.storage.np.lib.isreal",
    "signature": "(x)",
    "description": "Returns a bool array, where True if input element is real."
  },
  "1730": {
    "name": "isrealobj",
    "module": "torch.storage.np.lib",
    "fullName": "torch.storage.np.lib.isrealobj",
    "signature": "(x)",
    "description": "Return True if x is a not complex type or an array of complex numbers."
  },
  "1731": {
    "name": "issubclass_",
    "module": "torch.storage.np.lib",
    "fullName": "torch.storage.np.lib.issubclass_",
    "signature": "(arg1, arg2)",
    "description": "Determine if a class is a subclass of a second class."
  },
  "1732": {
    "name": "issubdtype",
    "module": "torch.storage.np.lib",
    "fullName": "torch.storage.np.lib.issubdtype",
    "signature": "(arg1, arg2)",
    "description": "Returns True if first argument is a typecode lower/equal in type hierarchy."
  },
  "1733": {
    "name": "issubsctype",
    "module": "torch.storage.np.lib",
    "fullName": "torch.storage.np.lib.issubsctype",
    "signature": "(arg1, arg2)",
    "description": "Determine if the first argument is a subclass of the second argument."
  },
  "1734": {
    "name": "iterable",
    "module": "torch.storage.np.lib",
    "fullName": "torch.storage.np.lib.iterable",
    "signature": "(y)",
    "description": "Check whether or not an object can be iterated over."
  },
  "1735": {
    "name": "ix_",
    "module": "torch.storage.np.lib",
    "fullName": "torch.storage.np.lib.ix_",
    "signature": "(*args)",
    "description": "Construct an open mesh from multiple sequences."
  },
  "1736": {
    "name": "kaiser",
    "module": "torch.storage.np.lib",
    "fullName": "torch.storage.np.lib.kaiser",
    "signature": "(M, beta)",
    "description": "Return the Kaiser window."
  },
  "1737": {
    "name": "kron",
    "module": "torch.storage.np.lib",
    "fullName": "torch.storage.np.lib.kron",
    "signature": "(a, b)",
    "description": "Kronecker product of two arrays."
  },
  "1738": {
    "name": "load",
    "module": "torch.storage.np.lib",
    "fullName": "torch.storage.np.lib.load",
    "signature": "(file, mmap_mode=None, allow_pickle=False, fix_imports=True, encoding='ASCII', *, max_header_size=10000)",
    "description": "Load arrays or pickled objects from ``.npy``, ``.npz`` or pickled files."
  },
  "1739": {
    "name": "loadtxt",
    "module": "torch.storage.np.lib",
    "fullName": "torch.storage.np.lib.loadtxt",
    "signature": "(fname, dtype=<class 'float'>, comments='#', delimiter=None, converters=None, skiprows=0, usecols=None, unpack=False, ndmin=0, encoding='bytes', max_rows=None, *, quotechar=None, like=None)",
    "description": "Load data from a text file."
  },
  "1740": {
    "name": "lookfor",
    "module": "torch.storage.np.lib",
    "fullName": "torch.storage.np.lib.lookfor",
    "signature": "(what, module=None, import_modules=True, regenerate=False, output=None)",
    "description": "Do a keyword search on docstrings."
  },
  "1741": {
    "name": "mask_indices",
    "module": "torch.storage.np.lib",
    "fullName": "torch.storage.np.lib.mask_indices",
    "signature": "(n, mask_func, k=0)",
    "description": "Return the indices to access (n, n) arrays, given a masking function."
  },
  "1742": {
    "name": "median",
    "module": "torch.storage.np.lib",
    "fullName": "torch.storage.np.lib.median",
    "signature": "(a, axis=None, out=None, overwrite_input=False, keepdims=False)",
    "description": "Compute the median along the specified axis."
  },
  "1743": {
    "name": "meshgrid",
    "module": "torch.storage.np.lib",
    "fullName": "torch.storage.np.lib.meshgrid",
    "signature": "(*xi, copy=True, sparse=False, indexing='xy')",
    "description": "Return coordinate matrices from coordinate vectors."
  },
  "1744": {
    "name": "mintypecode",
    "module": "torch.storage.np.lib",
    "fullName": "torch.storage.np.lib.mintypecode",
    "signature": "(typechars, typeset='GDFgdf', default='d')",
    "description": "Return the character for the minimum-size type to which given types can"
  },
  "1745": {
    "name": "nan_to_num",
    "module": "torch.storage.np.lib",
    "fullName": "torch.storage.np.lib.nan_to_num",
    "signature": "(x, copy=True, nan=0.0, posinf=None, neginf=None)",
    "description": "Replace NaN with zero and infinity with large finite numbers (default"
  },
  "1746": {
    "name": "nanargmax",
    "module": "torch.storage.np.lib",
    "fullName": "torch.storage.np.lib.nanargmax",
    "signature": "(a, axis=None, out=None, *, keepdims=<no value>)",
    "description": "Return the indices of the maximum values in the specified axis ignoring"
  },
  "1747": {
    "name": "nanargmin",
    "module": "torch.storage.np.lib",
    "fullName": "torch.storage.np.lib.nanargmin",
    "signature": "(a, axis=None, out=None, *, keepdims=<no value>)",
    "description": "Return the indices of the minimum values in the specified axis ignoring"
  },
  "1748": {
    "name": "nancumprod",
    "module": "torch.storage.np.lib",
    "fullName": "torch.storage.np.lib.nancumprod",
    "signature": "(a, axis=None, dtype=None, out=None)",
    "description": "Return the cumulative product of array elements over a given axis treating Not a"
  },
  "1749": {
    "name": "nancumsum",
    "module": "torch.storage.np.lib",
    "fullName": "torch.storage.np.lib.nancumsum",
    "signature": "(a, axis=None, dtype=None, out=None)",
    "description": "Return the cumulative sum of array elements over a given axis treating Not a"
  },
  "1750": {
    "name": "nanmax",
    "module": "torch.storage.np.lib",
    "fullName": "torch.storage.np.lib.nanmax",
    "signature": "(a, axis=None, out=None, keepdims=<no value>, initial=<no value>, where=<no value>)",
    "description": "Return the maximum of an array or maximum along an axis, ignoring any"
  },
  "1751": {
    "name": "nanmean",
    "module": "torch.storage.np.lib",
    "fullName": "torch.storage.np.lib.nanmean",
    "signature": "(a, axis=None, dtype=None, out=None, keepdims=<no value>, *, where=<no value>)",
    "description": "Compute the arithmetic mean along the specified axis, ignoring NaNs."
  },
  "1752": {
    "name": "nanmedian",
    "module": "torch.storage.np.lib",
    "fullName": "torch.storage.np.lib.nanmedian",
    "signature": "(a, axis=None, out=None, overwrite_input=False, keepdims=<no value>)",
    "description": "Compute the median along the specified axis, while ignoring NaNs."
  },
  "1753": {
    "name": "nanmin",
    "module": "torch.storage.np.lib",
    "fullName": "torch.storage.np.lib.nanmin",
    "signature": "(a, axis=None, out=None, keepdims=<no value>, initial=<no value>, where=<no value>)",
    "description": "Return minimum of an array or minimum along an axis, ignoring any NaNs."
  },
  "1754": {
    "name": "nanprod",
    "module": "torch.storage.np.lib",
    "fullName": "torch.storage.np.lib.nanprod",
    "signature": "(a, axis=None, dtype=None, out=None, keepdims=<no value>, initial=<no value>, where=<no value>)",
    "description": "Return the product of array elements over a given axis treating Not a"
  },
  "1755": {
    "name": "nanstd",
    "module": "torch.storage.np.lib",
    "fullName": "torch.storage.np.lib.nanstd",
    "signature": "(a, axis=None, dtype=None, out=None, ddof=0, keepdims=<no value>, *, where=<no value>)",
    "description": "Compute the standard deviation along the specified axis, while"
  },
  "1756": {
    "name": "nansum",
    "module": "torch.storage.np.lib",
    "fullName": "torch.storage.np.lib.nansum",
    "signature": "(a, axis=None, dtype=None, out=None, keepdims=<no value>, initial=<no value>, where=<no value>)",
    "description": "Return the sum of array elements over a given axis treating Not a"
  },
  "1757": {
    "name": "nanvar",
    "module": "torch.storage.np.lib",
    "fullName": "torch.storage.np.lib.nanvar",
    "signature": "(a, axis=None, dtype=None, out=None, ddof=0, keepdims=<no value>, *, where=<no value>)",
    "description": "Compute the variance along the specified axis, while ignoring NaNs."
  },
  "1758": {
    "name": "packbits",
    "module": "torch.storage.np.lib",
    "fullName": "torch.storage.np.lib.packbits",
    "signature": "N/A",
    "description": "packbits(a, /, axis=None, bitorder='big')"
  },
  "1759": {
    "name": "pad",
    "module": "torch.storage.np.lib",
    "fullName": "torch.storage.np.lib.pad",
    "signature": "(array, pad_width, mode='constant', **kwargs)",
    "description": "Pad an array."
  },
  "1760": {
    "name": "piecewise",
    "module": "torch.storage.np.lib",
    "fullName": "torch.storage.np.lib.piecewise",
    "signature": "(x, condlist, funclist, *args, **kw)",
    "description": "Evaluate a piecewise-defined function."
  },
  "1761": {
    "name": "place",
    "module": "torch.storage.np.lib",
    "fullName": "torch.storage.np.lib.place",
    "signature": "(arr, mask, vals)",
    "description": "Change elements of an array based on conditional and input values."
  },
  "1762": {
    "name": "poly",
    "module": "torch.storage.np.lib",
    "fullName": "torch.storage.np.lib.poly",
    "signature": "(seq_of_zeros)",
    "description": "Find the coefficients of a polynomial with the given sequence of roots."
  },
  "1763": {
    "name": "polyadd",
    "module": "torch.storage.np.lib",
    "fullName": "torch.storage.np.lib.polyadd",
    "signature": "(a1, a2)",
    "description": "Find the sum of two polynomials."
  },
  "1764": {
    "name": "polyder",
    "module": "torch.storage.np.lib",
    "fullName": "torch.storage.np.lib.polyder",
    "signature": "(p, m=1)",
    "description": "Return the derivative of the specified order of a polynomial."
  },
  "1765": {
    "name": "polydiv",
    "module": "torch.storage.np.lib",
    "fullName": "torch.storage.np.lib.polydiv",
    "signature": "(u, v)",
    "description": "Returns the quotient and remainder of polynomial division."
  },
  "1766": {
    "name": "polyfit",
    "module": "torch.storage.np.lib",
    "fullName": "torch.storage.np.lib.polyfit",
    "signature": "(x, y, deg, rcond=None, full=False, w=None, cov=False)",
    "description": "Least squares polynomial fit."
  },
  "1767": {
    "name": "polyint",
    "module": "torch.storage.np.lib",
    "fullName": "torch.storage.np.lib.polyint",
    "signature": "(p, m=1, k=None)",
    "description": "Return an antiderivative (indefinite integral) of a polynomial."
  },
  "1768": {
    "name": "polymul",
    "module": "torch.storage.np.lib",
    "fullName": "torch.storage.np.lib.polymul",
    "signature": "(a1, a2)",
    "description": "Find the product of two polynomials."
  },
  "1769": {
    "name": "polysub",
    "module": "torch.storage.np.lib",
    "fullName": "torch.storage.np.lib.polysub",
    "signature": "(a1, a2)",
    "description": "Difference (subtraction) of two polynomials."
  },
  "1770": {
    "name": "polyval",
    "module": "torch.storage.np.lib",
    "fullName": "torch.storage.np.lib.polyval",
    "signature": "(p, x)",
    "description": "Evaluate a polynomial at specific values."
  },
  "1771": {
    "name": "put_along_axis",
    "module": "torch.storage.np.lib",
    "fullName": "torch.storage.np.lib.put_along_axis",
    "signature": "(arr, indices, values, axis)",
    "description": "Put values into the destination array by matching 1d index and data slices."
  },
  "1772": {
    "name": "ravel_multi_index",
    "module": "torch.storage.np.lib",
    "fullName": "torch.storage.np.lib.ravel_multi_index",
    "signature": "N/A",
    "description": "ravel_multi_index(multi_index, dims, mode='raise', order='C')"
  },
  "1773": {
    "name": "real",
    "module": "torch.storage.np.lib",
    "fullName": "torch.storage.np.lib.real",
    "signature": "(val)",
    "description": "Return the real part of the complex argument."
  },
  "1774": {
    "name": "real_if_close",
    "module": "torch.storage.np.lib",
    "fullName": "torch.storage.np.lib.real_if_close",
    "signature": "(a, tol=100)",
    "description": "If input is complex with all imaginary parts close to zero, return"
  },
  "1775": {
    "name": "recfromcsv",
    "module": "torch.storage.np.lib",
    "fullName": "torch.storage.np.lib.recfromcsv",
    "signature": "(fname, **kwargs)",
    "description": "Load ASCII data stored in a comma-separated file."
  },
  "1776": {
    "name": "recfromtxt",
    "module": "torch.storage.np.lib",
    "fullName": "torch.storage.np.lib.recfromtxt",
    "signature": "(fname, **kwargs)",
    "description": "Load ASCII data from a file and return it in a record array."
  },
  "1777": {
    "name": "roots",
    "module": "torch.storage.np.lib",
    "fullName": "torch.storage.np.lib.roots",
    "signature": "(p)",
    "description": "Return the roots of a polynomial with coefficients given in p."
  },
  "1778": {
    "name": "rot90",
    "module": "torch.storage.np.lib",
    "fullName": "torch.storage.np.lib.rot90",
    "signature": "(m, k=1, axes=(0, 1))",
    "description": "Rotate an array by 90 degrees in the plane specified by axes."
  },
  "1779": {
    "name": "row_stack",
    "module": "torch.storage.np.lib",
    "fullName": "torch.storage.np.lib.row_stack",
    "signature": "(tup, *, dtype=None, casting='same_kind')",
    "description": "Stack arrays in sequence vertically (row wise)."
  },
  "1780": {
    "name": "safe_eval",
    "module": "torch.storage.np.lib",
    "fullName": "torch.storage.np.lib.safe_eval",
    "signature": "(source)",
    "description": "Protected string evaluation."
  },
  "1781": {
    "name": "save",
    "module": "torch.storage.np.lib",
    "fullName": "torch.storage.np.lib.save",
    "signature": "(file, arr, allow_pickle=True, fix_imports=True)",
    "description": "Save an array to a binary file in NumPy ``.npy`` format."
  },
  "1782": {
    "name": "savetxt",
    "module": "torch.storage.np.lib",
    "fullName": "torch.storage.np.lib.savetxt",
    "signature": "(fname, X, fmt='%.18e', delimiter=' ', newline='\\n', header='', footer='', comments='# ', encoding=None)",
    "description": "Save an array to a text file."
  },
  "1783": {
    "name": "savez",
    "module": "torch.storage.np.lib",
    "fullName": "torch.storage.np.lib.savez",
    "signature": "(file, *args, **kwds)",
    "description": "Save several arrays into a single file in uncompressed ``.npz`` format."
  },
  "1784": {
    "name": "savez_compressed",
    "module": "torch.storage.np.lib",
    "fullName": "torch.storage.np.lib.savez_compressed",
    "signature": "(file, *args, **kwds)",
    "description": "Save several arrays into a single file in compressed ``.npz`` format."
  },
  "1785": {
    "name": "select",
    "module": "torch.storage.np.lib",
    "fullName": "torch.storage.np.lib.select",
    "signature": "(condlist, choicelist, default=0)",
    "description": "Return an array drawn from elements in choicelist, depending on conditions."
  },
  "1786": {
    "name": "setdiff1d",
    "module": "torch.storage.np.lib",
    "fullName": "torch.storage.np.lib.setdiff1d",
    "signature": "(ar1, ar2, assume_unique=False)",
    "description": "Find the set difference of two arrays."
  },
  "1787": {
    "name": "setxor1d",
    "module": "torch.storage.np.lib",
    "fullName": "torch.storage.np.lib.setxor1d",
    "signature": "(ar1, ar2, assume_unique=False)",
    "description": "Find the set exclusive-or of two arrays."
  },
  "1788": {
    "name": "show_runtime",
    "module": "torch.storage.np.lib",
    "fullName": "torch.storage.np.lib.show_runtime",
    "signature": "()",
    "description": "Print information about various resources in the system"
  },
  "1789": {
    "name": "sinc",
    "module": "torch.storage.np.lib",
    "fullName": "torch.storage.np.lib.sinc",
    "signature": "(x)",
    "description": "Return the normalized sinc function."
  },
  "1790": {
    "name": "sort_complex",
    "module": "torch.storage.np.lib",
    "fullName": "torch.storage.np.lib.sort_complex",
    "signature": "(a)",
    "description": "Sort a complex array using the real part first, then the imaginary part."
  },
  "1791": {
    "name": "source",
    "module": "torch.storage.np.lib",
    "fullName": "torch.storage.np.lib.source",
    "signature": "(object, output=<_io.TextIOWrapper name='<stdout>' mode='w' encoding='utf-8'>)",
    "description": "Print or write to a file the source code for a NumPy object."
  },
  "1792": {
    "name": "split",
    "module": "torch.storage.np.lib",
    "fullName": "torch.storage.np.lib.split",
    "signature": "(ary, indices_or_sections, axis=0)",
    "description": "Split an array into multiple sub-arrays as views into `ary`."
  },
  "1793": {
    "name": "take_along_axis",
    "module": "torch.storage.np.lib",
    "fullName": "torch.storage.np.lib.take_along_axis",
    "signature": "(arr, indices, axis)",
    "description": "Take values from the input array by matching 1d index and data slices."
  },
  "1794": {
    "name": "tile",
    "module": "torch.storage.np.lib",
    "fullName": "torch.storage.np.lib.tile",
    "signature": "(A, reps)",
    "description": "Construct an array by repeating A the number of times given by reps."
  },
  "1795": {
    "name": "trapz",
    "module": "torch.storage.np.lib",
    "fullName": "torch.storage.np.lib.trapz",
    "signature": "(y, x=None, dx=1.0, axis=-1)",
    "description": "Integrate along the given axis using the composite trapezoidal rule."
  },
  "1796": {
    "name": "tri",
    "module": "torch.storage.np.lib",
    "fullName": "torch.storage.np.lib.tri",
    "signature": "(N, M=None, k=0, dtype=<class 'float'>, *, like=None)",
    "description": "An array with ones at and below the given diagonal and zeros elsewhere."
  },
  "1797": {
    "name": "tril",
    "module": "torch.storage.np.lib",
    "fullName": "torch.storage.np.lib.tril",
    "signature": "(m, k=0)",
    "description": "Lower triangle of an array."
  },
  "1798": {
    "name": "tril_indices",
    "module": "torch.storage.np.lib",
    "fullName": "torch.storage.np.lib.tril_indices",
    "signature": "(n, k=0, m=None)",
    "description": "Return the indices for the lower-triangle of an (n, m) array."
  },
  "1799": {
    "name": "tril_indices_from",
    "module": "torch.storage.np.lib",
    "fullName": "torch.storage.np.lib.tril_indices_from",
    "signature": "(arr, k=0)",
    "description": "Return the indices for the lower-triangle of arr."
  },
  "1800": {
    "name": "trim_zeros",
    "module": "torch.storage.np.lib",
    "fullName": "torch.storage.np.lib.trim_zeros",
    "signature": "(filt, trim='fb')",
    "description": "Trim the leading and/or trailing zeros from a 1-D array or sequence."
  },
  "1801": {
    "name": "triu",
    "module": "torch.storage.np.lib",
    "fullName": "torch.storage.np.lib.triu",
    "signature": "(m, k=0)",
    "description": "Upper triangle of an array."
  },
  "1802": {
    "name": "triu_indices",
    "module": "torch.storage.np.lib",
    "fullName": "torch.storage.np.lib.triu_indices",
    "signature": "(n, k=0, m=None)",
    "description": "Return the indices for the upper-triangle of an (n, m) array."
  },
  "1803": {
    "name": "triu_indices_from",
    "module": "torch.storage.np.lib",
    "fullName": "torch.storage.np.lib.triu_indices_from",
    "signature": "(arr, k=0)",
    "description": "Return the indices for the upper-triangle of arr."
  },
  "1804": {
    "name": "typename",
    "module": "torch.storage.np.lib",
    "fullName": "torch.storage.np.lib.typename",
    "signature": "(char)",
    "description": "Return a description for the given data type code."
  },
  "1805": {
    "name": "union1d",
    "module": "torch.storage.np.lib",
    "fullName": "torch.storage.np.lib.union1d",
    "signature": "(ar1, ar2)",
    "description": "Find the union of two arrays."
  },
  "1806": {
    "name": "unique",
    "module": "torch.storage.np.lib",
    "fullName": "torch.storage.np.lib.unique",
    "signature": "(ar, return_index=False, return_inverse=False, return_counts=False, axis=None, *, equal_nan=True)",
    "description": "Find the unique elements of an array."
  },
  "1807": {
    "name": "unpackbits",
    "module": "torch.storage.np.lib",
    "fullName": "torch.storage.np.lib.unpackbits",
    "signature": "N/A",
    "description": "unpackbits(a, /, axis=None, count=None, bitorder='big')"
  },
  "1808": {
    "name": "unravel_index",
    "module": "torch.storage.np.lib",
    "fullName": "torch.storage.np.lib.unravel_index",
    "signature": "N/A",
    "description": "unravel_index(indices, shape, order='C')"
  },
  "1809": {
    "name": "unwrap",
    "module": "torch.storage.np.lib",
    "fullName": "torch.storage.np.lib.unwrap",
    "signature": "(p, discont=None, axis=-1, *, period=6.283185307179586)",
    "description": "Unwrap by taking the complement of large deltas with respect to the period."
  },
  "1810": {
    "name": "vander",
    "module": "torch.storage.np.lib",
    "fullName": "torch.storage.np.lib.vander",
    "signature": "(x, N=None, increasing=False)",
    "description": "Generate a Vandermonde matrix."
  },
  "1811": {
    "name": "vsplit",
    "module": "torch.storage.np.lib",
    "fullName": "torch.storage.np.lib.vsplit",
    "signature": "(ary, indices_or_sections)",
    "description": "Split an array into multiple sub-arrays vertically (row-wise)."
  },
  "1812": {
    "name": "who",
    "module": "torch.storage.np.lib",
    "fullName": "torch.storage.np.lib.who",
    "signature": "(vardict=None)",
    "description": "Print the NumPy arrays in the given dictionary."
  },
  "1813": {
    "name": "byte_bounds",
    "module": "torch.storage.np.lib.utils",
    "fullName": "torch.storage.np.lib.utils.byte_bounds",
    "signature": "(a)",
    "description": "Returns pointers to the end-points of an array."
  },
  "1814": {
    "name": "deprecate_with_doc",
    "module": "torch.storage.np.lib.utils",
    "fullName": "torch.storage.np.lib.utils.deprecate_with_doc",
    "signature": "(msg)",
    "description": "Deprecates a function and includes the deprecation in its docstring."
  },
  "1815": {
    "name": "get_include",
    "module": "torch.storage.np.lib.utils",
    "fullName": "torch.storage.np.lib.utils.get_include",
    "signature": "()",
    "description": "Return the directory that contains the NumPy \\*.h header files."
  },
  "1816": {
    "name": "info",
    "module": "torch.storage.np.lib.utils",
    "fullName": "torch.storage.np.lib.utils.info",
    "signature": "(object=None, maxwidth=76, output=None, toplevel='numpy')",
    "description": "Get help information for a function, class, or module."
  },
  "1817": {
    "name": "issubclass_",
    "module": "torch.storage.np.lib.utils",
    "fullName": "torch.storage.np.lib.utils.issubclass_",
    "signature": "(arg1, arg2)",
    "description": "Determine if a class is a subclass of a second class."
  },
  "1818": {
    "name": "issubdtype",
    "module": "torch.storage.np.lib.utils",
    "fullName": "torch.storage.np.lib.utils.issubdtype",
    "signature": "(arg1, arg2)",
    "description": "Returns True if first argument is a typecode lower/equal in type hierarchy."
  },
  "1819": {
    "name": "issubsctype",
    "module": "torch.storage.np.lib.utils",
    "fullName": "torch.storage.np.lib.utils.issubsctype",
    "signature": "(arg1, arg2)",
    "description": "Determine if the first argument is a subclass of the second argument."
  },
  "1820": {
    "name": "lookfor",
    "module": "torch.storage.np.lib.utils",
    "fullName": "torch.storage.np.lib.utils.lookfor",
    "signature": "(what, module=None, import_modules=True, regenerate=False, output=None)",
    "description": "Do a keyword search on docstrings."
  },
  "1821": {
    "name": "safe_eval",
    "module": "torch.storage.np.lib.utils",
    "fullName": "torch.storage.np.lib.utils.safe_eval",
    "signature": "(source)",
    "description": "Protected string evaluation."
  },
  "1822": {
    "name": "set_module",
    "module": "torch.storage.np.lib.utils",
    "fullName": "torch.storage.np.lib.utils.set_module",
    "signature": "(module)",
    "description": "Decorator for overriding __module__ on a function or class."
  },
  "1823": {
    "name": "show_runtime",
    "module": "torch.storage.np.lib.utils",
    "fullName": "torch.storage.np.lib.utils.show_runtime",
    "signature": "()",
    "description": "Print information about various resources in the system"
  },
  "1824": {
    "name": "source",
    "module": "torch.storage.np.lib.utils",
    "fullName": "torch.storage.np.lib.utils.source",
    "signature": "(object, output=<_io.TextIOWrapper name='<stdout>' mode='w' encoding='utf-8'>)",
    "description": "Print or write to a file the source code for a NumPy object."
  },
  "1825": {
    "name": "who",
    "module": "torch.storage.np.lib.utils",
    "fullName": "torch.storage.np.lib.utils.who",
    "signature": "(vardict=None)",
    "description": "Print the NumPy arrays in the given dictionary."
  },
  "1826": {
    "name": "array_function_dispatch",
    "module": "torch.storage.np.lib.ufunclike",
    "fullName": "torch.storage.np.lib.ufunclike.array_function_dispatch",
    "signature": "(dispatcher, module=None, verify=True, docs_from_dispatcher=False, use_like=False)",
    "description": "Decorator for adding dispatch with the __array_function__ protocol."
  },
  "1827": {
    "name": "fix",
    "module": "torch.storage.np.lib.ufunclike",
    "fullName": "torch.storage.np.lib.ufunclike.fix",
    "signature": "(x, out=None)",
    "description": "Round to nearest integer towards zero."
  },
  "1828": {
    "name": "isneginf",
    "module": "torch.storage.np.lib.ufunclike",
    "fullName": "torch.storage.np.lib.ufunclike.isneginf",
    "signature": "(x, out=None)",
    "description": "Test element-wise for negative infinity, return result as bool array."
  },
  "1829": {
    "name": "isposinf",
    "module": "torch.storage.np.lib.ufunclike",
    "fullName": "torch.storage.np.lib.ufunclike.isposinf",
    "signature": "(x, out=None)",
    "description": "Test element-wise for positive infinity, return result as bool array."
  },
  "1830": {
    "name": "asfarray",
    "module": "torch.storage.np.lib.type_check",
    "fullName": "torch.storage.np.lib.type_check.asfarray",
    "signature": "(a, dtype=<class 'numpy.float64'>)",
    "description": "Return an array converted to a float type."
  },
  "1831": {
    "name": "common_type",
    "module": "torch.storage.np.lib.type_check",
    "fullName": "torch.storage.np.lib.type_check.common_type",
    "signature": "(*arrays)",
    "description": "Return a scalar type which is common to the input arrays."
  },
  "1832": {
    "name": "imag",
    "module": "torch.storage.np.lib.type_check",
    "fullName": "torch.storage.np.lib.type_check.imag",
    "signature": "(val)",
    "description": "Return the imaginary part of the complex argument."
  },
  "1833": {
    "name": "iscomplex",
    "module": "torch.storage.np.lib.type_check",
    "fullName": "torch.storage.np.lib.type_check.iscomplex",
    "signature": "(x)",
    "description": "Returns a bool array, where True if input element is complex."
  },
  "1834": {
    "name": "iscomplexobj",
    "module": "torch.storage.np.lib.type_check",
    "fullName": "torch.storage.np.lib.type_check.iscomplexobj",
    "signature": "(x)",
    "description": "Check for a complex type or an array of complex numbers."
  },
  "1835": {
    "name": "isneginf",
    "module": "torch.storage.np.lib.type_check",
    "fullName": "torch.storage.np.lib.type_check.isneginf",
    "signature": "(x, out=None)",
    "description": "Test element-wise for negative infinity, return result as bool array."
  },
  "1836": {
    "name": "isposinf",
    "module": "torch.storage.np.lib.type_check",
    "fullName": "torch.storage.np.lib.type_check.isposinf",
    "signature": "(x, out=None)",
    "description": "Test element-wise for positive infinity, return result as bool array."
  },
  "1837": {
    "name": "isreal",
    "module": "torch.storage.np.lib.type_check",
    "fullName": "torch.storage.np.lib.type_check.isreal",
    "signature": "(x)",
    "description": "Returns a bool array, where True if input element is real."
  },
  "1838": {
    "name": "isrealobj",
    "module": "torch.storage.np.lib.type_check",
    "fullName": "torch.storage.np.lib.type_check.isrealobj",
    "signature": "(x)",
    "description": "Return True if x is a not complex type or an array of complex numbers."
  },
  "1839": {
    "name": "mintypecode",
    "module": "torch.storage.np.lib.type_check",
    "fullName": "torch.storage.np.lib.type_check.mintypecode",
    "signature": "(typechars, typeset='GDFgdf', default='d')",
    "description": "Return the character for the minimum-size type to which given types can"
  },
  "1840": {
    "name": "nan_to_num",
    "module": "torch.storage.np.lib.type_check",
    "fullName": "torch.storage.np.lib.type_check.nan_to_num",
    "signature": "(x, copy=True, nan=0.0, posinf=None, neginf=None)",
    "description": "Replace NaN with zero and infinity with large finite numbers (default"
  },
  "1841": {
    "name": "real",
    "module": "torch.storage.np.lib.type_check",
    "fullName": "torch.storage.np.lib.type_check.real",
    "signature": "(val)",
    "description": "Return the real part of the complex argument."
  },
  "1842": {
    "name": "real_if_close",
    "module": "torch.storage.np.lib.type_check",
    "fullName": "torch.storage.np.lib.type_check.real_if_close",
    "signature": "(a, tol=100)",
    "description": "If input is complex with all imaginary parts close to zero, return"
  },
  "1843": {
    "name": "set_module",
    "module": "torch.storage.np.lib.type_check",
    "fullName": "torch.storage.np.lib.type_check.set_module",
    "signature": "(module)",
    "description": "Decorator for overriding __module__ on a function or class."
  },
  "1844": {
    "name": "typename",
    "module": "torch.storage.np.lib.type_check",
    "fullName": "torch.storage.np.lib.type_check.typename",
    "signature": "(char)",
    "description": "Return a description for the given data type code."
  },
  "1845": {
    "name": "broadcast_to",
    "module": "torch.storage.np.lib.twodim_base",
    "fullName": "torch.storage.np.lib.twodim_base.broadcast_to",
    "signature": "(array, shape, subok=False)",
    "description": "Broadcast an array to a new shape."
  },
  "1846": {
    "name": "diag",
    "module": "torch.storage.np.lib.twodim_base",
    "fullName": "torch.storage.np.lib.twodim_base.diag",
    "signature": "(v, k=0)",
    "description": "Extract a diagonal or construct a diagonal array."
  },
  "1847": {
    "name": "diagflat",
    "module": "torch.storage.np.lib.twodim_base",
    "fullName": "torch.storage.np.lib.twodim_base.diagflat",
    "signature": "(v, k=0)",
    "description": "Create a two-dimensional array with the flattened input as a diagonal."
  },
  "1848": {
    "name": "eye",
    "module": "torch.storage.np.lib.twodim_base",
    "fullName": "torch.storage.np.lib.twodim_base.eye",
    "signature": "(N, M=None, k=0, dtype=<class 'float'>, order='C', *, like=None)",
    "description": "Return a 2-D array with ones on the diagonal and zeros elsewhere."
  },
  "1849": {
    "name": "fliplr",
    "module": "torch.storage.np.lib.twodim_base",
    "fullName": "torch.storage.np.lib.twodim_base.fliplr",
    "signature": "(m)",
    "description": "Reverse the order of elements along axis 1 (left/right)."
  },
  "1850": {
    "name": "flipud",
    "module": "torch.storage.np.lib.twodim_base",
    "fullName": "torch.storage.np.lib.twodim_base.flipud",
    "signature": "(m)",
    "description": "Reverse the order of elements along axis 0 (up/down)."
  },
  "1851": {
    "name": "histogram2d",
    "module": "torch.storage.np.lib.twodim_base",
    "fullName": "torch.storage.np.lib.twodim_base.histogram2d",
    "signature": "(x, y, bins=10, range=None, density=None, weights=None)",
    "description": "Compute the bi-dimensional histogram of two data samples."
  },
  "1852": {
    "name": "indices",
    "module": "torch.storage.np.lib.twodim_base",
    "fullName": "torch.storage.np.lib.twodim_base.indices",
    "signature": "(dimensions, dtype=<class 'int'>, sparse=False)",
    "description": "Return an array representing the indices of a grid."
  },
  "1853": {
    "name": "mask_indices",
    "module": "torch.storage.np.lib.twodim_base",
    "fullName": "torch.storage.np.lib.twodim_base.mask_indices",
    "signature": "(n, mask_func, k=0)",
    "description": "Return the indices to access (n, n) arrays, given a masking function."
  },
  "1854": {
    "name": "ones",
    "module": "torch.storage.np.lib.twodim_base",
    "fullName": "torch.storage.np.lib.twodim_base.ones",
    "signature": "(shape, dtype=None, order='C', *, like=None)",
    "description": "Return a new array of given shape and type, filled with ones."
  },
  "1855": {
    "name": "set_array_function_like_doc",
    "module": "torch.storage.np.lib.twodim_base",
    "fullName": "torch.storage.np.lib.twodim_base.set_array_function_like_doc",
    "signature": "(public_api)",
    "description": "No description available."
  },
  "1856": {
    "name": "set_module",
    "module": "torch.storage.np.lib.twodim_base",
    "fullName": "torch.storage.np.lib.twodim_base.set_module",
    "signature": "(module)",
    "description": "Decorator for overriding __module__ on a function or class."
  },
  "1857": {
    "name": "tri",
    "module": "torch.storage.np.lib.twodim_base",
    "fullName": "torch.storage.np.lib.twodim_base.tri",
    "signature": "(N, M=None, k=0, dtype=<class 'float'>, *, like=None)",
    "description": "An array with ones at and below the given diagonal and zeros elsewhere."
  },
  "1858": {
    "name": "tril",
    "module": "torch.storage.np.lib.twodim_base",
    "fullName": "torch.storage.np.lib.twodim_base.tril",
    "signature": "(m, k=0)",
    "description": "Lower triangle of an array."
  },
  "1859": {
    "name": "tril_indices",
    "module": "torch.storage.np.lib.twodim_base",
    "fullName": "torch.storage.np.lib.twodim_base.tril_indices",
    "signature": "(n, k=0, m=None)",
    "description": "Return the indices for the lower-triangle of an (n, m) array."
  },
  "1860": {
    "name": "tril_indices_from",
    "module": "torch.storage.np.lib.twodim_base",
    "fullName": "torch.storage.np.lib.twodim_base.tril_indices_from",
    "signature": "(arr, k=0)",
    "description": "Return the indices for the lower-triangle of arr."
  },
  "1861": {
    "name": "triu",
    "module": "torch.storage.np.lib.twodim_base",
    "fullName": "torch.storage.np.lib.twodim_base.triu",
    "signature": "(m, k=0)",
    "description": "Upper triangle of an array."
  },
  "1862": {
    "name": "triu_indices",
    "module": "torch.storage.np.lib.twodim_base",
    "fullName": "torch.storage.np.lib.twodim_base.triu_indices",
    "signature": "(n, k=0, m=None)",
    "description": "Return the indices for the upper-triangle of an (n, m) array."
  },
  "1863": {
    "name": "triu_indices_from",
    "module": "torch.storage.np.lib.twodim_base",
    "fullName": "torch.storage.np.lib.twodim_base.triu_indices_from",
    "signature": "(arr, k=0)",
    "description": "Return the indices for the upper-triangle of arr."
  },
  "1864": {
    "name": "vander",
    "module": "torch.storage.np.lib.twodim_base",
    "fullName": "torch.storage.np.lib.twodim_base.vander",
    "signature": "(x, N=None, increasing=False)",
    "description": "Generate a Vandermonde matrix."
  },
  "1865": {
    "name": "where",
    "module": "torch.storage.np.lib.twodim_base",
    "fullName": "torch.storage.np.lib.twodim_base.where",
    "signature": "N/A",
    "description": "where(condition, [x, y], /)"
  },
  "1866": {
    "name": "array_function_dispatch",
    "module": "torch.storage.np.lib.stride_tricks",
    "fullName": "torch.storage.np.lib.stride_tricks.array_function_dispatch",
    "signature": "(dispatcher, module=None, verify=True, docs_from_dispatcher=False, use_like=False)",
    "description": "Decorator for adding dispatch with the __array_function__ protocol."
  },
  "1867": {
    "name": "as_strided",
    "module": "torch.storage.np.lib.stride_tricks",
    "fullName": "torch.storage.np.lib.stride_tricks.as_strided",
    "signature": "(x, shape=None, strides=None, subok=False, writeable=True)",
    "description": "Create a view into the array with the given shape and strides."
  },
  "1868": {
    "name": "broadcast_shapes",
    "module": "torch.storage.np.lib.stride_tricks",
    "fullName": "torch.storage.np.lib.stride_tricks.broadcast_shapes",
    "signature": "(*args)",
    "description": "Broadcast the input shapes into a single shape."
  },
  "1869": {
    "name": "broadcast_to",
    "module": "torch.storage.np.lib.stride_tricks",
    "fullName": "torch.storage.np.lib.stride_tricks.broadcast_to",
    "signature": "(array, shape, subok=False)",
    "description": "Broadcast an array to a new shape."
  },
  "1870": {
    "name": "normalize_axis_tuple",
    "module": "torch.storage.np.lib.stride_tricks",
    "fullName": "torch.storage.np.lib.stride_tricks.normalize_axis_tuple",
    "signature": "(axis, ndim, argname=None, allow_duplicate=False)",
    "description": "Normalizes an axis argument into a tuple of non-negative integer axes."
  },
  "1871": {
    "name": "set_module",
    "module": "torch.storage.np.lib.stride_tricks",
    "fullName": "torch.storage.np.lib.stride_tricks.set_module",
    "signature": "(module)",
    "description": "Decorator for overriding __module__ on a function or class."
  },
  "1872": {
    "name": "sliding_window_view",
    "module": "torch.storage.np.lib.stride_tricks",
    "fullName": "torch.storage.np.lib.stride_tricks.sliding_window_view",
    "signature": "(x, window_shape, axis=None, *, subok=False, writeable=False)",
    "description": "Create a sliding window view into the array with the given window shape."
  },
  "1873": {
    "name": "apply_along_axis",
    "module": "torch.storage.np.lib.shape_base",
    "fullName": "torch.storage.np.lib.shape_base.apply_along_axis",
    "signature": "(func1d, axis, arr, *args, **kwargs)",
    "description": "Apply a function to 1-D slices along the given axis."
  },
  "1874": {
    "name": "apply_over_axes",
    "module": "torch.storage.np.lib.shape_base",
    "fullName": "torch.storage.np.lib.shape_base.apply_over_axes",
    "signature": "(func, a, axes)",
    "description": "Apply a function repeatedly over multiple axes."
  },
  "1875": {
    "name": "array_split",
    "module": "torch.storage.np.lib.shape_base",
    "fullName": "torch.storage.np.lib.shape_base.array_split",
    "signature": "(ary, indices_or_sections, axis=0)",
    "description": "Split an array into multiple sub-arrays."
  },
  "1876": {
    "name": "atleast_3d",
    "module": "torch.storage.np.lib.shape_base",
    "fullName": "torch.storage.np.lib.shape_base.atleast_3d",
    "signature": "(*arys)",
    "description": "View inputs as arrays with at least three dimensions."
  },
  "1877": {
    "name": "column_stack",
    "module": "torch.storage.np.lib.shape_base",
    "fullName": "torch.storage.np.lib.shape_base.column_stack",
    "signature": "(tup)",
    "description": "Stack 1-D arrays as columns into a 2-D array."
  },
  "1878": {
    "name": "concatenate",
    "module": "torch.storage.np.lib.shape_base",
    "fullName": "torch.storage.np.lib.shape_base.concatenate",
    "signature": "N/A",
    "description": "concatenate((a1, a2, ...), axis=0, out=None, dtype=None, casting=\"same_kind\")"
  },
  "1879": {
    "name": "dsplit",
    "module": "torch.storage.np.lib.shape_base",
    "fullName": "torch.storage.np.lib.shape_base.dsplit",
    "signature": "(ary, indices_or_sections)",
    "description": "Split array into multiple sub-arrays along the 3rd axis (depth)."
  },
  "1880": {
    "name": "dstack",
    "module": "torch.storage.np.lib.shape_base",
    "fullName": "torch.storage.np.lib.shape_base.dstack",
    "signature": "(tup)",
    "description": "Stack arrays in sequence depth wise (along third axis)."
  },
  "1881": {
    "name": "get_array_prepare",
    "module": "torch.storage.np.lib.shape_base",
    "fullName": "torch.storage.np.lib.shape_base.get_array_prepare",
    "signature": "(*args)",
    "description": "Find the wrapper for the array with the highest priority."
  },
  "1882": {
    "name": "get_array_wrap",
    "module": "torch.storage.np.lib.shape_base",
    "fullName": "torch.storage.np.lib.shape_base.get_array_wrap",
    "signature": "(*args)",
    "description": "Find the wrapper for the array with the highest priority."
  },
  "1883": {
    "name": "hsplit",
    "module": "torch.storage.np.lib.shape_base",
    "fullName": "torch.storage.np.lib.shape_base.hsplit",
    "signature": "(ary, indices_or_sections)",
    "description": "Split an array into multiple sub-arrays horizontally (column-wise)."
  },
  "1884": {
    "name": "kron",
    "module": "torch.storage.np.lib.shape_base",
    "fullName": "torch.storage.np.lib.shape_base.kron",
    "signature": "(a, b)",
    "description": "Kronecker product of two arrays."
  },
  "1885": {
    "name": "normalize_axis_tuple",
    "module": "torch.storage.np.lib.shape_base",
    "fullName": "torch.storage.np.lib.shape_base.normalize_axis_tuple",
    "signature": "(axis, ndim, argname=None, allow_duplicate=False)",
    "description": "Normalizes an axis argument into a tuple of non-negative integer axes."
  },
  "1886": {
    "name": "outer",
    "module": "torch.storage.np.lib.shape_base",
    "fullName": "torch.storage.np.lib.shape_base.outer",
    "signature": "(a, b, out=None)",
    "description": "Compute the outer product of two vectors."
  },
  "1887": {
    "name": "put_along_axis",
    "module": "torch.storage.np.lib.shape_base",
    "fullName": "torch.storage.np.lib.shape_base.put_along_axis",
    "signature": "(arr, indices, values, axis)",
    "description": "Put values into the destination array by matching 1d index and data slices."
  },
  "1888": {
    "name": "reshape",
    "module": "torch.storage.np.lib.shape_base",
    "fullName": "torch.storage.np.lib.shape_base.reshape",
    "signature": "(a, newshape, order='C')",
    "description": "Gives a new shape to an array without changing its data."
  },
  "1889": {
    "name": "row_stack",
    "module": "torch.storage.np.lib.shape_base",
    "fullName": "torch.storage.np.lib.shape_base.row_stack",
    "signature": "(tup, *, dtype=None, casting='same_kind')",
    "description": "Stack arrays in sequence vertically (row wise)."
  },
  "1890": {
    "name": "split",
    "module": "torch.storage.np.lib.shape_base",
    "fullName": "torch.storage.np.lib.shape_base.split",
    "signature": "(ary, indices_or_sections, axis=0)",
    "description": "Split an array into multiple sub-arrays as views into `ary`."
  },
  "1891": {
    "name": "take_along_axis",
    "module": "torch.storage.np.lib.shape_base",
    "fullName": "torch.storage.np.lib.shape_base.take_along_axis",
    "signature": "(arr, indices, axis)",
    "description": "Take values from the input array by matching 1d index and data slices."
  },
  "1892": {
    "name": "tile",
    "module": "torch.storage.np.lib.shape_base",
    "fullName": "torch.storage.np.lib.shape_base.tile",
    "signature": "(A, reps)",
    "description": "Construct an array by repeating A the number of times given by reps."
  },
  "1893": {
    "name": "transpose",
    "module": "torch.storage.np.lib.shape_base",
    "fullName": "torch.storage.np.lib.shape_base.transpose",
    "signature": "(a, axes=None)",
    "description": "Returns an array with axes transposed."
  },
  "1894": {
    "name": "vsplit",
    "module": "torch.storage.np.lib.shape_base",
    "fullName": "torch.storage.np.lib.shape_base.vsplit",
    "signature": "(ary, indices_or_sections)",
    "description": "Split an array into multiple sub-arrays vertically (row-wise)."
  },
  "1895": {
    "name": "vstack",
    "module": "torch.storage.np.lib.shape_base",
    "fullName": "torch.storage.np.lib.shape_base.vstack",
    "signature": "(tup, *, dtype=None, casting='same_kind')",
    "description": "Stack arrays in sequence vertically (row wise)."
  },
  "1896": {
    "name": "any",
    "module": "torch.storage.np.lib.scimath",
    "fullName": "torch.storage.np.lib.scimath.any",
    "signature": "(a, axis=None, out=None, keepdims=<no value>, *, where=<no value>)",
    "description": "Test whether any array element along a given axis evaluates to True."
  },
  "1897": {
    "name": "arccos",
    "module": "torch.storage.np.lib.scimath",
    "fullName": "torch.storage.np.lib.scimath.arccos",
    "signature": "(x)",
    "description": "Compute the inverse cosine of x."
  },
  "1898": {
    "name": "arcsin",
    "module": "torch.storage.np.lib.scimath",
    "fullName": "torch.storage.np.lib.scimath.arcsin",
    "signature": "(x)",
    "description": "Compute the inverse sine of x."
  },
  "1899": {
    "name": "arctanh",
    "module": "torch.storage.np.lib.scimath",
    "fullName": "torch.storage.np.lib.scimath.arctanh",
    "signature": "(x)",
    "description": "Compute the inverse hyperbolic tangent of `x`."
  },
  "1900": {
    "name": "array_function_dispatch",
    "module": "torch.storage.np.lib.scimath",
    "fullName": "torch.storage.np.lib.scimath.array_function_dispatch",
    "signature": "(dispatcher, module=None, verify=True, docs_from_dispatcher=False, use_like=False)",
    "description": "Decorator for adding dispatch with the __array_function__ protocol."
  },
  "1901": {
    "name": "isreal",
    "module": "torch.storage.np.lib.scimath",
    "fullName": "torch.storage.np.lib.scimath.isreal",
    "signature": "(x)",
    "description": "Returns a bool array, where True if input element is real."
  },
  "1902": {
    "name": "log",
    "module": "torch.storage.np.lib.scimath",
    "fullName": "torch.storage.np.lib.scimath.log",
    "signature": "(x)",
    "description": "Compute the natural logarithm of `x`."
  },
  "1903": {
    "name": "log10",
    "module": "torch.storage.np.lib.scimath",
    "fullName": "torch.storage.np.lib.scimath.log10",
    "signature": "(x)",
    "description": "Compute the logarithm base 10 of `x`."
  },
  "1904": {
    "name": "log2",
    "module": "torch.storage.np.lib.scimath",
    "fullName": "torch.storage.np.lib.scimath.log2",
    "signature": "(x)",
    "description": "Compute the logarithm base 2 of `x`."
  },
  "1905": {
    "name": "logn",
    "module": "torch.storage.np.lib.scimath",
    "fullName": "torch.storage.np.lib.scimath.logn",
    "signature": "(n, x)",
    "description": "Take log base n of x."
  },
  "1906": {
    "name": "power",
    "module": "torch.storage.np.lib.scimath",
    "fullName": "torch.storage.np.lib.scimath.power",
    "signature": "(x, p)",
    "description": "Return x to the power p, (x**p)."
  },
  "1907": {
    "name": "sqrt",
    "module": "torch.storage.np.lib.scimath",
    "fullName": "torch.storage.np.lib.scimath.sqrt",
    "signature": "(x)",
    "description": "Compute the square root of x."
  },
  "1908": {
    "name": "atleast_1d",
    "module": "torch.storage.np.lib.polynomial",
    "fullName": "torch.storage.np.lib.polynomial.atleast_1d",
    "signature": "(*arys)",
    "description": "Convert inputs to arrays with at least one dimension."
  },
  "1909": {
    "name": "diag",
    "module": "torch.storage.np.lib.polynomial",
    "fullName": "torch.storage.np.lib.polynomial.diag",
    "signature": "(v, k=0)",
    "description": "Extract a diagonal or construct a diagonal array."
  },
  "1910": {
    "name": "dot",
    "module": "torch.storage.np.lib.polynomial",
    "fullName": "torch.storage.np.lib.polynomial.dot",
    "signature": "N/A",
    "description": "dot(a, b, out=None)"
  },
  "1911": {
    "name": "eigvals",
    "module": "torch.storage.np.lib.polynomial",
    "fullName": "torch.storage.np.lib.polynomial.eigvals",
    "signature": "(a)",
    "description": "Compute the eigenvalues of a general matrix."
  },
  "1912": {
    "name": "hstack",
    "module": "torch.storage.np.lib.polynomial",
    "fullName": "torch.storage.np.lib.polynomial.hstack",
    "signature": "(tup, *, dtype=None, casting='same_kind')",
    "description": "Stack arrays in sequence horizontally (column wise)."
  },
  "1913": {
    "name": "imag",
    "module": "torch.storage.np.lib.polynomial",
    "fullName": "torch.storage.np.lib.polynomial.imag",
    "signature": "(val)",
    "description": "Return the imaginary part of the complex argument."
  },
  "1914": {
    "name": "inv",
    "module": "torch.storage.np.lib.polynomial",
    "fullName": "torch.storage.np.lib.polynomial.inv",
    "signature": "(a)",
    "description": "Compute the (multiplicative) inverse of a matrix."
  },
  "1915": {
    "name": "iscomplex",
    "module": "torch.storage.np.lib.polynomial",
    "fullName": "torch.storage.np.lib.polynomial.iscomplex",
    "signature": "(x)",
    "description": "Returns a bool array, where True if input element is complex."
  },
  "1916": {
    "name": "isscalar",
    "module": "torch.storage.np.lib.polynomial",
    "fullName": "torch.storage.np.lib.polynomial.isscalar",
    "signature": "(element)",
    "description": "Returns True if the type of `element` is a scalar type."
  },
  "1917": {
    "name": "lstsq",
    "module": "torch.storage.np.lib.polynomial",
    "fullName": "torch.storage.np.lib.polynomial.lstsq",
    "signature": "(a, b, rcond='warn')",
    "description": "Return the least-squares solution to a linear matrix equation."
  },
  "1918": {
    "name": "mintypecode",
    "module": "torch.storage.np.lib.polynomial",
    "fullName": "torch.storage.np.lib.polynomial.mintypecode",
    "signature": "(typechars, typeset='GDFgdf', default='d')",
    "description": "Return the character for the minimum-size type to which given types can"
  },
  "1919": {
    "name": "ones",
    "module": "torch.storage.np.lib.polynomial",
    "fullName": "torch.storage.np.lib.polynomial.ones",
    "signature": "(shape, dtype=None, order='C', *, like=None)",
    "description": "Return a new array of given shape and type, filled with ones."
  },
  "1920": {
    "name": "poly",
    "module": "torch.storage.np.lib.polynomial",
    "fullName": "torch.storage.np.lib.polynomial.poly",
    "signature": "(seq_of_zeros)",
    "description": "Find the coefficients of a polynomial with the given sequence of roots."
  },
  "1921": {
    "name": "polyadd",
    "module": "torch.storage.np.lib.polynomial",
    "fullName": "torch.storage.np.lib.polynomial.polyadd",
    "signature": "(a1, a2)",
    "description": "Find the sum of two polynomials."
  },
  "1922": {
    "name": "polyder",
    "module": "torch.storage.np.lib.polynomial",
    "fullName": "torch.storage.np.lib.polynomial.polyder",
    "signature": "(p, m=1)",
    "description": "Return the derivative of the specified order of a polynomial."
  },
  "1923": {
    "name": "polydiv",
    "module": "torch.storage.np.lib.polynomial",
    "fullName": "torch.storage.np.lib.polynomial.polydiv",
    "signature": "(u, v)",
    "description": "Returns the quotient and remainder of polynomial division."
  },
  "1924": {
    "name": "polyfit",
    "module": "torch.storage.np.lib.polynomial",
    "fullName": "torch.storage.np.lib.polynomial.polyfit",
    "signature": "(x, y, deg, rcond=None, full=False, w=None, cov=False)",
    "description": "Least squares polynomial fit."
  },
  "1925": {
    "name": "polyint",
    "module": "torch.storage.np.lib.polynomial",
    "fullName": "torch.storage.np.lib.polynomial.polyint",
    "signature": "(p, m=1, k=None)",
    "description": "Return an antiderivative (indefinite integral) of a polynomial."
  },
  "1926": {
    "name": "polymul",
    "module": "torch.storage.np.lib.polynomial",
    "fullName": "torch.storage.np.lib.polynomial.polymul",
    "signature": "(a1, a2)",
    "description": "Find the product of two polynomials."
  },
  "1927": {
    "name": "polysub",
    "module": "torch.storage.np.lib.polynomial",
    "fullName": "torch.storage.np.lib.polynomial.polysub",
    "signature": "(a1, a2)",
    "description": "Difference (subtraction) of two polynomials."
  },
  "1928": {
    "name": "polyval",
    "module": "torch.storage.np.lib.polynomial",
    "fullName": "torch.storage.np.lib.polynomial.polyval",
    "signature": "(p, x)",
    "description": "Evaluate a polynomial at specific values."
  },
  "1929": {
    "name": "real",
    "module": "torch.storage.np.lib.polynomial",
    "fullName": "torch.storage.np.lib.polynomial.real",
    "signature": "(val)",
    "description": "Return the real part of the complex argument."
  },
  "1930": {
    "name": "roots",
    "module": "torch.storage.np.lib.polynomial",
    "fullName": "torch.storage.np.lib.polynomial.roots",
    "signature": "(p)",
    "description": "Return the roots of a polynomial with coefficients given in p."
  },
  "1931": {
    "name": "set_module",
    "module": "torch.storage.np.lib.polynomial",
    "fullName": "torch.storage.np.lib.polynomial.set_module",
    "signature": "(module)",
    "description": "Decorator for overriding __module__ on a function or class."
  },
  "1932": {
    "name": "trim_zeros",
    "module": "torch.storage.np.lib.polynomial",
    "fullName": "torch.storage.np.lib.polynomial.trim_zeros",
    "signature": "(filt, trim='fb')",
    "description": "Trim the leading and/or trailing zeros from a 1-D array or sequence."
  },
  "1933": {
    "name": "vander",
    "module": "torch.storage.np.lib.polynomial",
    "fullName": "torch.storage.np.lib.polynomial.vander",
    "signature": "(x, N=None, increasing=False)",
    "description": "Generate a Vandermonde matrix."
  },
  "1934": {
    "name": "asbytes",
    "module": "torch.storage.np.lib.npyio",
    "fullName": "torch.storage.np.lib.npyio.asbytes",
    "signature": "(s)",
    "description": "No description available."
  },
  "1935": {
    "name": "asstr",
    "module": "torch.storage.np.lib.npyio",
    "fullName": "torch.storage.np.lib.npyio.asstr",
    "signature": "(s)",
    "description": "No description available."
  },
  "1936": {
    "name": "asunicode",
    "module": "torch.storage.np.lib.npyio",
    "fullName": "torch.storage.np.lib.npyio.asunicode",
    "signature": "(s)",
    "description": "No description available."
  },
  "1937": {
    "name": "easy_dtype",
    "module": "torch.storage.np.lib.npyio",
    "fullName": "torch.storage.np.lib.npyio.easy_dtype",
    "signature": "(ndtype, names=None, defaultfmt='f%i', **validationargs)",
    "description": "Convenience function to create a `np.dtype` object."
  },
  "1938": {
    "name": "flatten_dtype",
    "module": "torch.storage.np.lib.npyio",
    "fullName": "torch.storage.np.lib.npyio.flatten_dtype",
    "signature": "(ndtype, flatten_base=False)",
    "description": "Unpack a structured data-type by collapsing nested fields and/or fields"
  },
  "1939": {
    "name": "fromregex",
    "module": "torch.storage.np.lib.npyio",
    "fullName": "torch.storage.np.lib.npyio.fromregex",
    "signature": "(file, regexp, dtype, encoding=None)",
    "description": "Construct an array from a text file, using regular expression parsing."
  },
  "1940": {
    "name": "genfromtxt",
    "module": "torch.storage.np.lib.npyio",
    "fullName": "torch.storage.np.lib.npyio.genfromtxt",
    "signature": "(fname, dtype=<class 'float'>, comments='#', delimiter=None, skip_header=0, skip_footer=0, converters=None, missing_values=None, filling_values=None, usecols=None, names=None, excludelist=None, deletechars=\" !#$%&'()*+,-./:;<=>?@[\\\\]^{|}~\", replace_space='_', autostrip=False, case_sensitive=True, defaultfmt='f%i', unpack=None, usemask=False, loose=True, invalid_raise=True, max_rows=None, encoding='bytes', *, ndmin=0, like=None)",
    "description": "Load data from a text file, with missing values handled as specified."
  },
  "1941": {
    "name": "has_nested_fields",
    "module": "torch.storage.np.lib.npyio",
    "fullName": "torch.storage.np.lib.npyio.has_nested_fields",
    "signature": "(ndtype)",
    "description": "Returns whether one or several fields of a dtype are nested."
  },
  "1942": {
    "name": "load",
    "module": "torch.storage.np.lib.npyio",
    "fullName": "torch.storage.np.lib.npyio.load",
    "signature": "(file, mmap_mode=None, allow_pickle=False, fix_imports=True, encoding='ASCII', *, max_header_size=10000)",
    "description": "Load arrays or pickled objects from ``.npy``, ``.npz`` or pickled files."
  },
  "1943": {
    "name": "loadtxt",
    "module": "torch.storage.np.lib.npyio",
    "fullName": "torch.storage.np.lib.npyio.loadtxt",
    "signature": "(fname, dtype=<class 'float'>, comments='#', delimiter=None, converters=None, skiprows=0, usecols=None, unpack=False, ndmin=0, encoding='bytes', max_rows=None, *, quotechar=None, like=None)",
    "description": "Load data from a text file."
  },
  "1944": {
    "name": "packbits",
    "module": "torch.storage.np.lib.npyio",
    "fullName": "torch.storage.np.lib.npyio.packbits",
    "signature": "N/A",
    "description": "packbits(a, /, axis=None, bitorder='big')"
  },
  "1945": {
    "name": "recfromcsv",
    "module": "torch.storage.np.lib.npyio",
    "fullName": "torch.storage.np.lib.npyio.recfromcsv",
    "signature": "(fname, **kwargs)",
    "description": "Load ASCII data stored in a comma-separated file."
  },
  "1946": {
    "name": "recfromtxt",
    "module": "torch.storage.np.lib.npyio",
    "fullName": "torch.storage.np.lib.npyio.recfromtxt",
    "signature": "(fname, **kwargs)",
    "description": "Load ASCII data from a file and return it in a record array."
  },
  "1947": {
    "name": "save",
    "module": "torch.storage.np.lib.npyio",
    "fullName": "torch.storage.np.lib.npyio.save",
    "signature": "(file, arr, allow_pickle=True, fix_imports=True)",
    "description": "Save an array to a binary file in NumPy ``.npy`` format."
  },
  "1948": {
    "name": "savetxt",
    "module": "torch.storage.np.lib.npyio",
    "fullName": "torch.storage.np.lib.npyio.savetxt",
    "signature": "(fname, X, fmt='%.18e', delimiter=' ', newline='\\n', header='', footer='', comments='# ', encoding=None)",
    "description": "Save an array to a text file."
  },
  "1949": {
    "name": "savez",
    "module": "torch.storage.np.lib.npyio",
    "fullName": "torch.storage.np.lib.npyio.savez",
    "signature": "(file, *args, **kwds)",
    "description": "Save several arrays into a single file in uncompressed ``.npz`` format."
  },
  "1950": {
    "name": "savez_compressed",
    "module": "torch.storage.np.lib.npyio",
    "fullName": "torch.storage.np.lib.npyio.savez_compressed",
    "signature": "(file, *args, **kwds)",
    "description": "Save several arrays into a single file in compressed ``.npz`` format."
  },
  "1951": {
    "name": "set_array_function_like_doc",
    "module": "torch.storage.np.lib.npyio",
    "fullName": "torch.storage.np.lib.npyio.set_array_function_like_doc",
    "signature": "(public_api)",
    "description": "No description available."
  },
  "1952": {
    "name": "set_module",
    "module": "torch.storage.np.lib.npyio",
    "fullName": "torch.storage.np.lib.npyio.set_module",
    "signature": "(module)",
    "description": "Decorator for overriding __module__ on a function or class."
  },
  "1953": {
    "name": "unpackbits",
    "module": "torch.storage.np.lib.npyio",
    "fullName": "torch.storage.np.lib.npyio.unpackbits",
    "signature": "N/A",
    "description": "unpackbits(a, /, axis=None, count=None, bitorder='big')"
  },
  "1954": {
    "name": "zipfile_factory",
    "module": "torch.storage.np.lib.npyio",
    "fullName": "torch.storage.np.lib.npyio.zipfile_factory",
    "signature": "(file, *args, **kwargs)",
    "description": "Create a ZipFile."
  },
  "1955": {
    "name": "descr_to_dtype",
    "module": "torch.storage.np.lib.npyio.format",
    "fullName": "torch.storage.np.lib.npyio.format.descr_to_dtype",
    "signature": "(descr)",
    "description": "Returns a dtype based off the given description."
  },
  "1956": {
    "name": "dtype_to_descr",
    "module": "torch.storage.np.lib.npyio.format",
    "fullName": "torch.storage.np.lib.npyio.format.dtype_to_descr",
    "signature": "(dtype)",
    "description": "Get a serializable descriptor from the dtype."
  },
  "1957": {
    "name": "header_data_from_array_1_0",
    "module": "torch.storage.np.lib.npyio.format",
    "fullName": "torch.storage.np.lib.npyio.format.header_data_from_array_1_0",
    "signature": "(array)",
    "description": "Get the dictionary of header metadata from a numpy.ndarray."
  },
  "1958": {
    "name": "magic",
    "module": "torch.storage.np.lib.npyio.format",
    "fullName": "torch.storage.np.lib.npyio.format.magic",
    "signature": "(major, minor)",
    "description": "Return the magic string for the given file format version."
  },
  "1959": {
    "name": "open_memmap",
    "module": "torch.storage.np.lib.npyio.format",
    "fullName": "torch.storage.np.lib.npyio.format.open_memmap",
    "signature": "(filename, mode='r+', dtype=None, shape=None, fortran_order=False, version=None, *, max_header_size=10000)",
    "description": "Open a .npy file as a memory-mapped array."
  },
  "1960": {
    "name": "read_array",
    "module": "torch.storage.np.lib.npyio.format",
    "fullName": "torch.storage.np.lib.npyio.format.read_array",
    "signature": "(fp, allow_pickle=False, pickle_kwargs=None, *, max_header_size=10000)",
    "description": "Read an array from an NPY file."
  },
  "1961": {
    "name": "read_array_header_1_0",
    "module": "torch.storage.np.lib.npyio.format",
    "fullName": "torch.storage.np.lib.npyio.format.read_array_header_1_0",
    "signature": "(fp, max_header_size=10000)",
    "description": "Read an array header from a filelike object using the 1.0 file format"
  },
  "1962": {
    "name": "read_array_header_2_0",
    "module": "torch.storage.np.lib.npyio.format",
    "fullName": "torch.storage.np.lib.npyio.format.read_array_header_2_0",
    "signature": "(fp, max_header_size=10000)",
    "description": "Read an array header from a filelike object using the 2.0 file format"
  },
  "1963": {
    "name": "read_magic",
    "module": "torch.storage.np.lib.npyio.format",
    "fullName": "torch.storage.np.lib.npyio.format.read_magic",
    "signature": "(fp)",
    "description": "Read the magic string to get the version of the file format."
  },
  "1964": {
    "name": "safe_eval",
    "module": "torch.storage.np.lib.npyio.format",
    "fullName": "torch.storage.np.lib.npyio.format.safe_eval",
    "signature": "(source)",
    "description": "Protected string evaluation."
  },
  "1965": {
    "name": "write_array",
    "module": "torch.storage.np.lib.npyio.format",
    "fullName": "torch.storage.np.lib.npyio.format.write_array",
    "signature": "(fp, array, version=None, allow_pickle=True, pickle_kwargs=None)",
    "description": "Write an array to an NPY file, including a header."
  },
  "1966": {
    "name": "write_array_header_1_0",
    "module": "torch.storage.np.lib.npyio.format",
    "fullName": "torch.storage.np.lib.npyio.format.write_array_header_1_0",
    "signature": "(fp, d)",
    "description": "Write the header for an array using the 1.0 format."
  },
  "1967": {
    "name": "write_array_header_2_0",
    "module": "torch.storage.np.lib.npyio.format",
    "fullName": "torch.storage.np.lib.npyio.format.write_array_header_2_0",
    "signature": "(fp, d)",
    "description": "Write the header for an array using the 2.0 format."
  },
  "1968": {
    "name": "nanargmax",
    "module": "torch.storage.np.lib.nanfunctions",
    "fullName": "torch.storage.np.lib.nanfunctions.nanargmax",
    "signature": "(a, axis=None, out=None, *, keepdims=<no value>)",
    "description": "Return the indices of the maximum values in the specified axis ignoring"
  },
  "1969": {
    "name": "nanargmin",
    "module": "torch.storage.np.lib.nanfunctions",
    "fullName": "torch.storage.np.lib.nanfunctions.nanargmin",
    "signature": "(a, axis=None, out=None, *, keepdims=<no value>)",
    "description": "Return the indices of the minimum values in the specified axis ignoring"
  },
  "1970": {
    "name": "nancumprod",
    "module": "torch.storage.np.lib.nanfunctions",
    "fullName": "torch.storage.np.lib.nanfunctions.nancumprod",
    "signature": "(a, axis=None, dtype=None, out=None)",
    "description": "Return the cumulative product of array elements over a given axis treating Not a"
  },
  "1971": {
    "name": "nancumsum",
    "module": "torch.storage.np.lib.nanfunctions",
    "fullName": "torch.storage.np.lib.nanfunctions.nancumsum",
    "signature": "(a, axis=None, dtype=None, out=None)",
    "description": "Return the cumulative sum of array elements over a given axis treating Not a"
  },
  "1972": {
    "name": "nanmax",
    "module": "torch.storage.np.lib.nanfunctions",
    "fullName": "torch.storage.np.lib.nanfunctions.nanmax",
    "signature": "(a, axis=None, out=None, keepdims=<no value>, initial=<no value>, where=<no value>)",
    "description": "Return the maximum of an array or maximum along an axis, ignoring any"
  },
  "1973": {
    "name": "nanmean",
    "module": "torch.storage.np.lib.nanfunctions",
    "fullName": "torch.storage.np.lib.nanfunctions.nanmean",
    "signature": "(a, axis=None, dtype=None, out=None, keepdims=<no value>, *, where=<no value>)",
    "description": "Compute the arithmetic mean along the specified axis, ignoring NaNs."
  },
  "1974": {
    "name": "nanmedian",
    "module": "torch.storage.np.lib.nanfunctions",
    "fullName": "torch.storage.np.lib.nanfunctions.nanmedian",
    "signature": "(a, axis=None, out=None, overwrite_input=False, keepdims=<no value>)",
    "description": "Compute the median along the specified axis, while ignoring NaNs."
  },
  "1975": {
    "name": "nanmin",
    "module": "torch.storage.np.lib.nanfunctions",
    "fullName": "torch.storage.np.lib.nanfunctions.nanmin",
    "signature": "(a, axis=None, out=None, keepdims=<no value>, initial=<no value>, where=<no value>)",
    "description": "Return minimum of an array or minimum along an axis, ignoring any NaNs."
  },
  "1976": {
    "name": "nanprod",
    "module": "torch.storage.np.lib.nanfunctions",
    "fullName": "torch.storage.np.lib.nanfunctions.nanprod",
    "signature": "(a, axis=None, dtype=None, out=None, keepdims=<no value>, initial=<no value>, where=<no value>)",
    "description": "Return the product of array elements over a given axis treating Not a"
  },
  "1977": {
    "name": "nanstd",
    "module": "torch.storage.np.lib.nanfunctions",
    "fullName": "torch.storage.np.lib.nanfunctions.nanstd",
    "signature": "(a, axis=None, dtype=None, out=None, ddof=0, keepdims=<no value>, *, where=<no value>)",
    "description": "Compute the standard deviation along the specified axis, while"
  },
  "1978": {
    "name": "nansum",
    "module": "torch.storage.np.lib.nanfunctions",
    "fullName": "torch.storage.np.lib.nanfunctions.nansum",
    "signature": "(a, axis=None, dtype=None, out=None, keepdims=<no value>, initial=<no value>, where=<no value>)",
    "description": "Return the sum of array elements over a given axis treating Not a"
  },
  "1979": {
    "name": "nanvar",
    "module": "torch.storage.np.lib.nanfunctions",
    "fullName": "torch.storage.np.lib.nanfunctions.nanvar",
    "signature": "(a, axis=None, dtype=None, out=None, ddof=0, keepdims=<no value>, *, where=<no value>)",
    "description": "Compute the variance along the specified axis, while ignoring NaNs."
  },
  "1980": {
    "name": "add_newdoc",
    "module": "torch.storage.np.lib.nanfunctions.function_base",
    "fullName": "torch.storage.np.lib.nanfunctions.function_base.add_newdoc",
    "signature": "(place, obj, doc, warn_on_python=True)",
    "description": "Add documentation to an existing object, typically one defined in C"
  },
  "1981": {
    "name": "angle",
    "module": "torch.storage.np.lib.nanfunctions.function_base",
    "fullName": "torch.storage.np.lib.nanfunctions.function_base.angle",
    "signature": "(z, deg=False)",
    "description": "Return the angle of the complex argument."
  },
  "1982": {
    "name": "any",
    "module": "torch.storage.np.lib.nanfunctions.function_base",
    "fullName": "torch.storage.np.lib.nanfunctions.function_base.any",
    "signature": "(a, axis=None, out=None, keepdims=<no value>, *, where=<no value>)",
    "description": "Test whether any array element along a given axis evaluates to True."
  },
  "1983": {
    "name": "append",
    "module": "torch.storage.np.lib.nanfunctions.function_base",
    "fullName": "torch.storage.np.lib.nanfunctions.function_base.append",
    "signature": "(arr, values, axis=None)",
    "description": "Append values to the end of an array."
  },
  "1984": {
    "name": "asarray_chkfinite",
    "module": "torch.storage.np.lib.nanfunctions.function_base",
    "fullName": "torch.storage.np.lib.nanfunctions.function_base.asarray_chkfinite",
    "signature": "(a, dtype=None, order=None)",
    "description": "Convert the input to an array, checking for NaNs or Infs."
  },
  "1985": {
    "name": "average",
    "module": "torch.storage.np.lib.nanfunctions.function_base",
    "fullName": "torch.storage.np.lib.nanfunctions.function_base.average",
    "signature": "(a, axis=None, weights=None, returned=False, *, keepdims=<no value>)",
    "description": "Compute the weighted average along the specified axis."
  },
  "1986": {
    "name": "bartlett",
    "module": "torch.storage.np.lib.nanfunctions.function_base",
    "fullName": "torch.storage.np.lib.nanfunctions.function_base.bartlett",
    "signature": "(M)",
    "description": "Return the Bartlett window."
  },
  "1987": {
    "name": "bincount",
    "module": "torch.storage.np.lib.nanfunctions.function_base",
    "fullName": "torch.storage.np.lib.nanfunctions.function_base.bincount",
    "signature": "N/A",
    "description": "bincount(x, /, weights=None, minlength=0)"
  },
  "1988": {
    "name": "blackman",
    "module": "torch.storage.np.lib.nanfunctions.function_base",
    "fullName": "torch.storage.np.lib.nanfunctions.function_base.blackman",
    "signature": "(M)",
    "description": "Return the Blackman window."
  },
  "1989": {
    "name": "concatenate",
    "module": "torch.storage.np.lib.nanfunctions.function_base",
    "fullName": "torch.storage.np.lib.nanfunctions.function_base.concatenate",
    "signature": "N/A",
    "description": "concatenate((a1, a2, ...), axis=0, out=None, dtype=None, casting=\"same_kind\")"
  },
  "1990": {
    "name": "copy",
    "module": "torch.storage.np.lib.nanfunctions.function_base",
    "fullName": "torch.storage.np.lib.nanfunctions.function_base.copy",
    "signature": "(a, order='K', subok=False)",
    "description": "Return an array copy of the given object."
  },
  "1991": {
    "name": "cov",
    "module": "torch.storage.np.lib.nanfunctions.function_base",
    "fullName": "torch.storage.np.lib.nanfunctions.function_base.cov",
    "signature": "(m, y=None, rowvar=True, bias=False, ddof=None, fweights=None, aweights=None, *, dtype=None)",
    "description": "Estimate a covariance matrix, given data and weights."
  },
  "1992": {
    "name": "delete",
    "module": "torch.storage.np.lib.nanfunctions.function_base",
    "fullName": "torch.storage.np.lib.nanfunctions.function_base.delete",
    "signature": "(arr, obj, axis=None)",
    "description": "Return a new array with sub-arrays along an axis deleted. For a one"
  },
  "1993": {
    "name": "diag",
    "module": "torch.storage.np.lib.nanfunctions.function_base",
    "fullName": "torch.storage.np.lib.nanfunctions.function_base.diag",
    "signature": "(v, k=0)",
    "description": "Extract a diagonal or construct a diagonal array."
  },
  "1994": {
    "name": "diff",
    "module": "torch.storage.np.lib.nanfunctions.function_base",
    "fullName": "torch.storage.np.lib.nanfunctions.function_base.diff",
    "signature": "(a, n=1, axis=-1, prepend=<no value>, append=<no value>)",
    "description": "Calculate the n-th discrete difference along the given axis."
  },
  "1995": {
    "name": "digitize",
    "module": "torch.storage.np.lib.nanfunctions.function_base",
    "fullName": "torch.storage.np.lib.nanfunctions.function_base.digitize",
    "signature": "(x, bins, right=False)",
    "description": "Return the indices of the bins to which each value in input array belongs."
  },
  "1996": {
    "name": "disp",
    "module": "torch.storage.np.lib.nanfunctions.function_base",
    "fullName": "torch.storage.np.lib.nanfunctions.function_base.disp",
    "signature": "(mesg, device=None, linefeed=True)",
    "description": "Display a message on a device."
  },
  "1997": {
    "name": "dot",
    "module": "torch.storage.np.lib.nanfunctions.function_base",
    "fullName": "torch.storage.np.lib.nanfunctions.function_base.dot",
    "signature": "N/A",
    "description": "dot(a, b, out=None)"
  },
  "1998": {
    "name": "extract",
    "module": "torch.storage.np.lib.nanfunctions.function_base",
    "fullName": "torch.storage.np.lib.nanfunctions.function_base.extract",
    "signature": "(condition, arr)",
    "description": "Return the elements of an array that satisfy some condition."
  },
  "1999": {
    "name": "flip",
    "module": "torch.storage.np.lib.nanfunctions.function_base",
    "fullName": "torch.storage.np.lib.nanfunctions.function_base.flip",
    "signature": "(m, axis=None)",
    "description": "Reverse the order of elements in an array along the given axis."
  },
  "2000": {
    "name": "gradient",
    "module": "torch.storage.np.lib.nanfunctions.function_base",
    "fullName": "torch.storage.np.lib.nanfunctions.function_base.gradient",
    "signature": "(f, *varargs, axis=None, edge_order=1)",
    "description": "Return the gradient of an N-dimensional array."
  },
  "2001": {
    "name": "hamming",
    "module": "torch.storage.np.lib.nanfunctions.function_base",
    "fullName": "torch.storage.np.lib.nanfunctions.function_base.hamming",
    "signature": "(M)",
    "description": "Return the Hamming window."
  },
  "2002": {
    "name": "hanning",
    "module": "torch.storage.np.lib.nanfunctions.function_base",
    "fullName": "torch.storage.np.lib.nanfunctions.function_base.hanning",
    "signature": "(M)",
    "description": "Return the Hanning window."
  },
  "2003": {
    "name": "histogram",
    "module": "torch.storage.np.lib.nanfunctions.function_base",
    "fullName": "torch.storage.np.lib.nanfunctions.function_base.histogram",
    "signature": "(a, bins=10, range=None, density=None, weights=None)",
    "description": "Compute the histogram of a dataset."
  },
  "2004": {
    "name": "histogramdd",
    "module": "torch.storage.np.lib.nanfunctions.function_base",
    "fullName": "torch.storage.np.lib.nanfunctions.function_base.histogramdd",
    "signature": "(sample, bins=10, range=None, density=None, weights=None)",
    "description": "Compute the multidimensional histogram of some data."
  },
  "2005": {
    "name": "i0",
    "module": "torch.storage.np.lib.nanfunctions.function_base",
    "fullName": "torch.storage.np.lib.nanfunctions.function_base.i0",
    "signature": "(x)",
    "description": "Modified Bessel function of the first kind, order 0."
  },
  "2006": {
    "name": "insert",
    "module": "torch.storage.np.lib.nanfunctions.function_base",
    "fullName": "torch.storage.np.lib.nanfunctions.function_base.insert",
    "signature": "(arr, obj, values, axis=None)",
    "description": "Insert values along the given axis before the given indices."
  },
  "2007": {
    "name": "interp",
    "module": "torch.storage.np.lib.nanfunctions.function_base",
    "fullName": "torch.storage.np.lib.nanfunctions.function_base.interp",
    "signature": "(x, xp, fp, left=None, right=None, period=None)",
    "description": "One-dimensional linear interpolation for monotonically increasing sample points."
  },
  "2008": {
    "name": "isscalar",
    "module": "torch.storage.np.lib.nanfunctions.function_base",
    "fullName": "torch.storage.np.lib.nanfunctions.function_base.isscalar",
    "signature": "(element)",
    "description": "Returns True if the type of `element` is a scalar type."
  },
  "2009": {
    "name": "iterable",
    "module": "torch.storage.np.lib.nanfunctions.function_base",
    "fullName": "torch.storage.np.lib.nanfunctions.function_base.iterable",
    "signature": "(y)",
    "description": "Check whether or not an object can be iterated over."
  },
  "2010": {
    "name": "kaiser",
    "module": "torch.storage.np.lib.nanfunctions.function_base",
    "fullName": "torch.storage.np.lib.nanfunctions.function_base.kaiser",
    "signature": "(M, beta)",
    "description": "Return the Kaiser window."
  },
  "2011": {
    "name": "mean",
    "module": "torch.storage.np.lib.nanfunctions.function_base",
    "fullName": "torch.storage.np.lib.nanfunctions.function_base.mean",
    "signature": "(a, axis=None, dtype=None, out=None, keepdims=<no value>, *, where=<no value>)",
    "description": "Compute the arithmetic mean along the specified axis."
  },
  "2012": {
    "name": "median",
    "module": "torch.storage.np.lib.nanfunctions.function_base",
    "fullName": "torch.storage.np.lib.nanfunctions.function_base.median",
    "signature": "(a, axis=None, out=None, overwrite_input=False, keepdims=False)",
    "description": "Compute the median along the specified axis."
  },
  "2013": {
    "name": "meshgrid",
    "module": "torch.storage.np.lib.nanfunctions.function_base",
    "fullName": "torch.storage.np.lib.nanfunctions.function_base.meshgrid",
    "signature": "(*xi, copy=True, sparse=False, indexing='xy')",
    "description": "Return coordinate matrices from coordinate vectors."
  },
  "2014": {
    "name": "ones",
    "module": "torch.storage.np.lib.nanfunctions.function_base",
    "fullName": "torch.storage.np.lib.nanfunctions.function_base.ones",
    "signature": "(shape, dtype=None, order='C', *, like=None)",
    "description": "Return a new array of given shape and type, filled with ones."
  },
  "2015": {
    "name": "piecewise",
    "module": "torch.storage.np.lib.nanfunctions.function_base",
    "fullName": "torch.storage.np.lib.nanfunctions.function_base.piecewise",
    "signature": "(x, condlist, funclist, *args, **kw)",
    "description": "Evaluate a piecewise-defined function."
  },
  "2016": {
    "name": "place",
    "module": "torch.storage.np.lib.nanfunctions.function_base",
    "fullName": "torch.storage.np.lib.nanfunctions.function_base.place",
    "signature": "(arr, mask, vals)",
    "description": "Change elements of an array based on conditional and input values."
  },
  "2017": {
    "name": "ravel",
    "module": "torch.storage.np.lib.nanfunctions.function_base",
    "fullName": "torch.storage.np.lib.nanfunctions.function_base.ravel",
    "signature": "(a, order='C')",
    "description": "Return a contiguous flattened array."
  },
  "2018": {
    "name": "rot90",
    "module": "torch.storage.np.lib.nanfunctions.function_base",
    "fullName": "torch.storage.np.lib.nanfunctions.function_base.rot90",
    "signature": "(m, k=1, axes=(0, 1))",
    "description": "Rotate an array by 90 degrees in the plane specified by axes."
  },
  "2019": {
    "name": "select",
    "module": "torch.storage.np.lib.nanfunctions.function_base",
    "fullName": "torch.storage.np.lib.nanfunctions.function_base.select",
    "signature": "(condlist, choicelist, default=0)",
    "description": "Return an array drawn from elements in choicelist, depending on conditions."
  },
  "2020": {
    "name": "set_module",
    "module": "torch.storage.np.lib.nanfunctions.function_base",
    "fullName": "torch.storage.np.lib.nanfunctions.function_base.set_module",
    "signature": "(module)",
    "description": "Decorator for overriding __module__ on a function or class."
  },
  "2021": {
    "name": "sinc",
    "module": "torch.storage.np.lib.nanfunctions.function_base",
    "fullName": "torch.storage.np.lib.nanfunctions.function_base.sinc",
    "signature": "(x)",
    "description": "Return the normalized sinc function."
  },
  "2022": {
    "name": "sort_complex",
    "module": "torch.storage.np.lib.nanfunctions.function_base",
    "fullName": "torch.storage.np.lib.nanfunctions.function_base.sort_complex",
    "signature": "(a)",
    "description": "Sort a complex array using the real part first, then the imaginary part."
  },
  "2023": {
    "name": "sum",
    "module": "torch.storage.np.lib.nanfunctions.function_base",
    "fullName": "torch.storage.np.lib.nanfunctions.function_base.sum",
    "signature": "(a, axis=None, dtype=None, out=None, keepdims=<no value>, initial=<no value>, where=<no value>)",
    "description": "Sum of array elements over a given axis."
  },
  "2024": {
    "name": "take",
    "module": "torch.storage.np.lib.nanfunctions.function_base",
    "fullName": "torch.storage.np.lib.nanfunctions.function_base.take",
    "signature": "(a, indices, axis=None, out=None, mode='raise')",
    "description": "Take elements from an array along an axis."
  },
  "2025": {
    "name": "transpose",
    "module": "torch.storage.np.lib.nanfunctions.function_base",
    "fullName": "torch.storage.np.lib.nanfunctions.function_base.transpose",
    "signature": "(a, axes=None)",
    "description": "Returns an array with axes transposed."
  },
  "2026": {
    "name": "trapz",
    "module": "torch.storage.np.lib.nanfunctions.function_base",
    "fullName": "torch.storage.np.lib.nanfunctions.function_base.trapz",
    "signature": "(y, x=None, dx=1.0, axis=-1)",
    "description": "Integrate along the given axis using the composite trapezoidal rule."
  },
  "2027": {
    "name": "trim_zeros",
    "module": "torch.storage.np.lib.nanfunctions.function_base",
    "fullName": "torch.storage.np.lib.nanfunctions.function_base.trim_zeros",
    "signature": "(filt, trim='fb')",
    "description": "Trim the leading and/or trailing zeros from a 1-D array or sequence."
  },
  "2028": {
    "name": "unwrap",
    "module": "torch.storage.np.lib.nanfunctions.function_base",
    "fullName": "torch.storage.np.lib.nanfunctions.function_base.unwrap",
    "signature": "(p, discont=None, axis=-1, *, period=6.283185307179586)",
    "description": "Unwrap by taking the complement of large deltas with respect to the period."
  },
  "2029": {
    "name": "where",
    "module": "torch.storage.np.lib.nanfunctions.function_base",
    "fullName": "torch.storage.np.lib.nanfunctions.function_base.where",
    "signature": "N/A",
    "description": "where(condition, [x, y], /)"
  },
  "2030": {
    "name": "zeros_like",
    "module": "torch.storage.np.lib.nanfunctions.function_base",
    "fullName": "torch.storage.np.lib.nanfunctions.function_base.zeros_like",
    "signature": "(a, dtype=None, order='K', subok=True, shape=None)",
    "description": "Return an array of zeros with the same shape and type as a given array."
  },
  "2031": {
    "name": "alltrue",
    "module": "torch.storage.np.lib.index_tricks",
    "fullName": "torch.storage.np.lib.index_tricks.alltrue",
    "signature": "(*args, **kwargs)",
    "description": "Check if all elements of input array are true."
  },
  "2032": {
    "name": "as_strided",
    "module": "torch.storage.np.lib.index_tricks",
    "fullName": "torch.storage.np.lib.index_tricks.as_strided",
    "signature": "(x, shape=None, strides=None, subok=False, writeable=True)",
    "description": "Create a view into the array with the given shape and strides."
  },
  "2033": {
    "name": "cumprod",
    "module": "torch.storage.np.lib.index_tricks",
    "fullName": "torch.storage.np.lib.index_tricks.cumprod",
    "signature": "(a, axis=None, dtype=None, out=None)",
    "description": "Return the cumulative product of elements along a given axis."
  },
  "2034": {
    "name": "diag_indices",
    "module": "torch.storage.np.lib.index_tricks",
    "fullName": "torch.storage.np.lib.index_tricks.diag_indices",
    "signature": "(n, ndim=2)",
    "description": "Return the indices to access the main diagonal of an array."
  },
  "2035": {
    "name": "diag_indices_from",
    "module": "torch.storage.np.lib.index_tricks",
    "fullName": "torch.storage.np.lib.index_tricks.diag_indices_from",
    "signature": "(arr)",
    "description": "Return the indices to access the main diagonal of an n-dimensional array."
  },
  "2036": {
    "name": "diff",
    "module": "torch.storage.np.lib.index_tricks",
    "fullName": "torch.storage.np.lib.index_tricks.diff",
    "signature": "(a, n=1, axis=-1, prepend=<no value>, append=<no value>)",
    "description": "Calculate the n-th discrete difference along the given axis."
  },
  "2037": {
    "name": "fill_diagonal",
    "module": "torch.storage.np.lib.index_tricks",
    "fullName": "torch.storage.np.lib.index_tricks.fill_diagonal",
    "signature": "(a, val, wrap=False)",
    "description": "Fill the main diagonal of the given array of any dimensionality."
  },
  "2038": {
    "name": "find_common_type",
    "module": "torch.storage.np.lib.index_tricks",
    "fullName": "torch.storage.np.lib.index_tricks.find_common_type",
    "signature": "(array_types, scalar_types)",
    "description": "Determine common type following standard coercion rules."
  },
  "2039": {
    "name": "issubdtype",
    "module": "torch.storage.np.lib.index_tricks",
    "fullName": "torch.storage.np.lib.index_tricks.issubdtype",
    "signature": "(arg1, arg2)",
    "description": "Returns True if first argument is a typecode lower/equal in type hierarchy."
  },
  "2040": {
    "name": "ix_",
    "module": "torch.storage.np.lib.index_tricks",
    "fullName": "torch.storage.np.lib.index_tricks.ix_",
    "signature": "(*args)",
    "description": "Construct an open mesh from multiple sequences."
  },
  "2041": {
    "name": "linspace",
    "module": "torch.storage.np.lib.index_tricks",
    "fullName": "torch.storage.np.lib.index_tricks.linspace",
    "signature": "(start, stop, num=50, endpoint=True, retstep=False, dtype=None, axis=0)",
    "description": "Return evenly spaced numbers over a specified interval."
  },
  "2042": {
    "name": "ndim",
    "module": "torch.storage.np.lib.index_tricks",
    "fullName": "torch.storage.np.lib.index_tricks.ndim",
    "signature": "(a)",
    "description": "Return the number of dimensions of an array."
  },
  "2043": {
    "name": "ravel_multi_index",
    "module": "torch.storage.np.lib.index_tricks",
    "fullName": "torch.storage.np.lib.index_tricks.ravel_multi_index",
    "signature": "N/A",
    "description": "ravel_multi_index(multi_index, dims, mode='raise', order='C')"
  },
  "2044": {
    "name": "set_module",
    "module": "torch.storage.np.lib.index_tricks",
    "fullName": "torch.storage.np.lib.index_tricks.set_module",
    "signature": "(module)",
    "description": "Decorator for overriding __module__ on a function or class."
  },
  "2045": {
    "name": "unravel_index",
    "module": "torch.storage.np.lib.index_tricks",
    "fullName": "torch.storage.np.lib.index_tricks.unravel_index",
    "signature": "N/A",
    "description": "unravel_index(indices, shape, order='C')"
  },
  "2046": {
    "name": "asmatrix",
    "module": "torch.storage.np.lib.index_tricks.matrixlib",
    "fullName": "torch.storage.np.lib.index_tricks.matrixlib.asmatrix",
    "signature": "(data, dtype=None)",
    "description": "Interpret the input as a matrix."
  },
  "2047": {
    "name": "bmat",
    "module": "torch.storage.np.lib.index_tricks.matrixlib",
    "fullName": "torch.storage.np.lib.index_tricks.matrixlib.bmat",
    "signature": "(obj, ldict=None, gdict=None)",
    "description": "Build a matrix object from a string, nested sequence, or array."
  },
  "2048": {
    "name": "mat",
    "module": "torch.storage.np.lib.index_tricks.matrixlib",
    "fullName": "torch.storage.np.lib.index_tricks.matrixlib.mat",
    "signature": "(data, dtype=None)",
    "description": "Interpret the input as a matrix."
  },
  "2049": {
    "name": "asmatrix",
    "module": "torch.storage.np.lib.index_tricks.matrixlib.defmatrix",
    "fullName": "torch.storage.np.lib.index_tricks.matrixlib.defmatrix.asmatrix",
    "signature": "(data, dtype=None)",
    "description": "Interpret the input as a matrix."
  },
  "2050": {
    "name": "bmat",
    "module": "torch.storage.np.lib.index_tricks.matrixlib.defmatrix",
    "fullName": "torch.storage.np.lib.index_tricks.matrixlib.defmatrix.bmat",
    "signature": "(obj, ldict=None, gdict=None)",
    "description": "Build a matrix object from a string, nested sequence, or array."
  },
  "2051": {
    "name": "concatenate",
    "module": "torch.storage.np.lib.index_tricks.matrixlib.defmatrix",
    "fullName": "torch.storage.np.lib.index_tricks.matrixlib.defmatrix.concatenate",
    "signature": "N/A",
    "description": "concatenate((a1, a2, ...), axis=0, out=None, dtype=None, casting=\"same_kind\")"
  },
  "2052": {
    "name": "isscalar",
    "module": "torch.storage.np.lib.index_tricks.matrixlib.defmatrix",
    "fullName": "torch.storage.np.lib.index_tricks.matrixlib.defmatrix.isscalar",
    "signature": "(element)",
    "description": "Returns True if the type of `element` is a scalar type."
  },
  "2053": {
    "name": "mat",
    "module": "torch.storage.np.lib.index_tricks.matrixlib.defmatrix",
    "fullName": "torch.storage.np.lib.index_tricks.matrixlib.defmatrix.mat",
    "signature": "(data, dtype=None)",
    "description": "Interpret the input as a matrix."
  },
  "2054": {
    "name": "matrix_power",
    "module": "torch.storage.np.lib.index_tricks.matrixlib.defmatrix",
    "fullName": "torch.storage.np.lib.index_tricks.matrixlib.defmatrix.matrix_power",
    "signature": "(a, n)",
    "description": "Raise a square matrix to the (integer) power `n`."
  },
  "2055": {
    "name": "set_module",
    "module": "torch.storage.np.lib.index_tricks.matrixlib.defmatrix",
    "fullName": "torch.storage.np.lib.index_tricks.matrixlib.defmatrix.set_module",
    "signature": "(module)",
    "description": "Decorator for overriding __module__ on a function or class."
  },
  "2056": {
    "name": "histogram",
    "module": "torch.storage.np.lib.histograms",
    "fullName": "torch.storage.np.lib.histograms.histogram",
    "signature": "(a, bins=10, range=None, density=None, weights=None)",
    "description": "Compute the histogram of a dataset."
  },
  "2057": {
    "name": "histogram_bin_edges",
    "module": "torch.storage.np.lib.histograms",
    "fullName": "torch.storage.np.lib.histograms.histogram_bin_edges",
    "signature": "(a, bins=10, range=None, weights=None)",
    "description": "Function to calculate only the edges of the bins used by the `histogram`"
  },
  "2058": {
    "name": "histogramdd",
    "module": "torch.storage.np.lib.histograms",
    "fullName": "torch.storage.np.lib.histograms.histogramdd",
    "signature": "(sample, bins=10, range=None, density=None, weights=None)",
    "description": "Compute the multidimensional histogram of some data."
  },
  "2059": {
    "name": "ediff1d",
    "module": "torch.storage.np.lib.arraysetops",
    "fullName": "torch.storage.np.lib.arraysetops.ediff1d",
    "signature": "(ary, to_end=None, to_begin=None)",
    "description": "The differences between consecutive elements of an array."
  },
  "2060": {
    "name": "in1d",
    "module": "torch.storage.np.lib.arraysetops",
    "fullName": "torch.storage.np.lib.arraysetops.in1d",
    "signature": "(ar1, ar2, assume_unique=False, invert=False, *, kind=None)",
    "description": "Test whether each element of a 1-D array is also present in a second array."
  },
  "2061": {
    "name": "intersect1d",
    "module": "torch.storage.np.lib.arraysetops",
    "fullName": "torch.storage.np.lib.arraysetops.intersect1d",
    "signature": "(ar1, ar2, assume_unique=False, return_indices=False)",
    "description": "Find the intersection of two arrays."
  },
  "2062": {
    "name": "isin",
    "module": "torch.storage.np.lib.arraysetops",
    "fullName": "torch.storage.np.lib.arraysetops.isin",
    "signature": "(element, test_elements, assume_unique=False, invert=False, *, kind=None)",
    "description": "Calculates ``element in test_elements``, broadcasting over `element` only."
  },
  "2063": {
    "name": "setdiff1d",
    "module": "torch.storage.np.lib.arraysetops",
    "fullName": "torch.storage.np.lib.arraysetops.setdiff1d",
    "signature": "(ar1, ar2, assume_unique=False)",
    "description": "Find the set difference of two arrays."
  },
  "2064": {
    "name": "setxor1d",
    "module": "torch.storage.np.lib.arraysetops",
    "fullName": "torch.storage.np.lib.arraysetops.setxor1d",
    "signature": "(ar1, ar2, assume_unique=False)",
    "description": "Find the set exclusive-or of two arrays."
  },
  "2065": {
    "name": "union1d",
    "module": "torch.storage.np.lib.arraysetops",
    "fullName": "torch.storage.np.lib.arraysetops.union1d",
    "signature": "(ar1, ar2)",
    "description": "Find the union of two arrays."
  },
  "2066": {
    "name": "unique",
    "module": "torch.storage.np.lib.arraysetops",
    "fullName": "torch.storage.np.lib.arraysetops.unique",
    "signature": "(ar, return_index=False, return_inverse=False, return_counts=False, axis=None, *, equal_nan=True)",
    "description": "Find the unique elements of an array."
  },
  "2067": {
    "name": "array_function_dispatch",
    "module": "torch.storage.np.lib.arraypad",
    "fullName": "torch.storage.np.lib.arraypad.array_function_dispatch",
    "signature": "(dispatcher, module=None, verify=True, docs_from_dispatcher=False, use_like=False)",
    "description": "Decorator for adding dispatch with the __array_function__ protocol."
  },
  "2068": {
    "name": "pad",
    "module": "torch.storage.np.lib.arraypad",
    "fullName": "torch.storage.np.lib.arraypad.pad",
    "signature": "(array, pad_width, mode='constant', **kwargs)",
    "description": "Pad an array."
  },
  "2069": {
    "name": "fft",
    "module": "torch.storage.np.fft",
    "fullName": "torch.storage.np.fft.fft",
    "signature": "(a, n=None, axis=-1, norm=None)",
    "description": "Compute the one-dimensional discrete Fourier Transform."
  },
  "2070": {
    "name": "fft2",
    "module": "torch.storage.np.fft",
    "fullName": "torch.storage.np.fft.fft2",
    "signature": "(a, s=None, axes=(-2, -1), norm=None)",
    "description": "Compute the 2-dimensional discrete Fourier Transform."
  },
  "2071": {
    "name": "fftfreq",
    "module": "torch.storage.np.fft",
    "fullName": "torch.storage.np.fft.fftfreq",
    "signature": "(n, d=1.0)",
    "description": "Return the Discrete Fourier Transform sample frequencies."
  },
  "2072": {
    "name": "fftn",
    "module": "torch.storage.np.fft",
    "fullName": "torch.storage.np.fft.fftn",
    "signature": "(a, s=None, axes=None, norm=None)",
    "description": "Compute the N-dimensional discrete Fourier Transform."
  },
  "2073": {
    "name": "fftshift",
    "module": "torch.storage.np.fft",
    "fullName": "torch.storage.np.fft.fftshift",
    "signature": "(x, axes=None)",
    "description": "Shift the zero-frequency component to the center of the spectrum."
  },
  "2074": {
    "name": "hfft",
    "module": "torch.storage.np.fft",
    "fullName": "torch.storage.np.fft.hfft",
    "signature": "(a, n=None, axis=-1, norm=None)",
    "description": "Compute the FFT of a signal that has Hermitian symmetry, i.e., a real"
  },
  "2075": {
    "name": "ifft",
    "module": "torch.storage.np.fft",
    "fullName": "torch.storage.np.fft.ifft",
    "signature": "(a, n=None, axis=-1, norm=None)",
    "description": "Compute the one-dimensional inverse discrete Fourier Transform."
  },
  "2076": {
    "name": "ifft2",
    "module": "torch.storage.np.fft",
    "fullName": "torch.storage.np.fft.ifft2",
    "signature": "(a, s=None, axes=(-2, -1), norm=None)",
    "description": "Compute the 2-dimensional inverse discrete Fourier Transform."
  },
  "2077": {
    "name": "ifftn",
    "module": "torch.storage.np.fft",
    "fullName": "torch.storage.np.fft.ifftn",
    "signature": "(a, s=None, axes=None, norm=None)",
    "description": "Compute the N-dimensional inverse discrete Fourier Transform."
  },
  "2078": {
    "name": "ifftshift",
    "module": "torch.storage.np.fft",
    "fullName": "torch.storage.np.fft.ifftshift",
    "signature": "(x, axes=None)",
    "description": "The inverse of `fftshift`. Although identical for even-length `x`, the"
  },
  "2079": {
    "name": "ihfft",
    "module": "torch.storage.np.fft",
    "fullName": "torch.storage.np.fft.ihfft",
    "signature": "(a, n=None, axis=-1, norm=None)",
    "description": "Compute the inverse FFT of a signal that has Hermitian symmetry."
  },
  "2080": {
    "name": "irfft",
    "module": "torch.storage.np.fft",
    "fullName": "torch.storage.np.fft.irfft",
    "signature": "(a, n=None, axis=-1, norm=None)",
    "description": "Computes the inverse of `rfft`."
  },
  "2081": {
    "name": "irfft2",
    "module": "torch.storage.np.fft",
    "fullName": "torch.storage.np.fft.irfft2",
    "signature": "(a, s=None, axes=(-2, -1), norm=None)",
    "description": "Computes the inverse of `rfft2`."
  },
  "2082": {
    "name": "irfftn",
    "module": "torch.storage.np.fft",
    "fullName": "torch.storage.np.fft.irfftn",
    "signature": "(a, s=None, axes=None, norm=None)",
    "description": "Computes the inverse of `rfftn`."
  },
  "2083": {
    "name": "rfft",
    "module": "torch.storage.np.fft",
    "fullName": "torch.storage.np.fft.rfft",
    "signature": "(a, n=None, axis=-1, norm=None)",
    "description": "Compute the one-dimensional discrete Fourier Transform for real input."
  },
  "2084": {
    "name": "rfft2",
    "module": "torch.storage.np.fft",
    "fullName": "torch.storage.np.fft.rfft2",
    "signature": "(a, s=None, axes=(-2, -1), norm=None)",
    "description": "Compute the 2-dimensional FFT of a real array."
  },
  "2085": {
    "name": "rfftfreq",
    "module": "torch.storage.np.fft",
    "fullName": "torch.storage.np.fft.rfftfreq",
    "signature": "(n, d=1.0)",
    "description": "Return the Discrete Fourier Transform sample frequencies"
  },
  "2086": {
    "name": "rfftn",
    "module": "torch.storage.np.fft",
    "fullName": "torch.storage.np.fft.rfftn",
    "signature": "(a, s=None, axes=None, norm=None)",
    "description": "Compute the N-dimensional discrete Fourier Transform for real input."
  },
  "2087": {
    "name": "array_function_dispatch",
    "module": "torch.storage.np.fft.helper",
    "fullName": "torch.storage.np.fft.helper.array_function_dispatch",
    "signature": "(dispatcher, module=None, verify=True, docs_from_dispatcher=False, use_like=False)",
    "description": "Decorator for adding dispatch with the __array_function__ protocol."
  },
  "2088": {
    "name": "fftfreq",
    "module": "torch.storage.np.fft.helper",
    "fullName": "torch.storage.np.fft.helper.fftfreq",
    "signature": "(n, d=1.0)",
    "description": "Return the Discrete Fourier Transform sample frequencies."
  },
  "2089": {
    "name": "fftshift",
    "module": "torch.storage.np.fft.helper",
    "fullName": "torch.storage.np.fft.helper.fftshift",
    "signature": "(x, axes=None)",
    "description": "Shift the zero-frequency component to the center of the spectrum."
  },
  "2090": {
    "name": "ifftshift",
    "module": "torch.storage.np.fft.helper",
    "fullName": "torch.storage.np.fft.helper.ifftshift",
    "signature": "(x, axes=None)",
    "description": "The inverse of `fftshift`. Although identical for even-length `x`, the"
  },
  "2091": {
    "name": "rfftfreq",
    "module": "torch.storage.np.fft.helper",
    "fullName": "torch.storage.np.fft.helper.rfftfreq",
    "signature": "(n, d=1.0)",
    "description": "Return the Discrete Fourier Transform sample frequencies"
  },
  "2092": {
    "name": "roll",
    "module": "torch.storage.np.fft.helper",
    "fullName": "torch.storage.np.fft.helper.roll",
    "signature": "(a, shift, axis=None)",
    "description": "Roll array elements along a given axis."
  },
  "2093": {
    "name": "set_module",
    "module": "torch.storage.np.fft.helper",
    "fullName": "torch.storage.np.fft.helper.set_module",
    "signature": "(module)",
    "description": "Decorator for overriding __module__ on a function or class."
  },
  "2094": {
    "name": "as_array",
    "module": "torch.storage.np.ctypeslib",
    "fullName": "torch.storage.np.ctypeslib.as_array",
    "signature": "(obj, shape=None)",
    "description": "Create a numpy array from a ctypes array or POINTER."
  },
  "2095": {
    "name": "as_ctypes",
    "module": "torch.storage.np.ctypeslib",
    "fullName": "torch.storage.np.ctypeslib.as_ctypes",
    "signature": "(obj)",
    "description": "Create and return a ctypes object from a numpy array.  Actually"
  },
  "2096": {
    "name": "as_ctypes_type",
    "module": "torch.storage.np.ctypeslib",
    "fullName": "torch.storage.np.ctypeslib.as_ctypes_type",
    "signature": "(dtype)",
    "description": "Convert a dtype into a ctypes type."
  },
  "2097": {
    "name": "load_library",
    "module": "torch.storage.np.ctypeslib",
    "fullName": "torch.storage.np.ctypeslib.load_library",
    "signature": "(libname, loader_path)",
    "description": "It is possible to load a library using"
  },
  "2098": {
    "name": "ndpointer",
    "module": "torch.storage.np.ctypeslib",
    "fullName": "torch.storage.np.ctypeslib.ndpointer",
    "signature": "(dtype=None, ndim=None, shape=None, flags=None)",
    "description": "Array-checking restype/argtypes."
  },
  "2099": {
    "name": "ARRAY",
    "module": "torch.storage.np.ctypeslib.ctypes",
    "fullName": "torch.storage.np.ctypeslib.ctypes.ARRAY",
    "signature": "(typ, len)",
    "description": "No description available."
  },
  "2100": {
    "name": "CFUNCTYPE",
    "module": "torch.storage.np.ctypeslib.ctypes",
    "fullName": "torch.storage.np.ctypeslib.ctypes.CFUNCTYPE",
    "signature": "(restype, *argtypes, **kw)",
    "description": "CFUNCTYPE(restype, *argtypes,"
  },
  "2101": {
    "name": "PYFUNCTYPE",
    "module": "torch.storage.np.ctypeslib.ctypes",
    "fullName": "torch.storage.np.ctypeslib.ctypes.PYFUNCTYPE",
    "signature": "(restype, *argtypes)",
    "description": "No description available."
  },
  "2102": {
    "name": "SetPointerType",
    "module": "torch.storage.np.ctypeslib.ctypes",
    "fullName": "torch.storage.np.ctypeslib.ctypes.SetPointerType",
    "signature": "(pointer, cls)",
    "description": "No description available."
  },
  "2103": {
    "name": "c_buffer",
    "module": "torch.storage.np.ctypeslib.ctypes",
    "fullName": "torch.storage.np.ctypeslib.ctypes.c_buffer",
    "signature": "(init, size=None)",
    "description": "No description available."
  },
  "2104": {
    "name": "cast",
    "module": "torch.storage.np.ctypeslib.ctypes",
    "fullName": "torch.storage.np.ctypeslib.ctypes.cast",
    "signature": "(obj, typ)",
    "description": "No description available."
  },
  "2105": {
    "name": "create_string_buffer",
    "module": "torch.storage.np.ctypeslib.ctypes",
    "fullName": "torch.storage.np.ctypeslib.ctypes.create_string_buffer",
    "signature": "(init, size=None)",
    "description": "create_string_buffer(aBytes) -> character array"
  },
  "2106": {
    "name": "create_unicode_buffer",
    "module": "torch.storage.np.ctypeslib.ctypes",
    "fullName": "torch.storage.np.ctypeslib.ctypes.create_unicode_buffer",
    "signature": "(init, size=None)",
    "description": "create_unicode_buffer(aString) -> character array"
  },
  "2107": {
    "name": "string_at",
    "module": "torch.storage.np.ctypeslib.ctypes",
    "fullName": "torch.storage.np.ctypeslib.ctypes.string_at",
    "signature": "(ptr, size=-1)",
    "description": "string_at(addr[, size]) -> string"
  },
  "2108": {
    "name": "wstring_at",
    "module": "torch.storage.np.ctypeslib.ctypes",
    "fullName": "torch.storage.np.ctypeslib.ctypes.wstring_at",
    "signature": "(ptr, size=-1)",
    "description": "wstring_at(addr[, size]) -> string"
  },
  "2109": {
    "name": "asbytes",
    "module": "torch.storage.np.compat",
    "fullName": "torch.storage.np.compat.asbytes",
    "signature": "(s)",
    "description": "No description available."
  },
  "2110": {
    "name": "asbytes_nested",
    "module": "torch.storage.np.compat",
    "fullName": "torch.storage.np.compat.asbytes_nested",
    "signature": "(x)",
    "description": "No description available."
  },
  "2111": {
    "name": "asstr",
    "module": "torch.storage.np.compat",
    "fullName": "torch.storage.np.compat.asstr",
    "signature": "(s)",
    "description": "No description available."
  },
  "2112": {
    "name": "asunicode",
    "module": "torch.storage.np.compat",
    "fullName": "torch.storage.np.compat.asunicode",
    "signature": "(s)",
    "description": "No description available."
  },
  "2113": {
    "name": "asunicode_nested",
    "module": "torch.storage.np.compat",
    "fullName": "torch.storage.np.compat.asunicode_nested",
    "signature": "(x)",
    "description": "No description available."
  },
  "2114": {
    "name": "formatargspec",
    "module": "torch.storage.np.compat",
    "fullName": "torch.storage.np.compat.formatargspec",
    "signature": "(args, varargs=None, varkw=None, defaults=None, formatarg=<class 'str'>, formatvarargs=<function <lambda> at 0x7fce232ebf70>, formatvarkw=<function <lambda> at 0x7fce232ebca0>, formatvalue=<function <lambda> at 0x7fce232ebdc0>, join=<function joinseq at 0x7fce232eb3a0>)",
    "description": "Format an argument spec from the 4 values returned by getargspec."
  },
  "2115": {
    "name": "getargspec",
    "module": "torch.storage.np.compat",
    "fullName": "torch.storage.np.compat.getargspec",
    "signature": "(func)",
    "description": "Get the names and default values of a function's arguments."
  },
  "2116": {
    "name": "getexception",
    "module": "torch.storage.np.compat",
    "fullName": "torch.storage.np.compat.getexception",
    "signature": "()",
    "description": "No description available."
  },
  "2117": {
    "name": "is_pathlib_path",
    "module": "torch.storage.np.compat",
    "fullName": "torch.storage.np.compat.is_pathlib_path",
    "signature": "(obj)",
    "description": "Check whether obj is a `pathlib.Path` object."
  },
  "2118": {
    "name": "isfileobj",
    "module": "torch.storage.np.compat",
    "fullName": "torch.storage.np.compat.isfileobj",
    "signature": "(f)",
    "description": "No description available."
  },
  "2119": {
    "name": "open_latin1",
    "module": "torch.storage.np.compat",
    "fullName": "torch.storage.np.compat.open_latin1",
    "signature": "(filename, mode='r')",
    "description": "No description available."
  },
  "2120": {
    "name": "sixu",
    "module": "torch.storage.np.compat",
    "fullName": "torch.storage.np.compat.sixu",
    "signature": "(s)",
    "description": "No description available."
  },
  "2121": {
    "name": "asbytes",
    "module": "torch.storage.np.compat.py3k",
    "fullName": "torch.storage.np.compat.py3k.asbytes",
    "signature": "(s)",
    "description": "No description available."
  },
  "2122": {
    "name": "asbytes_nested",
    "module": "torch.storage.np.compat.py3k",
    "fullName": "torch.storage.np.compat.py3k.asbytes_nested",
    "signature": "(x)",
    "description": "No description available."
  },
  "2123": {
    "name": "asstr",
    "module": "torch.storage.np.compat.py3k",
    "fullName": "torch.storage.np.compat.py3k.asstr",
    "signature": "(s)",
    "description": "No description available."
  },
  "2124": {
    "name": "asunicode",
    "module": "torch.storage.np.compat.py3k",
    "fullName": "torch.storage.np.compat.py3k.asunicode",
    "signature": "(s)",
    "description": "No description available."
  },
  "2125": {
    "name": "asunicode_nested",
    "module": "torch.storage.np.compat.py3k",
    "fullName": "torch.storage.np.compat.py3k.asunicode_nested",
    "signature": "(x)",
    "description": "No description available."
  },
  "2126": {
    "name": "getexception",
    "module": "torch.storage.np.compat.py3k",
    "fullName": "torch.storage.np.compat.py3k.getexception",
    "signature": "()",
    "description": "No description available."
  },
  "2127": {
    "name": "is_pathlib_path",
    "module": "torch.storage.np.compat.py3k",
    "fullName": "torch.storage.np.compat.py3k.is_pathlib_path",
    "signature": "(obj)",
    "description": "Check whether obj is a `pathlib.Path` object."
  },
  "2128": {
    "name": "isfileobj",
    "module": "torch.storage.np.compat.py3k",
    "fullName": "torch.storage.np.compat.py3k.isfileobj",
    "signature": "(f)",
    "description": "No description available."
  },
  "2129": {
    "name": "open_latin1",
    "module": "torch.storage.np.compat.py3k",
    "fullName": "torch.storage.np.compat.py3k.open_latin1",
    "signature": "(filename, mode='r')",
    "description": "No description available."
  },
  "2130": {
    "name": "sixu",
    "module": "torch.storage.np.compat.py3k",
    "fullName": "torch.storage.np.compat.py3k.sixu",
    "signature": "(s)",
    "description": "No description available."
  },
  "2131": {
    "name": "add",
    "module": "torch.storage.np.char",
    "fullName": "torch.storage.np.char.add",
    "signature": "(x1, x2)",
    "description": "Return element-wise string concatenation for two arrays of str or unicode."
  },
  "2132": {
    "name": "array",
    "module": "torch.storage.np.char",
    "fullName": "torch.storage.np.char.array",
    "signature": "(obj, itemsize=None, copy=True, unicode=None, order=None)",
    "description": "Create a `chararray`."
  },
  "2133": {
    "name": "asarray",
    "module": "torch.storage.np.char",
    "fullName": "torch.storage.np.char.asarray",
    "signature": "(obj, itemsize=None, unicode=None, order=None)",
    "description": "Convert the input to a `chararray`, copying the data only if"
  },
  "2134": {
    "name": "asbytes",
    "module": "torch.storage.np.char",
    "fullName": "torch.storage.np.char.asbytes",
    "signature": "(s)",
    "description": "No description available."
  },
  "2135": {
    "name": "capitalize",
    "module": "torch.storage.np.char",
    "fullName": "torch.storage.np.char.capitalize",
    "signature": "(a)",
    "description": "Return a copy of `a` with only the first character of each element"
  },
  "2136": {
    "name": "center",
    "module": "torch.storage.np.char",
    "fullName": "torch.storage.np.char.center",
    "signature": "(a, width, fillchar=' ')",
    "description": "Return a copy of `a` with its elements centered in a string of"
  },
  "2137": {
    "name": "count",
    "module": "torch.storage.np.char",
    "fullName": "torch.storage.np.char.count",
    "signature": "(a, sub, start=0, end=None)",
    "description": "Returns an array with the number of non-overlapping occurrences of"
  },
  "2138": {
    "name": "decode",
    "module": "torch.storage.np.char",
    "fullName": "torch.storage.np.char.decode",
    "signature": "(a, encoding=None, errors=None)",
    "description": "Calls ``bytes.decode`` element-wise."
  },
  "2139": {
    "name": "encode",
    "module": "torch.storage.np.char",
    "fullName": "torch.storage.np.char.encode",
    "signature": "(a, encoding=None, errors=None)",
    "description": "Calls `str.encode` element-wise."
  },
  "2140": {
    "name": "endswith",
    "module": "torch.storage.np.char",
    "fullName": "torch.storage.np.char.endswith",
    "signature": "(a, suffix, start=0, end=None)",
    "description": "Returns a boolean array which is `True` where the string element"
  },
  "2141": {
    "name": "equal",
    "module": "torch.storage.np.char",
    "fullName": "torch.storage.np.char.equal",
    "signature": "(x1, x2)",
    "description": "Return (x1 == x2) element-wise."
  },
  "2142": {
    "name": "expandtabs",
    "module": "torch.storage.np.char",
    "fullName": "torch.storage.np.char.expandtabs",
    "signature": "(a, tabsize=8)",
    "description": "Return a copy of each string element where all tab characters are"
  },
  "2143": {
    "name": "find",
    "module": "torch.storage.np.char",
    "fullName": "torch.storage.np.char.find",
    "signature": "(a, sub, start=0, end=None)",
    "description": "For each element, return the lowest index in the string where"
  },
  "2144": {
    "name": "greater",
    "module": "torch.storage.np.char",
    "fullName": "torch.storage.np.char.greater",
    "signature": "(x1, x2)",
    "description": "Return (x1 > x2) element-wise."
  },
  "2145": {
    "name": "greater_equal",
    "module": "torch.storage.np.char",
    "fullName": "torch.storage.np.char.greater_equal",
    "signature": "(x1, x2)",
    "description": "Return (x1 >= x2) element-wise."
  },
  "2146": {
    "name": "index",
    "module": "torch.storage.np.char",
    "fullName": "torch.storage.np.char.index",
    "signature": "(a, sub, start=0, end=None)",
    "description": "Like `find`, but raises `ValueError` when the substring is not found."
  },
  "2147": {
    "name": "isalnum",
    "module": "torch.storage.np.char",
    "fullName": "torch.storage.np.char.isalnum",
    "signature": "(a)",
    "description": "Returns true for each element if all characters in the string are"
  },
  "2148": {
    "name": "isalpha",
    "module": "torch.storage.np.char",
    "fullName": "torch.storage.np.char.isalpha",
    "signature": "(a)",
    "description": "Returns true for each element if all characters in the string are"
  },
  "2149": {
    "name": "isdecimal",
    "module": "torch.storage.np.char",
    "fullName": "torch.storage.np.char.isdecimal",
    "signature": "(a)",
    "description": "For each element, return True if there are only decimal"
  },
  "2150": {
    "name": "isdigit",
    "module": "torch.storage.np.char",
    "fullName": "torch.storage.np.char.isdigit",
    "signature": "(a)",
    "description": "Returns true for each element if all characters in the string are"
  },
  "2151": {
    "name": "islower",
    "module": "torch.storage.np.char",
    "fullName": "torch.storage.np.char.islower",
    "signature": "(a)",
    "description": "Returns true for each element if all cased characters in the"
  },
  "2152": {
    "name": "isnumeric",
    "module": "torch.storage.np.char",
    "fullName": "torch.storage.np.char.isnumeric",
    "signature": "(a)",
    "description": "For each element, return True if there are only numeric"
  },
  "2153": {
    "name": "isspace",
    "module": "torch.storage.np.char",
    "fullName": "torch.storage.np.char.isspace",
    "signature": "(a)",
    "description": "Returns true for each element if there are only whitespace"
  },
  "2154": {
    "name": "istitle",
    "module": "torch.storage.np.char",
    "fullName": "torch.storage.np.char.istitle",
    "signature": "(a)",
    "description": "Returns true for each element if the element is a titlecased"
  },
  "2155": {
    "name": "isupper",
    "module": "torch.storage.np.char",
    "fullName": "torch.storage.np.char.isupper",
    "signature": "(a)",
    "description": "Return true for each element if all cased characters in the"
  },
  "2156": {
    "name": "join",
    "module": "torch.storage.np.char",
    "fullName": "torch.storage.np.char.join",
    "signature": "(sep, seq)",
    "description": "Return a string which is the concatenation of the strings in the"
  },
  "2157": {
    "name": "less",
    "module": "torch.storage.np.char",
    "fullName": "torch.storage.np.char.less",
    "signature": "(x1, x2)",
    "description": "Return (x1 < x2) element-wise."
  },
  "2158": {
    "name": "less_equal",
    "module": "torch.storage.np.char",
    "fullName": "torch.storage.np.char.less_equal",
    "signature": "(x1, x2)",
    "description": "Return (x1 <= x2) element-wise."
  },
  "2159": {
    "name": "ljust",
    "module": "torch.storage.np.char",
    "fullName": "torch.storage.np.char.ljust",
    "signature": "(a, width, fillchar=' ')",
    "description": "Return an array with the elements of `a` left-justified in a"
  },
  "2160": {
    "name": "lower",
    "module": "torch.storage.np.char",
    "fullName": "torch.storage.np.char.lower",
    "signature": "(a)",
    "description": "Return an array with the elements converted to lowercase."
  },
  "2161": {
    "name": "lstrip",
    "module": "torch.storage.np.char",
    "fullName": "torch.storage.np.char.lstrip",
    "signature": "(a, chars=None)",
    "description": "For each element in `a`, return a copy with the leading characters"
  },
  "2162": {
    "name": "mod",
    "module": "torch.storage.np.char",
    "fullName": "torch.storage.np.char.mod",
    "signature": "(a, values)",
    "description": "Return (a % i), that is pre-Python 2.6 string formatting"
  },
  "2163": {
    "name": "multiply",
    "module": "torch.storage.np.char",
    "fullName": "torch.storage.np.char.multiply",
    "signature": "(a, i)",
    "description": "Return (a * i), that is string multiple concatenation,"
  },
  "2164": {
    "name": "not_equal",
    "module": "torch.storage.np.char",
    "fullName": "torch.storage.np.char.not_equal",
    "signature": "(x1, x2)",
    "description": "Return (x1 != x2) element-wise."
  },
  "2165": {
    "name": "partition",
    "module": "torch.storage.np.char",
    "fullName": "torch.storage.np.char.partition",
    "signature": "(a, sep)",
    "description": "Partition each element in `a` around `sep`."
  },
  "2166": {
    "name": "replace",
    "module": "torch.storage.np.char",
    "fullName": "torch.storage.np.char.replace",
    "signature": "(a, old, new, count=None)",
    "description": "For each element in `a`, return a copy of the string with all"
  },
  "2167": {
    "name": "rfind",
    "module": "torch.storage.np.char",
    "fullName": "torch.storage.np.char.rfind",
    "signature": "(a, sub, start=0, end=None)",
    "description": "For each element in `a`, return the highest index in the string"
  },
  "2168": {
    "name": "rindex",
    "module": "torch.storage.np.char",
    "fullName": "torch.storage.np.char.rindex",
    "signature": "(a, sub, start=0, end=None)",
    "description": "Like `rfind`, but raises `ValueError` when the substring `sub` is"
  },
  "2169": {
    "name": "rjust",
    "module": "torch.storage.np.char",
    "fullName": "torch.storage.np.char.rjust",
    "signature": "(a, width, fillchar=' ')",
    "description": "Return an array with the elements of `a` right-justified in a"
  },
  "2170": {
    "name": "rpartition",
    "module": "torch.storage.np.char",
    "fullName": "torch.storage.np.char.rpartition",
    "signature": "(a, sep)",
    "description": "Partition (split) each element around the right-most separator."
  },
  "2171": {
    "name": "rsplit",
    "module": "torch.storage.np.char",
    "fullName": "torch.storage.np.char.rsplit",
    "signature": "(a, sep=None, maxsplit=None)",
    "description": "For each element in `a`, return a list of the words in the"
  },
  "2172": {
    "name": "rstrip",
    "module": "torch.storage.np.char",
    "fullName": "torch.storage.np.char.rstrip",
    "signature": "(a, chars=None)",
    "description": "For each element in `a`, return a copy with the trailing"
  },
  "2173": {
    "name": "set_module",
    "module": "torch.storage.np.char",
    "fullName": "torch.storage.np.char.set_module",
    "signature": "(module)",
    "description": "Decorator for overriding __module__ on a function or class."
  },
  "2174": {
    "name": "split",
    "module": "torch.storage.np.char",
    "fullName": "torch.storage.np.char.split",
    "signature": "(a, sep=None, maxsplit=None)",
    "description": "For each element in `a`, return a list of the words in the"
  },
  "2175": {
    "name": "splitlines",
    "module": "torch.storage.np.char",
    "fullName": "torch.storage.np.char.splitlines",
    "signature": "(a, keepends=None)",
    "description": "For each element in `a`, return a list of the lines in the"
  },
  "2176": {
    "name": "startswith",
    "module": "torch.storage.np.char",
    "fullName": "torch.storage.np.char.startswith",
    "signature": "(a, prefix, start=0, end=None)",
    "description": "Returns a boolean array which is `True` where the string element"
  },
  "2177": {
    "name": "str_len",
    "module": "torch.storage.np.char",
    "fullName": "torch.storage.np.char.str_len",
    "signature": "(a)",
    "description": "Return len(a) element-wise."
  },
  "2178": {
    "name": "strip",
    "module": "torch.storage.np.char",
    "fullName": "torch.storage.np.char.strip",
    "signature": "(a, chars=None)",
    "description": "For each element in `a`, return a copy with the leading and"
  },
  "2179": {
    "name": "swapcase",
    "module": "torch.storage.np.char",
    "fullName": "torch.storage.np.char.swapcase",
    "signature": "(a)",
    "description": "Return element-wise a copy of the string with"
  },
  "2180": {
    "name": "title",
    "module": "torch.storage.np.char",
    "fullName": "torch.storage.np.char.title",
    "signature": "(a)",
    "description": "Return element-wise title cased version of string or unicode."
  },
  "2181": {
    "name": "translate",
    "module": "torch.storage.np.char",
    "fullName": "torch.storage.np.char.translate",
    "signature": "(a, table, deletechars=None)",
    "description": "For each element in `a`, return a copy of the string where all"
  },
  "2182": {
    "name": "upper",
    "module": "torch.storage.np.char",
    "fullName": "torch.storage.np.char.upper",
    "signature": "(a)",
    "description": "Return an array with the elements converted to uppercase."
  },
  "2183": {
    "name": "zfill",
    "module": "torch.storage.np.char",
    "fullName": "torch.storage.np.char.zfill",
    "signature": "(a, width)",
    "description": "Return the numeric string left-filled with zeros"
  },
  "2184": {
    "name": "DimOrDims",
    "module": "torch.sparse",
    "fullName": "torch.sparse.DimOrDims",
    "signature": "(*args, **kwargs)",
    "description": "No description available."
  },
  "2185": {
    "name": "sum",
    "module": "torch.sparse",
    "fullName": "torch.sparse.sum",
    "signature": "(input: torch.Tensor, dim: Optional[Tuple[int]] = None, dtype: Optional[int] = None) -> torch.Tensor",
    "description": "Returns the sum of each row of the sparse tensor :attr:`input` in the given"
  },
  "2186": {
    "name": "cast",
    "module": "torch.serialization",
    "fullName": "torch.serialization.cast",
    "signature": "(typ, val)",
    "description": "Cast a value to a type."
  },
  "2187": {
    "name": "check_module_version_greater_or_equal",
    "module": "torch.serialization",
    "fullName": "torch.serialization.check_module_version_greater_or_equal",
    "signature": "(module, req_version_tuple, error_if_malformed=True)",
    "description": "Check if a module's version satisfies requirements"
  },
  "2188": {
    "name": "contextmanager",
    "module": "torch.serialization",
    "fullName": "torch.serialization.contextmanager",
    "signature": "(func)",
    "description": "@contextmanager decorator."
  },
  "2189": {
    "name": "default_restore_location",
    "module": "torch.serialization",
    "fullName": "torch.serialization.default_restore_location",
    "signature": "(storage, location)",
    "description": "No description available."
  },
  "2190": {
    "name": "get_source_lines_and_file",
    "module": "torch.serialization",
    "fullName": "torch.serialization.get_source_lines_and_file",
    "signature": "(obj: Any, error_msg: Optional[str] = None) -> Tuple[List[str], int, Optional[str]]",
    "description": "Wrapper around inspect.getsourcelines and inspect.getsourcefile."
  },
  "2191": {
    "name": "load",
    "module": "torch.serialization",
    "fullName": "torch.serialization.load",
    "signature": "(f, map_location=None, pickle_module=<module 'pickle' from '/root/miniconda3/envs/DlibFuzz/lib/python3.9/pickle.py'>, **pickle_load_args)",
    "description": "load(f, map_location=None, pickle_module=pickle, **pickle_load_args)"
  },
  "2192": {
    "name": "location_tag",
    "module": "torch.serialization",
    "fullName": "torch.serialization.location_tag",
    "signature": "(storage: Union[torch.types.Storage, torch.storage._TypedStorage, torch.storage._UntypedStorage])",
    "description": "No description available."
  },
  "2193": {
    "name": "mkdtemp",
    "module": "torch.serialization",
    "fullName": "torch.serialization.mkdtemp",
    "signature": "()",
    "description": "No description available."
  },
  "2194": {
    "name": "normalize_storage_type",
    "module": "torch.serialization",
    "fullName": "torch.serialization.normalize_storage_type",
    "signature": "(storage_type)",
    "description": "No description available."
  },
  "2195": {
    "name": "register_package",
    "module": "torch.serialization",
    "fullName": "torch.serialization.register_package",
    "signature": "(priority, tagger, deserializer)",
    "description": "No description available."
  },
  "2196": {
    "name": "save",
    "module": "torch.serialization",
    "fullName": "torch.serialization.save",
    "signature": "(obj, f: Union[str, os.PathLike, BinaryIO, IO[bytes]], pickle_module=<module 'pickle' from '/root/miniconda3/envs/DlibFuzz/lib/python3.9/pickle.py'>, pickle_protocol=2, _use_new_zipfile_serialization=True) -> None",
    "description": "save(obj, f, pickle_module=pickle, pickle_protocol=DEFAULT_PROTOCOL, _use_new_zipfile_serialization=True)"
  },
  "2197": {
    "name": "storage_to_tensor_type",
    "module": "torch.serialization",
    "fullName": "torch.serialization.storage_to_tensor_type",
    "signature": "(storage)",
    "description": "No description available."
  },
  "2198": {
    "name": "validate_cuda_device",
    "module": "torch.serialization",
    "fullName": "torch.serialization.validate_cuda_device",
    "signature": "(location)",
    "description": "No description available."
  },
  "2199": {
    "name": "calc_chksums",
    "module": "torch.serialization.tarfile",
    "fullName": "torch.serialization.tarfile.calc_chksums",
    "signature": "(buf)",
    "description": "Calculate the checksum for a member's header by summing up all"
  },
  "2200": {
    "name": "copyfileobj",
    "module": "torch.serialization.tarfile",
    "fullName": "torch.serialization.tarfile.copyfileobj",
    "signature": "(src, dst, length=None, exception=<class 'OSError'>, bufsize=None)",
    "description": "Copy length bytes from fileobj src to fileobj dst."
  },
  "2201": {
    "name": "data_filter",
    "module": "torch.serialization.tarfile",
    "fullName": "torch.serialization.tarfile.data_filter",
    "signature": "(member, dest_path)",
    "description": "No description available."
  },
  "2202": {
    "name": "fully_trusted_filter",
    "module": "torch.serialization.tarfile",
    "fullName": "torch.serialization.tarfile.fully_trusted_filter",
    "signature": "(member, dest_path)",
    "description": "No description available."
  },
  "2203": {
    "name": "is_tarfile",
    "module": "torch.serialization.tarfile",
    "fullName": "torch.serialization.tarfile.is_tarfile",
    "signature": "(name)",
    "description": "Return True if name points to a tar archive that we"
  },
  "2204": {
    "name": "itn",
    "module": "torch.serialization.tarfile",
    "fullName": "torch.serialization.tarfile.itn",
    "signature": "(n, digits=8, format=2)",
    "description": "Convert a python number to a number field."
  },
  "2205": {
    "name": "main",
    "module": "torch.serialization.tarfile",
    "fullName": "torch.serialization.tarfile.main",
    "signature": "()",
    "description": "No description available."
  },
  "2206": {
    "name": "nti",
    "module": "torch.serialization.tarfile",
    "fullName": "torch.serialization.tarfile.nti",
    "signature": "(s)",
    "description": "Convert a number field to a python number."
  },
  "2207": {
    "name": "nts",
    "module": "torch.serialization.tarfile",
    "fullName": "torch.serialization.tarfile.nts",
    "signature": "(s, encoding, errors)",
    "description": "Convert a null-terminated bytes object to a string."
  },
  "2208": {
    "name": "stn",
    "module": "torch.serialization.tarfile",
    "fullName": "torch.serialization.tarfile.stn",
    "signature": "(s, length, encoding, errors)",
    "description": "Convert a string to a null-terminated bytes object."
  },
  "2209": {
    "name": "tar_filter",
    "module": "torch.serialization.tarfile",
    "fullName": "torch.serialization.tarfile.tar_filter",
    "signature": "(member, dest_path)",
    "description": "No description available."
  },
  "2210": {
    "name": "chown",
    "module": "torch.serialization.tarfile.shutil",
    "fullName": "torch.serialization.tarfile.shutil.chown",
    "signature": "(path, user=None, group=None)",
    "description": "Change owner user and group of the given path."
  },
  "2211": {
    "name": "copy",
    "module": "torch.serialization.tarfile.shutil",
    "fullName": "torch.serialization.tarfile.shutil.copy",
    "signature": "(src, dst, *, follow_symlinks=True)",
    "description": "Copy data and mode bits (\"cp src dst\"). Return the file's destination."
  },
  "2212": {
    "name": "copy2",
    "module": "torch.serialization.tarfile.shutil",
    "fullName": "torch.serialization.tarfile.shutil.copy2",
    "signature": "(src, dst, *, follow_symlinks=True)",
    "description": "Copy data and metadata. Return the file's destination."
  },
  "2213": {
    "name": "copyfile",
    "module": "torch.serialization.tarfile.shutil",
    "fullName": "torch.serialization.tarfile.shutil.copyfile",
    "signature": "(src, dst, *, follow_symlinks=True)",
    "description": "Copy data from src to dst in the most efficient way possible."
  },
  "2214": {
    "name": "copyfileobj",
    "module": "torch.serialization.tarfile.shutil",
    "fullName": "torch.serialization.tarfile.shutil.copyfileobj",
    "signature": "(fsrc, fdst, length=0)",
    "description": "copy data from file-like object fsrc to file-like object fdst"
  },
  "2215": {
    "name": "copymode",
    "module": "torch.serialization.tarfile.shutil",
    "fullName": "torch.serialization.tarfile.shutil.copymode",
    "signature": "(src, dst, *, follow_symlinks=True)",
    "description": "Copy mode bits from src to dst."
  },
  "2216": {
    "name": "copystat",
    "module": "torch.serialization.tarfile.shutil",
    "fullName": "torch.serialization.tarfile.shutil.copystat",
    "signature": "(src, dst, *, follow_symlinks=True)",
    "description": "Copy file metadata"
  },
  "2217": {
    "name": "copytree",
    "module": "torch.serialization.tarfile.shutil",
    "fullName": "torch.serialization.tarfile.shutil.copytree",
    "signature": "(src, dst, symlinks=False, ignore=None, copy_function=<function copy2 at 0x7fce238aa4c0>, ignore_dangling_symlinks=False, dirs_exist_ok=False)",
    "description": "Recursively copy a directory tree and return the destination directory."
  },
  "2218": {
    "name": "disk_usage",
    "module": "torch.serialization.tarfile.shutil",
    "fullName": "torch.serialization.tarfile.shutil.disk_usage",
    "signature": "(path)",
    "description": "Return disk usage statistics about the given path."
  },
  "2219": {
    "name": "get_archive_formats",
    "module": "torch.serialization.tarfile.shutil",
    "fullName": "torch.serialization.tarfile.shutil.get_archive_formats",
    "signature": "()",
    "description": "Returns a list of supported formats for archiving and unarchiving."
  },
  "2220": {
    "name": "get_terminal_size",
    "module": "torch.serialization.tarfile.shutil",
    "fullName": "torch.serialization.tarfile.shutil.get_terminal_size",
    "signature": "(fallback=(80, 24))",
    "description": "Get the size of the terminal window."
  },
  "2221": {
    "name": "get_unpack_formats",
    "module": "torch.serialization.tarfile.shutil",
    "fullName": "torch.serialization.tarfile.shutil.get_unpack_formats",
    "signature": "()",
    "description": "Returns a list of supported formats for unpacking."
  },
  "2222": {
    "name": "ignore_patterns",
    "module": "torch.serialization.tarfile.shutil",
    "fullName": "torch.serialization.tarfile.shutil.ignore_patterns",
    "signature": "(*patterns)",
    "description": "Function that can be used as copytree() ignore parameter."
  },
  "2223": {
    "name": "make_archive",
    "module": "torch.serialization.tarfile.shutil",
    "fullName": "torch.serialization.tarfile.shutil.make_archive",
    "signature": "(base_name, format, root_dir=None, base_dir=None, verbose=0, dry_run=0, owner=None, group=None, logger=None)",
    "description": "Create an archive file (eg. zip or tar)."
  },
  "2224": {
    "name": "move",
    "module": "torch.serialization.tarfile.shutil",
    "fullName": "torch.serialization.tarfile.shutil.move",
    "signature": "(src, dst, copy_function=<function copy2 at 0x7fce238aa4c0>)",
    "description": "Recursively move a file or directory to another location. This is"
  },
  "2225": {
    "name": "register_archive_format",
    "module": "torch.serialization.tarfile.shutil",
    "fullName": "torch.serialization.tarfile.shutil.register_archive_format",
    "signature": "(name, function, extra_args=None, description='')",
    "description": "Registers an archive format."
  },
  "2226": {
    "name": "register_unpack_format",
    "module": "torch.serialization.tarfile.shutil",
    "fullName": "torch.serialization.tarfile.shutil.register_unpack_format",
    "signature": "(name, extensions, function, extra_args=None, description='')",
    "description": "Registers an unpack format."
  },
  "2227": {
    "name": "rmtree",
    "module": "torch.serialization.tarfile.shutil",
    "fullName": "torch.serialization.tarfile.shutil.rmtree",
    "signature": "(path, ignore_errors=False, onerror=None)",
    "description": "Recursively delete a directory tree."
  },
  "2228": {
    "name": "unpack_archive",
    "module": "torch.serialization.tarfile.shutil",
    "fullName": "torch.serialization.tarfile.shutil.unpack_archive",
    "signature": "(filename, extract_dir=None, format=None, *, filter=None)",
    "description": "Unpack an archive."
  },
  "2229": {
    "name": "unregister_archive_format",
    "module": "torch.serialization.tarfile.shutil",
    "fullName": "torch.serialization.tarfile.shutil.unregister_archive_format",
    "signature": "(name)",
    "description": "No description available."
  },
  "2230": {
    "name": "unregister_unpack_format",
    "module": "torch.serialization.tarfile.shutil",
    "fullName": "torch.serialization.tarfile.shutil.unregister_unpack_format",
    "signature": "(name)",
    "description": "Removes the pack format from the registry."
  },
  "2231": {
    "name": "which",
    "module": "torch.serialization.tarfile.shutil",
    "fullName": "torch.serialization.tarfile.shutil.which",
    "signature": "(cmd, mode=1, path=None)",
    "description": "Given a command, mode, and a PATH string, return the path which"
  },
  "2232": {
    "name": "IS_CHARACTER_JUNK",
    "module": "torch.serialization.difflib",
    "fullName": "torch.serialization.difflib.IS_CHARACTER_JUNK",
    "signature": "(ch, ws=' \\t')",
    "description": "Return True for ignorable character: iff `ch` is a space or tab."
  },
  "2233": {
    "name": "IS_LINE_JUNK",
    "module": "torch.serialization.difflib",
    "fullName": "torch.serialization.difflib.IS_LINE_JUNK",
    "signature": "(line, pat=<built-in method match of re.Pattern object at 0x7fce1c1c6300>)",
    "description": "Return True for ignorable line: iff `line` is blank or contains a single '#'."
  },
  "2234": {
    "name": "context_diff",
    "module": "torch.serialization.difflib",
    "fullName": "torch.serialization.difflib.context_diff",
    "signature": "(a, b, fromfile='', tofile='', fromfiledate='', tofiledate='', n=3, lineterm='\\n')",
    "description": "Compare two sequences of lines; generate the delta as a context diff."
  },
  "2235": {
    "name": "diff_bytes",
    "module": "torch.serialization.difflib",
    "fullName": "torch.serialization.difflib.diff_bytes",
    "signature": "(dfunc, a, b, fromfile=b'', tofile=b'', fromfiledate=b'', tofiledate=b'', n=3, lineterm=b'\\n')",
    "description": "Compare `a` and `b`, two sequences of lines represented as bytes rather"
  },
  "2236": {
    "name": "get_close_matches",
    "module": "torch.serialization.difflib",
    "fullName": "torch.serialization.difflib.get_close_matches",
    "signature": "(word, possibilities, n=3, cutoff=0.6)",
    "description": "Use SequenceMatcher to return list of the best \"good enough\" matches."
  },
  "2237": {
    "name": "ndiff",
    "module": "torch.serialization.difflib",
    "fullName": "torch.serialization.difflib.ndiff",
    "signature": "(a, b, linejunk=None, charjunk=<function IS_CHARACTER_JUNK at 0x7fce1c1cf790>)",
    "description": "Compare `a` and `b` (lists of strings); return a `Differ`-style delta."
  },
  "2238": {
    "name": "restore",
    "module": "torch.serialization.difflib",
    "fullName": "torch.serialization.difflib.restore",
    "signature": "(delta, which)",
    "description": "Generate one of the two sequences that generated a delta."
  },
  "2239": {
    "name": "unified_diff",
    "module": "torch.serialization.difflib",
    "fullName": "torch.serialization.difflib.unified_diff",
    "signature": "(a, b, fromfile='', tofile='', fromfiledate='', tofiledate='', n=3, lineterm='\\n')",
    "description": "Compare two sequences of lines; generate the delta as a unified diff."
  },
  "2240": {
    "name": "pytree_register_structseq",
    "module": "torch.return_types",
    "fullName": "torch.return_types.pytree_register_structseq",
    "signature": "(cls)",
    "description": "No description available."
  },
  "2241": {
    "name": "fork_rng",
    "module": "torch.random",
    "fullName": "torch.random.fork_rng",
    "signature": "(devices=None, enabled=True, _caller='fork_rng', _devices_kw='devices') -> Generator",
    "description": "Forks the RNG, so that when you return, the RNG is reset"
  },
  "2242": {
    "name": "get_rng_state",
    "module": "torch.random",
    "fullName": "torch.random.get_rng_state",
    "signature": "() -> torch.Tensor",
    "description": "Returns the random number generator state as a `torch.ByteTensor`."
  },
  "2243": {
    "name": "initial_seed",
    "module": "torch.random",
    "fullName": "torch.random.initial_seed",
    "signature": "() -> int",
    "description": "Returns the initial seed for generating random numbers as a"
  },
  "2244": {
    "name": "manual_seed",
    "module": "torch.random",
    "fullName": "torch.random.manual_seed",
    "signature": "(seed) -> torch._C.Generator",
    "description": "Sets the seed for generating random numbers. Returns a"
  },
  "2245": {
    "name": "seed",
    "module": "torch.random",
    "fullName": "torch.random.seed",
    "signature": "() -> int",
    "description": "Sets the seed for generating random numbers to a non-deterministic"
  },
  "2246": {
    "name": "set_rng_state",
    "module": "torch.random",
    "fullName": "torch.random.set_rng_state",
    "signature": "(new_state: torch.Tensor) -> None",
    "description": "Sets the random number generator state."
  },
  "2247": {
    "name": "QConfigAny",
    "module": "torch.quantization",
    "fullName": "torch.quantization.QConfigAny",
    "signature": "(*args, **kwargs)",
    "description": "No description available."
  },
  "2248": {
    "name": "add_module_to_qconfig_obs_ctr",
    "module": "torch.quantization",
    "fullName": "torch.quantization.add_module_to_qconfig_obs_ctr",
    "signature": "(qconfig: Optional[torch.ao.quantization.qconfig.QConfig], module: Optional[torch.nn.modules.module.Module]) -> Any",
    "description": "This is a helper function for use in quantization prepare that updates a qconfig so that"
  },
  "2249": {
    "name": "add_observer_",
    "module": "torch.quantization",
    "fullName": "torch.quantization.add_observer_",
    "signature": "(module, qconfig_propagation_list=None, non_leaf_module_list=None, device=None, custom_module_class_mapping=None)",
    "description": "Add observer for the leaf child of the module."
  },
  "2250": {
    "name": "add_quant_dequant",
    "module": "torch.quantization",
    "fullName": "torch.quantization.add_quant_dequant",
    "signature": "(module)",
    "description": "Wrap the leaf child module in QuantWrapper if it has a valid qconfig"
  },
  "2251": {
    "name": "assert_valid_qconfig",
    "module": "torch.quantization",
    "fullName": "torch.quantization.assert_valid_qconfig",
    "signature": "(qconfig: Optional[torch.ao.quantization.qconfig.QConfig], mod: torch.nn.modules.module.Module) -> None",
    "description": "Verifies that this `qconfig` is valid."
  },
  "2252": {
    "name": "convert",
    "module": "torch.quantization",
    "fullName": "torch.quantization.convert",
    "signature": "(module, mapping=None, inplace=False, remove_qconfig=True, is_reference=False, convert_custom_config_dict=None)",
    "description": "Converts submodules in input module to a different module according to `mapping`"
  },
  "2253": {
    "name": "convert_dynamic_jit",
    "module": "torch.quantization",
    "fullName": "torch.quantization.convert_dynamic_jit",
    "signature": "(model, inplace=False, debug=False, preserved_attrs=None)",
    "description": "No description available."
  },
  "2254": {
    "name": "convert_jit",
    "module": "torch.quantization",
    "fullName": "torch.quantization.convert_jit",
    "signature": "(model, inplace=False, debug=False, preserved_attrs=None)",
    "description": "No description available."
  },
  "2255": {
    "name": "default_eval_fn",
    "module": "torch.quantization",
    "fullName": "torch.quantization.default_eval_fn",
    "signature": "(model, calib_data)",
    "description": "Default evaluation function takes a torch.utils.data.Dataset or a list of"
  },
  "2256": {
    "name": "disable_fake_quant",
    "module": "torch.quantization",
    "fullName": "torch.quantization.disable_fake_quant",
    "signature": "(mod)",
    "description": "Disable fake quantization for this module, if applicable. Example usage::"
  },
  "2257": {
    "name": "disable_observer",
    "module": "torch.quantization",
    "fullName": "torch.quantization.disable_observer",
    "signature": "(mod)",
    "description": "Disable observation for this module, if applicable. Example usage::"
  },
  "2258": {
    "name": "enable_fake_quant",
    "module": "torch.quantization",
    "fullName": "torch.quantization.enable_fake_quant",
    "signature": "(mod)",
    "description": "Enable fake quantization for this module, if applicable. Example usage::"
  },
  "2259": {
    "name": "enable_observer",
    "module": "torch.quantization",
    "fullName": "torch.quantization.enable_observer",
    "signature": "(mod)",
    "description": "Enable observation for this module, if applicable. Example usage::"
  },
  "2260": {
    "name": "fuse_conv_bn",
    "module": "torch.quantization",
    "fullName": "torch.quantization.fuse_conv_bn",
    "signature": "(is_qat, conv, bn)",
    "description": "Given the conv and bn modules, fuses them and returns the fused module"
  },
  "2261": {
    "name": "fuse_conv_bn_jit",
    "module": "torch.quantization",
    "fullName": "torch.quantization.fuse_conv_bn_jit",
    "signature": "(model, inplace=False)",
    "description": "Fuse conv - bn module"
  },
  "2262": {
    "name": "fuse_conv_bn_relu",
    "module": "torch.quantization",
    "fullName": "torch.quantization.fuse_conv_bn_relu",
    "signature": "(is_qat, conv, bn, relu)",
    "description": "Given the conv and bn modules, fuses them and returns the fused module"
  },
  "2263": {
    "name": "fuse_linear_bn",
    "module": "torch.quantization",
    "fullName": "torch.quantization.fuse_linear_bn",
    "signature": "(is_qat, linear, bn)",
    "description": "Given the linear and bn modules, fuses them and returns the fused module"
  },
  "2264": {
    "name": "fuse_modules",
    "module": "torch.quantization",
    "fullName": "torch.quantization.fuse_modules",
    "signature": "(model, modules_to_fuse, inplace=False, fuser_func=<function fuse_known_modules at 0x7fcda8157160>, fuse_custom_config_dict=None)",
    "description": "Fuses a list of modules into a single module"
  },
  "2265": {
    "name": "get_default_compare_output_module_list",
    "module": "torch.quantization",
    "fullName": "torch.quantization.get_default_compare_output_module_list",
    "signature": "() -> Set[Callable]",
    "description": "Get list of module class types that we will record output"
  },
  "2266": {
    "name": "get_default_dynamic_quant_module_mappings",
    "module": "torch.quantization",
    "fullName": "torch.quantization.get_default_dynamic_quant_module_mappings",
    "signature": "() -> Dict[Callable, Any]",
    "description": "Get module mapping for post training dynamic quantization"
  },
  "2267": {
    "name": "get_default_float_to_quantized_operator_mappings",
    "module": "torch.quantization",
    "fullName": "torch.quantization.get_default_float_to_quantized_operator_mappings",
    "signature": "() -> Dict[Union[Callable, str], Callable]",
    "description": "No description available."
  },
  "2268": {
    "name": "get_default_qat_module_mappings",
    "module": "torch.quantization",
    "fullName": "torch.quantization.get_default_qat_module_mappings",
    "signature": "() -> Dict[Callable, Any]",
    "description": "Get default module mapping for quantization aware training"
  },
  "2269": {
    "name": "get_default_qat_qconfig",
    "module": "torch.quantization",
    "fullName": "torch.quantization.get_default_qat_qconfig",
    "signature": "(backend='fbgemm', version=1)",
    "description": "Returns the default QAT qconfig for the specified backend."
  },
  "2270": {
    "name": "get_default_qconfig",
    "module": "torch.quantization",
    "fullName": "torch.quantization.get_default_qconfig",
    "signature": "(backend='fbgemm', version=0)",
    "description": "Returns the default PTQ qconfig for the specified backend."
  },
  "2271": {
    "name": "get_default_qconfig_propagation_list",
    "module": "torch.quantization",
    "fullName": "torch.quantization.get_default_qconfig_propagation_list",
    "signature": "() -> Set[Callable]",
    "description": "Get the default list of module types that we'll attach qconfig"
  },
  "2272": {
    "name": "get_default_static_quant_module_mappings",
    "module": "torch.quantization",
    "fullName": "torch.quantization.get_default_static_quant_module_mappings",
    "signature": "() -> Dict[Callable, Any]",
    "description": "Get module mapping for post training static quantization"
  },
  "2273": {
    "name": "get_dynamic_quant_module_class",
    "module": "torch.quantization",
    "fullName": "torch.quantization.get_dynamic_quant_module_class",
    "signature": "(float_module_class: Callable, additional_dynamic_quant_mapping: Optional[Dict[Callable, Any]] = None) -> Any",
    "description": "n Get the dynamically quantized module class corresponding to"
  },
  "2274": {
    "name": "get_fuser_method",
    "module": "torch.quantization",
    "fullName": "torch.quantization.get_fuser_method",
    "signature": "(op_list, additional_fuser_method_mapping=None)",
    "description": "Get fuser method for the given list of module types,"
  },
  "2275": {
    "name": "get_observer_dict",
    "module": "torch.quantization",
    "fullName": "torch.quantization.get_observer_dict",
    "signature": "(mod, target_dict, prefix='')",
    "description": "Traverse the modules and save all observers into dict."
  },
  "2276": {
    "name": "get_observer_state_dict",
    "module": "torch.quantization",
    "fullName": "torch.quantization.get_observer_state_dict",
    "signature": "(mod)",
    "description": "Returns the state dict corresponding to the observer stats."
  },
  "2277": {
    "name": "get_quantized_operator",
    "module": "torch.quantization",
    "fullName": "torch.quantization.get_quantized_operator",
    "signature": "(float_op: Union[Callable, str]) -> Callable",
    "description": "Get the quantized operator corresponding to the float operator"
  },
  "2278": {
    "name": "get_static_quant_module_class",
    "module": "torch.quantization",
    "fullName": "torch.quantization.get_static_quant_module_class",
    "signature": "(float_module_class: Callable, additional_static_quant_mapping: Optional[Dict[Callable, Any]] = None, is_reference: bool = False) -> Any",
    "description": "n Get the statically quantized module class corresponding to"
  },
  "2279": {
    "name": "get_unique_devices_",
    "module": "torch.quantization",
    "fullName": "torch.quantization.get_unique_devices_",
    "signature": "(module)",
    "description": "No description available."
  },
  "2280": {
    "name": "is_activation_post_process",
    "module": "torch.quantization",
    "fullName": "torch.quantization.is_activation_post_process",
    "signature": "(module)",
    "description": "No description available."
  },
  "2281": {
    "name": "load_observer_state_dict",
    "module": "torch.quantization",
    "fullName": "torch.quantization.load_observer_state_dict",
    "signature": "(mod, obs_dict)",
    "description": "Given input model and a state_dict containing model observer stats,"
  },
  "2282": {
    "name": "no_observer_set",
    "module": "torch.quantization",
    "fullName": "torch.quantization.no_observer_set",
    "signature": "() -> Set[Any]",
    "description": "These modules cannot have observers inserted by default."
  },
  "2283": {
    "name": "prepare",
    "module": "torch.quantization",
    "fullName": "torch.quantization.prepare",
    "signature": "(model, inplace=False, allow_list=None, observer_non_leaf_module_list=None, prepare_custom_config_dict=None)",
    "description": "Prepares a copy of the model for quantization calibration or quantization-aware training."
  },
  "2284": {
    "name": "prepare_dynamic_jit",
    "module": "torch.quantization",
    "fullName": "torch.quantization.prepare_dynamic_jit",
    "signature": "(model, qconfig_dict, inplace=False)",
    "description": "No description available."
  },
  "2285": {
    "name": "prepare_jit",
    "module": "torch.quantization",
    "fullName": "torch.quantization.prepare_jit",
    "signature": "(model, qconfig_dict, inplace=False)",
    "description": "No description available."
  },
  "2286": {
    "name": "prepare_qat",
    "module": "torch.quantization",
    "fullName": "torch.quantization.prepare_qat",
    "signature": "(model, mapping=None, inplace=False)",
    "description": "Prepares a copy of the model for quantization calibration or"
  },
  "2287": {
    "name": "propagate_qconfig_",
    "module": "torch.quantization",
    "fullName": "torch.quantization.propagate_qconfig_",
    "signature": "(module, qconfig_dict=None, prepare_custom_config_dict=None)",
    "description": "Propagate qconfig through the module hierarchy and assign `qconfig`"
  },
  "2288": {
    "name": "qconfig_equals",
    "module": "torch.quantization",
    "fullName": "torch.quantization.qconfig_equals",
    "signature": "(q1: Optional[torch.ao.quantization.qconfig.QConfig], q2: Optional[torch.ao.quantization.qconfig.QConfig])",
    "description": "Returns `True` if `q1` equals `q2`, and `False` otherwise."
  },
  "2289": {
    "name": "quant_type_to_str",
    "module": "torch.quantization",
    "fullName": "torch.quantization.quant_type_to_str",
    "signature": "(quant_type)",
    "description": "No description available."
  },
  "2290": {
    "name": "quantize",
    "module": "torch.quantization",
    "fullName": "torch.quantization.quantize",
    "signature": "(model, run_fn, run_args, mapping=None, inplace=False)",
    "description": "Quantize the input float model with post training static quantization."
  },
  "2291": {
    "name": "quantize_dynamic",
    "module": "torch.quantization",
    "fullName": "torch.quantization.quantize_dynamic",
    "signature": "(model, qconfig_spec=None, dtype=torch.qint8, mapping=None, inplace=False)",
    "description": "Converts a float model to dynamic (i.e. weights-only) quantized model."
  },
  "2292": {
    "name": "quantize_dynamic_jit",
    "module": "torch.quantization",
    "fullName": "torch.quantization.quantize_dynamic_jit",
    "signature": "(model, qconfig_dict, inplace=False, debug=False)",
    "description": "Quantize the input float TorchScript model with"
  },
  "2293": {
    "name": "quantize_jit",
    "module": "torch.quantization",
    "fullName": "torch.quantization.quantize_jit",
    "signature": "(model, qconfig_dict, run_fn, run_args, inplace=False, debug=False)",
    "description": "Quantize the input float TorchScript model with"
  },
  "2294": {
    "name": "quantize_qat",
    "module": "torch.quantization",
    "fullName": "torch.quantization.quantize_qat",
    "signature": "(model, run_fn, run_args, inplace=False)",
    "description": "Do quantization aware training and output a quantized model"
  },
  "2295": {
    "name": "register_activation_post_process_hook",
    "module": "torch.quantization",
    "fullName": "torch.quantization.register_activation_post_process_hook",
    "signature": "(module, pre_hook=False)",
    "description": "No description available."
  },
  "2296": {
    "name": "script_qconfig",
    "module": "torch.quantization",
    "fullName": "torch.quantization.script_qconfig",
    "signature": "(qconfig)",
    "description": "Instantiate the activation and weight observer modules and script"
  },
  "2297": {
    "name": "script_qconfig_dict",
    "module": "torch.quantization",
    "fullName": "torch.quantization.script_qconfig_dict",
    "signature": "(qconfig_dict)",
    "description": "Helper function used by `prepare_jit`."
  },
  "2298": {
    "name": "swap_module",
    "module": "torch.quantization",
    "fullName": "torch.quantization.swap_module",
    "signature": "(mod, mapping, custom_module_class_mapping)",
    "description": "Swaps the module if it has a quantized counterpart and it has an"
  },
  "2299": {
    "name": "get_default_compare_output_module_list",
    "module": "torch.quantization.quantization_mappings",
    "fullName": "torch.quantization.quantization_mappings.get_default_compare_output_module_list",
    "signature": "() -> Set[Callable]",
    "description": "Get list of module class types that we will record output"
  },
  "2300": {
    "name": "get_default_dynamic_quant_module_mappings",
    "module": "torch.quantization.quantization_mappings",
    "fullName": "torch.quantization.quantization_mappings.get_default_dynamic_quant_module_mappings",
    "signature": "() -> Dict[Callable, Any]",
    "description": "Get module mapping for post training dynamic quantization"
  },
  "2301": {
    "name": "get_default_float_to_quantized_operator_mappings",
    "module": "torch.quantization.quantization_mappings",
    "fullName": "torch.quantization.quantization_mappings.get_default_float_to_quantized_operator_mappings",
    "signature": "() -> Dict[Union[Callable, str], Callable]",
    "description": "No description available."
  },
  "2302": {
    "name": "get_default_qat_module_mappings",
    "module": "torch.quantization.quantization_mappings",
    "fullName": "torch.quantization.quantization_mappings.get_default_qat_module_mappings",
    "signature": "() -> Dict[Callable, Any]",
    "description": "Get default module mapping for quantization aware training"
  },
  "2303": {
    "name": "get_default_qconfig_propagation_list",
    "module": "torch.quantization.quantization_mappings",
    "fullName": "torch.quantization.quantization_mappings.get_default_qconfig_propagation_list",
    "signature": "() -> Set[Callable]",
    "description": "Get the default list of module types that we'll attach qconfig"
  },
  "2304": {
    "name": "get_default_static_quant_module_mappings",
    "module": "torch.quantization.quantization_mappings",
    "fullName": "torch.quantization.quantization_mappings.get_default_static_quant_module_mappings",
    "signature": "() -> Dict[Callable, Any]",
    "description": "Get module mapping for post training static quantization"
  },
  "2305": {
    "name": "get_dynamic_quant_module_class",
    "module": "torch.quantization.quantization_mappings",
    "fullName": "torch.quantization.quantization_mappings.get_dynamic_quant_module_class",
    "signature": "(float_module_class: Callable, additional_dynamic_quant_mapping: Optional[Dict[Callable, Any]] = None) -> Any",
    "description": "n Get the dynamically quantized module class corresponding to"
  },
  "2306": {
    "name": "get_quantized_operator",
    "module": "torch.quantization.quantization_mappings",
    "fullName": "torch.quantization.quantization_mappings.get_quantized_operator",
    "signature": "(float_op: Union[Callable, str]) -> Callable",
    "description": "Get the quantized operator corresponding to the float operator"
  },
  "2307": {
    "name": "get_static_quant_module_class",
    "module": "torch.quantization.quantization_mappings",
    "fullName": "torch.quantization.quantization_mappings.get_static_quant_module_class",
    "signature": "(float_module_class: Callable, additional_static_quant_mapping: Optional[Dict[Callable, Any]] = None, is_reference: bool = False) -> Any",
    "description": "n Get the statically quantized module class corresponding to"
  },
  "2308": {
    "name": "no_observer_set",
    "module": "torch.quantization.quantization_mappings",
    "fullName": "torch.quantization.quantization_mappings.no_observer_set",
    "signature": "() -> Set[Any]",
    "description": "These modules cannot have observers inserted by default."
  },
  "2309": {
    "name": "quant_type_to_str",
    "module": "torch.quantization.quant_type",
    "fullName": "torch.quantization.quant_type.quant_type_to_str",
    "signature": "(quant_type)",
    "description": "No description available."
  },
  "2310": {
    "name": "QConfigAny",
    "module": "torch.quantization.qconfig",
    "fullName": "torch.quantization.qconfig.QConfigAny",
    "signature": "(*args, **kwargs)",
    "description": "No description available."
  },
  "2311": {
    "name": "add_module_to_qconfig_obs_ctr",
    "module": "torch.quantization.qconfig",
    "fullName": "torch.quantization.qconfig.add_module_to_qconfig_obs_ctr",
    "signature": "(qconfig: Optional[torch.ao.quantization.qconfig.QConfig], module: Optional[torch.nn.modules.module.Module]) -> Any",
    "description": "This is a helper function for use in quantization prepare that updates a qconfig so that"
  },
  "2312": {
    "name": "assert_valid_qconfig",
    "module": "torch.quantization.qconfig",
    "fullName": "torch.quantization.qconfig.assert_valid_qconfig",
    "signature": "(qconfig: Optional[torch.ao.quantization.qconfig.QConfig], mod: torch.nn.modules.module.Module) -> None",
    "description": "Verifies that this `qconfig` is valid."
  },
  "2313": {
    "name": "get_default_qat_qconfig",
    "module": "torch.quantization.qconfig",
    "fullName": "torch.quantization.qconfig.get_default_qat_qconfig",
    "signature": "(backend='fbgemm', version=1)",
    "description": "Returns the default QAT qconfig for the specified backend."
  },
  "2314": {
    "name": "get_default_qconfig",
    "module": "torch.quantization.qconfig",
    "fullName": "torch.quantization.qconfig.get_default_qconfig",
    "signature": "(backend='fbgemm', version=0)",
    "description": "Returns the default PTQ qconfig for the specified backend."
  },
  "2315": {
    "name": "qconfig_equals",
    "module": "torch.quantization.qconfig",
    "fullName": "torch.quantization.qconfig.qconfig_equals",
    "signature": "(q1: Optional[torch.ao.quantization.qconfig.QConfig], q2: Optional[torch.ao.quantization.qconfig.QConfig])",
    "description": "Returns `True` if `q1` equals `q2`, and `False` otherwise."
  },
  "2316": {
    "name": "get_observer_state_dict",
    "module": "torch.quantization.observer",
    "fullName": "torch.quantization.observer.get_observer_state_dict",
    "signature": "(mod)",
    "description": "Returns the state dict corresponding to the observer stats."
  },
  "2317": {
    "name": "load_observer_state_dict",
    "module": "torch.quantization.observer",
    "fullName": "torch.quantization.observer.load_observer_state_dict",
    "signature": "(mod, obs_dict)",
    "description": "Given input model and a state_dict containing model observer stats,"
  },
  "2318": {
    "name": "fuse_conv_bn",
    "module": "torch.quantization.fuser_method_mappings",
    "fullName": "torch.quantization.fuser_method_mappings.fuse_conv_bn",
    "signature": "(is_qat, conv, bn)",
    "description": "Given the conv and bn modules, fuses them and returns the fused module"
  },
  "2319": {
    "name": "fuse_conv_bn_relu",
    "module": "torch.quantization.fuser_method_mappings",
    "fullName": "torch.quantization.fuser_method_mappings.fuse_conv_bn_relu",
    "signature": "(is_qat, conv, bn, relu)",
    "description": "Given the conv and bn modules, fuses them and returns the fused module"
  },
  "2320": {
    "name": "fuse_linear_bn",
    "module": "torch.quantization.fuser_method_mappings",
    "fullName": "torch.quantization.fuser_method_mappings.fuse_linear_bn",
    "signature": "(is_qat, linear, bn)",
    "description": "Given the linear and bn modules, fuses them and returns the fused module"
  },
  "2321": {
    "name": "get_fuser_method",
    "module": "torch.quantization.fuser_method_mappings",
    "fullName": "torch.quantization.fuser_method_mappings.get_fuser_method",
    "signature": "(op_list, additional_fuser_method_mapping=None)",
    "description": "Get fuser method for the given list of module types,"
  },
  "2322": {
    "name": "disable_fake_quant",
    "module": "torch.quantization.fake_quantize",
    "fullName": "torch.quantization.fake_quantize.disable_fake_quant",
    "signature": "(mod)",
    "description": "Disable fake quantization for this module, if applicable. Example usage::"
  },
  "2323": {
    "name": "disable_observer",
    "module": "torch.quantization.fake_quantize",
    "fullName": "torch.quantization.fake_quantize.disable_observer",
    "signature": "(mod)",
    "description": "Disable observation for this module, if applicable. Example usage::"
  },
  "2324": {
    "name": "enable_fake_quant",
    "module": "torch.quantization.fake_quantize",
    "fullName": "torch.quantization.fake_quantize.enable_fake_quant",
    "signature": "(mod)",
    "description": "Enable fake quantization for this module, if applicable. Example usage::"
  },
  "2325": {
    "name": "enable_observer",
    "module": "torch.quantization.fake_quantize",
    "fullName": "torch.quantization.fake_quantize.enable_observer",
    "signature": "(mod)",
    "description": "Enable observation for this module, if applicable. Example usage::"
  },
  "2326": {
    "name": "schedule",
    "module": "torch.profiler",
    "fullName": "torch.profiler.schedule",
    "signature": "(*, wait: int, warmup: int, active: int, repeat: int = 0, skip_first: int = 0) -> Callable",
    "description": "Returns a callable that can be used as profiler ``schedule`` argument. The profiler will skip"
  },
  "2327": {
    "name": "supported_activities",
    "module": "torch.profiler",
    "fullName": "torch.profiler.supported_activities",
    "signature": "()",
    "description": "Returns a set of supported profiler tracing activities."
  },
  "2328": {
    "name": "tensorboard_trace_handler",
    "module": "torch.profiler",
    "fullName": "torch.profiler.tensorboard_trace_handler",
    "signature": "(dir_name: str, worker_name: Optional[str] = None, use_gzip: bool = False)",
    "description": "Outputs tracing files to directory of ``dir_name``, then that directory can be"
  },
  "2329": {
    "name": "schedule",
    "module": "torch.profiler.profiler",
    "fullName": "torch.profiler.profiler.schedule",
    "signature": "(*, wait: int, warmup: int, active: int, repeat: int = 0, skip_first: int = 0) -> Callable",
    "description": "Returns a callable that can be used as profiler ``schedule`` argument. The profiler will skip"
  },
  "2330": {
    "name": "supported_activities",
    "module": "torch.profiler.profiler",
    "fullName": "torch.profiler.profiler.supported_activities",
    "signature": "()",
    "description": "Returns a set of supported profiler tracing activities."
  },
  "2331": {
    "name": "tensorboard_trace_handler",
    "module": "torch.profiler.profiler",
    "fullName": "torch.profiler.profiler.tensorboard_trace_handler",
    "signature": "(dir_name: str, worker_name: Optional[str] = None, use_gzip: bool = False)",
    "description": "Outputs tracing files to directory of ``dir_name``, then that directory can be"
  },
  "2332": {
    "name": "kineto_step",
    "module": "torch.profiler.profiler.prof",
    "fullName": "torch.profiler.profiler.prof.kineto_step",
    "signature": "()",
    "description": "Notify kineto so it is aware of iteration boundaries for asynchronous"
  },
  "2333": {
    "name": "load_nvprof",
    "module": "torch.profiler.profiler.prof",
    "fullName": "torch.profiler.profiler.prof.load_nvprof",
    "signature": "(path)",
    "description": "Opens an nvprof trace file and parses autograd annotations."
  },
  "2334": {
    "name": "parse_nvprof_trace",
    "module": "torch.profiler.profiler.prof",
    "fullName": "torch.profiler.profiler.prof.parse_nvprof_trace",
    "signature": "(path)",
    "description": "No description available."
  },
  "2335": {
    "name": "compress",
    "module": "torch.profiler.profiler.gzip",
    "fullName": "torch.profiler.profiler.gzip.compress",
    "signature": "(data, compresslevel=9, *, mtime=None)",
    "description": "Compress data in one shot and return the compressed string."
  },
  "2336": {
    "name": "decompress",
    "module": "torch.profiler.profiler.gzip",
    "fullName": "torch.profiler.profiler.gzip.decompress",
    "signature": "(data)",
    "description": "Decompress a gzip compressed string in one shot."
  },
  "2337": {
    "name": "main",
    "module": "torch.profiler.profiler.gzip",
    "fullName": "torch.profiler.profiler.gzip.main",
    "signature": "()",
    "description": "No description available."
  },
  "2338": {
    "name": "open",
    "module": "torch.profiler.profiler.gzip",
    "fullName": "torch.profiler.profiler.gzip.open",
    "signature": "(filename, mode='rb', compresslevel=9, encoding=None, errors=None, newline=None)",
    "description": "Open a gzip-compressed file in binary or text mode."
  },
  "2339": {
    "name": "write32u",
    "module": "torch.profiler.profiler.gzip",
    "fullName": "torch.profiler.profiler.gzip.write32u",
    "signature": "(output, value)",
    "description": "No description available."
  },
  "2340": {
    "name": "architecture",
    "module": "torch.platform",
    "fullName": "torch.platform.architecture",
    "signature": "(executable='/root/miniconda3/envs/DlibFuzz/bin/python', bits='', linkage='')",
    "description": "Queries the given executable (defaults to the Python interpreter"
  },
  "2341": {
    "name": "java_ver",
    "module": "torch.platform",
    "fullName": "torch.platform.java_ver",
    "signature": "(release='', vendor='', vminfo=('', '', ''), osinfo=('', '', ''))",
    "description": "Version interface for Jython."
  },
  "2342": {
    "name": "libc_ver",
    "module": "torch.platform",
    "fullName": "torch.platform.libc_ver",
    "signature": "(executable=None, lib='', version='', chunksize=16384)",
    "description": "Tries to determine the libc version that the file executable"
  },
  "2343": {
    "name": "mac_ver",
    "module": "torch.platform",
    "fullName": "torch.platform.mac_ver",
    "signature": "(release='', versioninfo=('', '', ''), machine='')",
    "description": "Get macOS version information and return it as tuple (release,"
  },
  "2344": {
    "name": "machine",
    "module": "torch.platform",
    "fullName": "torch.platform.machine",
    "signature": "()",
    "description": "Returns the machine type, e.g. 'i386'"
  },
  "2345": {
    "name": "node",
    "module": "torch.platform",
    "fullName": "torch.platform.node",
    "signature": "()",
    "description": "Returns the computer's network name (which may not be fully"
  },
  "2346": {
    "name": "platform",
    "module": "torch.platform",
    "fullName": "torch.platform.platform",
    "signature": "(aliased=0, terse=0)",
    "description": "Returns a single string identifying the underlying platform"
  },
  "2347": {
    "name": "processor",
    "module": "torch.platform",
    "fullName": "torch.platform.processor",
    "signature": "()",
    "description": "Returns the (true) processor name, e.g. 'amdk6'"
  },
  "2348": {
    "name": "python_branch",
    "module": "torch.platform",
    "fullName": "torch.platform.python_branch",
    "signature": "()",
    "description": "Returns a string identifying the Python implementation"
  },
  "2349": {
    "name": "python_build",
    "module": "torch.platform",
    "fullName": "torch.platform.python_build",
    "signature": "()",
    "description": "Returns a tuple (buildno, builddate) stating the Python"
  },
  "2350": {
    "name": "python_compiler",
    "module": "torch.platform",
    "fullName": "torch.platform.python_compiler",
    "signature": "()",
    "description": "Returns a string identifying the compiler used for compiling"
  },
  "2351": {
    "name": "python_implementation",
    "module": "torch.platform",
    "fullName": "torch.platform.python_implementation",
    "signature": "()",
    "description": "Returns a string identifying the Python implementation."
  },
  "2352": {
    "name": "python_revision",
    "module": "torch.platform",
    "fullName": "torch.platform.python_revision",
    "signature": "()",
    "description": "Returns a string identifying the Python implementation"
  },
  "2353": {
    "name": "python_version",
    "module": "torch.platform",
    "fullName": "torch.platform.python_version",
    "signature": "()",
    "description": "Returns the Python version as string 'major.minor.patchlevel'"
  },
  "2354": {
    "name": "python_version_tuple",
    "module": "torch.platform",
    "fullName": "torch.platform.python_version_tuple",
    "signature": "()",
    "description": "Returns the Python version as tuple (major, minor, patchlevel)"
  },
  "2355": {
    "name": "release",
    "module": "torch.platform",
    "fullName": "torch.platform.release",
    "signature": "()",
    "description": "Returns the system's release, e.g. '2.2.0' or 'NT'"
  },
  "2356": {
    "name": "system",
    "module": "torch.platform",
    "fullName": "torch.platform.system",
    "signature": "()",
    "description": "Returns the system/OS name, e.g. 'Linux', 'Windows' or 'Java'."
  },
  "2357": {
    "name": "system_alias",
    "module": "torch.platform",
    "fullName": "torch.platform.system_alias",
    "signature": "(system, release, version)",
    "description": "Returns (system, release, version) aliased to common"
  },
  "2358": {
    "name": "uname",
    "module": "torch.platform",
    "fullName": "torch.platform.uname",
    "signature": "()",
    "description": "Fairly portable uname interface. Returns a tuple"
  },
  "2359": {
    "name": "version",
    "module": "torch.platform",
    "fullName": "torch.platform.version",
    "signature": "()",
    "description": "Returns the system's release version, e.g. '#3 on degas'"
  },
  "2360": {
    "name": "win32_edition",
    "module": "torch.platform",
    "fullName": "torch.platform.win32_edition",
    "signature": "()",
    "description": "No description available."
  },
  "2361": {
    "name": "win32_is_iot",
    "module": "torch.platform",
    "fullName": "torch.platform.win32_is_iot",
    "signature": "()",
    "description": "No description available."
  },
  "2362": {
    "name": "win32_ver",
    "module": "torch.platform",
    "fullName": "torch.platform.win32_ver",
    "signature": "(release='', version='', csd='', ptype='')",
    "description": "No description available."
  },
  "2363": {
    "name": "call",
    "module": "torch.platform.subprocess",
    "fullName": "torch.platform.subprocess.call",
    "signature": "(*popenargs, timeout=None, **kwargs)",
    "description": "Run command with arguments.  Wait for command to complete or"
  },
  "2364": {
    "name": "check_call",
    "module": "torch.platform.subprocess",
    "fullName": "torch.platform.subprocess.check_call",
    "signature": "(*popenargs, **kwargs)",
    "description": "Run command with arguments.  Wait for command to complete.  If"
  },
  "2365": {
    "name": "check_output",
    "module": "torch.platform.subprocess",
    "fullName": "torch.platform.subprocess.check_output",
    "signature": "(*popenargs, timeout=None, **kwargs)",
    "description": "Run command with arguments and return its output."
  },
  "2366": {
    "name": "getoutput",
    "module": "torch.platform.subprocess",
    "fullName": "torch.platform.subprocess.getoutput",
    "signature": "(cmd)",
    "description": "Return output (stdout or stderr) of executing cmd in a shell."
  },
  "2367": {
    "name": "getstatusoutput",
    "module": "torch.platform.subprocess",
    "fullName": "torch.platform.subprocess.getstatusoutput",
    "signature": "(cmd)",
    "description": "Return (exitcode, output) of executing cmd in a shell."
  },
  "2368": {
    "name": "list2cmdline",
    "module": "torch.platform.subprocess",
    "fullName": "torch.platform.subprocess.list2cmdline",
    "signature": "(seq)",
    "description": "Translate a sequence of arguments into a command line"
  },
  "2369": {
    "name": "run",
    "module": "torch.platform.subprocess",
    "fullName": "torch.platform.subprocess.run",
    "signature": "(*popenargs, input=None, capture_output=False, timeout=None, check=False, **kwargs)",
    "description": "Run command with arguments and return a CompletedProcess instance."
  },
  "2370": {
    "name": "is_from_package",
    "module": "torch.package",
    "fullName": "torch.package.is_from_package",
    "signature": "(obj: Any) -> bool",
    "description": "Return whether an object was loaded from a package."
  },
  "2371": {
    "name": "GlobPattern",
    "module": "torch.package.package_importer",
    "fullName": "torch.package.package_importer.GlobPattern",
    "signature": "(*args, **kwargs)",
    "description": "No description available."
  },
  "2372": {
    "name": "cast",
    "module": "torch.package.package_importer",
    "fullName": "torch.package.package_importer.cast",
    "signature": "(typ, val)",
    "description": "Cast a value to a type."
  },
  "2373": {
    "name": "contextmanager",
    "module": "torch.package.package_importer",
    "fullName": "torch.package.package_importer.contextmanager",
    "signature": "(func)",
    "description": "@contextmanager decorator."
  },
  "2374": {
    "name": "demangle",
    "module": "torch.package.package_importer",
    "fullName": "torch.package.package_importer.demangle",
    "signature": "(name: str) -> str",
    "description": "Note: Unlike PackageMangler.demangle, this version works on any"
  },
  "2375": {
    "name": "patched_getfile",
    "module": "torch.package.package_importer",
    "fullName": "torch.package.package_importer.patched_getfile",
    "signature": "(object)",
    "description": "No description available."
  },
  "2376": {
    "name": "GlobPattern",
    "module": "torch.package.package_exporter",
    "fullName": "torch.package.package_exporter.GlobPattern",
    "signature": "(*args, **kwargs)",
    "description": "No description available."
  },
  "2377": {
    "name": "cast",
    "module": "torch.package.package_exporter",
    "fullName": "torch.package.package_exporter.cast",
    "signature": "(typ, val)",
    "description": "Cast a value to a type."
  },
  "2378": {
    "name": "create_pickler",
    "module": "torch.package.package_exporter",
    "fullName": "torch.package.package_exporter.create_pickler",
    "signature": "(data_buf, importer, protocol=4)",
    "description": "No description available."
  },
  "2379": {
    "name": "dataclass",
    "module": "torch.package.package_exporter",
    "fullName": "torch.package.package_exporter.dataclass",
    "signature": "(cls=None, /, *, init=True, repr=True, eq=True, order=False, unsafe_hash=False, frozen=False)",
    "description": "Returns the same class as was passed in, with dunder methods"
  },
  "2380": {
    "name": "demangle",
    "module": "torch.package.package_exporter",
    "fullName": "torch.package.package_exporter.demangle",
    "signature": "(name: str) -> str",
    "description": "Note: Unlike PackageMangler.demangle, this version works on any"
  },
  "2381": {
    "name": "is_mangled",
    "module": "torch.package.package_exporter",
    "fullName": "torch.package.package_exporter.is_mangled",
    "signature": "(name: str) -> bool",
    "description": "No description available."
  },
  "2382": {
    "name": "is_stdlib_module",
    "module": "torch.package.package_exporter",
    "fullName": "torch.package.package_exporter.is_stdlib_module",
    "signature": "(module: str) -> bool",
    "description": "No description available."
  },
  "2383": {
    "name": "location_tag",
    "module": "torch.package.package_exporter",
    "fullName": "torch.package.package_exporter.location_tag",
    "signature": "(storage: Union[torch.types.Storage, torch.storage._TypedStorage, torch.storage._UntypedStorage])",
    "description": "No description available."
  },
  "2384": {
    "name": "normalize_storage_type",
    "module": "torch.package.package_exporter",
    "fullName": "torch.package.package_exporter.normalize_storage_type",
    "signature": "(storage_type)",
    "description": "No description available."
  },
  "2385": {
    "name": "decode_long",
    "module": "torch.package.package_exporter.pickletools",
    "fullName": "torch.package.package_exporter.pickletools.decode_long",
    "signature": "(data)",
    "description": "Decode a long from a two's complement little-endian binary string."
  },
  "2386": {
    "name": "dis",
    "module": "torch.package.package_exporter.pickletools",
    "fullName": "torch.package.package_exporter.pickletools.dis",
    "signature": "(pickle, out=None, memo=None, indentlevel=4, annotate=0)",
    "description": "Produce a symbolic disassembly of a pickle."
  },
  "2387": {
    "name": "genops",
    "module": "torch.package.package_exporter.pickletools",
    "fullName": "torch.package.package_exporter.pickletools.genops",
    "signature": "(pickle)",
    "description": "Generate all the opcodes in a pickle."
  },
  "2388": {
    "name": "optimize",
    "module": "torch.package.package_exporter.pickletools",
    "fullName": "torch.package.package_exporter.pickletools.optimize",
    "signature": "(p)",
    "description": "Optimize a pickle string by removing unused PUT opcodes"
  },
  "2389": {
    "name": "read_bytearray8",
    "module": "torch.package.package_exporter.pickletools",
    "fullName": "torch.package.package_exporter.pickletools.read_bytearray8",
    "signature": "(f)",
    "description": ">>> import io, struct, sys"
  },
  "2390": {
    "name": "read_bytes1",
    "module": "torch.package.package_exporter.pickletools",
    "fullName": "torch.package.package_exporter.pickletools.read_bytes1",
    "signature": "(f)",
    "description": ">>> import io"
  },
  "2391": {
    "name": "read_bytes4",
    "module": "torch.package.package_exporter.pickletools",
    "fullName": "torch.package.package_exporter.pickletools.read_bytes4",
    "signature": "(f)",
    "description": ">>> import io"
  },
  "2392": {
    "name": "read_bytes8",
    "module": "torch.package.package_exporter.pickletools",
    "fullName": "torch.package.package_exporter.pickletools.read_bytes8",
    "signature": "(f)",
    "description": ">>> import io, struct, sys"
  },
  "2393": {
    "name": "read_decimalnl_long",
    "module": "torch.package.package_exporter.pickletools",
    "fullName": "torch.package.package_exporter.pickletools.read_decimalnl_long",
    "signature": "(f)",
    "description": ">>> import io"
  },
  "2394": {
    "name": "read_decimalnl_short",
    "module": "torch.package.package_exporter.pickletools",
    "fullName": "torch.package.package_exporter.pickletools.read_decimalnl_short",
    "signature": "(f)",
    "description": ">>> import io"
  },
  "2395": {
    "name": "read_float8",
    "module": "torch.package.package_exporter.pickletools",
    "fullName": "torch.package.package_exporter.pickletools.read_float8",
    "signature": "(f)",
    "description": ">>> import io, struct"
  },
  "2396": {
    "name": "read_floatnl",
    "module": "torch.package.package_exporter.pickletools",
    "fullName": "torch.package.package_exporter.pickletools.read_floatnl",
    "signature": "(f)",
    "description": ">>> import io"
  },
  "2397": {
    "name": "read_int4",
    "module": "torch.package.package_exporter.pickletools",
    "fullName": "torch.package.package_exporter.pickletools.read_int4",
    "signature": "(f)",
    "description": ">>> import io"
  },
  "2398": {
    "name": "read_long1",
    "module": "torch.package.package_exporter.pickletools",
    "fullName": "torch.package.package_exporter.pickletools.read_long1",
    "signature": "(f)",
    "description": ">>> import io"
  },
  "2399": {
    "name": "read_long4",
    "module": "torch.package.package_exporter.pickletools",
    "fullName": "torch.package.package_exporter.pickletools.read_long4",
    "signature": "(f)",
    "description": ">>> import io"
  },
  "2400": {
    "name": "read_string1",
    "module": "torch.package.package_exporter.pickletools",
    "fullName": "torch.package.package_exporter.pickletools.read_string1",
    "signature": "(f)",
    "description": ">>> import io"
  },
  "2401": {
    "name": "read_string4",
    "module": "torch.package.package_exporter.pickletools",
    "fullName": "torch.package.package_exporter.pickletools.read_string4",
    "signature": "(f)",
    "description": ">>> import io"
  },
  "2402": {
    "name": "read_stringnl",
    "module": "torch.package.package_exporter.pickletools",
    "fullName": "torch.package.package_exporter.pickletools.read_stringnl",
    "signature": "(f, decode=True, stripquotes=True)",
    "description": ">>> import io"
  },
  "2403": {
    "name": "read_stringnl_noescape",
    "module": "torch.package.package_exporter.pickletools",
    "fullName": "torch.package.package_exporter.pickletools.read_stringnl_noescape",
    "signature": "(f)",
    "description": "No description available."
  },
  "2404": {
    "name": "read_stringnl_noescape_pair",
    "module": "torch.package.package_exporter.pickletools",
    "fullName": "torch.package.package_exporter.pickletools.read_stringnl_noescape_pair",
    "signature": "(f)",
    "description": ">>> import io"
  },
  "2405": {
    "name": "read_uint1",
    "module": "torch.package.package_exporter.pickletools",
    "fullName": "torch.package.package_exporter.pickletools.read_uint1",
    "signature": "(f)",
    "description": ">>> import io"
  },
  "2406": {
    "name": "read_uint2",
    "module": "torch.package.package_exporter.pickletools",
    "fullName": "torch.package.package_exporter.pickletools.read_uint2",
    "signature": "(f)",
    "description": ">>> import io"
  },
  "2407": {
    "name": "read_uint4",
    "module": "torch.package.package_exporter.pickletools",
    "fullName": "torch.package.package_exporter.pickletools.read_uint4",
    "signature": "(f)",
    "description": ">>> import io"
  },
  "2408": {
    "name": "read_uint8",
    "module": "torch.package.package_exporter.pickletools",
    "fullName": "torch.package.package_exporter.pickletools.read_uint8",
    "signature": "(f)",
    "description": ">>> import io"
  },
  "2409": {
    "name": "read_unicodestring1",
    "module": "torch.package.package_exporter.pickletools",
    "fullName": "torch.package.package_exporter.pickletools.read_unicodestring1",
    "signature": "(f)",
    "description": ">>> import io"
  },
  "2410": {
    "name": "read_unicodestring4",
    "module": "torch.package.package_exporter.pickletools",
    "fullName": "torch.package.package_exporter.pickletools.read_unicodestring4",
    "signature": "(f)",
    "description": ">>> import io"
  },
  "2411": {
    "name": "read_unicodestring8",
    "module": "torch.package.package_exporter.pickletools",
    "fullName": "torch.package.package_exporter.pickletools.read_unicodestring8",
    "signature": "(f)",
    "description": ">>> import io"
  },
  "2412": {
    "name": "read_unicodestringnl",
    "module": "torch.package.package_exporter.pickletools",
    "fullName": "torch.package.package_exporter.pickletools.read_unicodestringnl",
    "signature": "(f)",
    "description": ">>> import io"
  },
  "2413": {
    "name": "abstractmethod",
    "module": "torch.package.importer",
    "fullName": "torch.package.importer.abstractmethod",
    "signature": "(funcobj)",
    "description": "A decorator indicating abstract methods."
  },
  "2414": {
    "name": "demangle",
    "module": "torch.package.importer",
    "fullName": "torch.package.importer.demangle",
    "signature": "(name: str) -> str",
    "description": "Note: Unlike PackageMangler.demangle, this version works on any"
  },
  "2415": {
    "name": "get_mangle_prefix",
    "module": "torch.package.importer",
    "fullName": "torch.package.importer.get_mangle_prefix",
    "signature": "(name: str) -> str",
    "description": "No description available."
  },
  "2416": {
    "name": "is_mangled",
    "module": "torch.package.importer",
    "fullName": "torch.package.importer.is_mangled",
    "signature": "(name: str) -> bool",
    "description": "No description available."
  },
  "2417": {
    "name": "GlobPattern",
    "module": "torch.package.glob_group",
    "fullName": "torch.package.glob_group.GlobPattern",
    "signature": "(*args, **kwargs)",
    "description": "No description available."
  },
  "2418": {
    "name": "GlobPattern",
    "module": "torch.package.file_structure_representation",
    "fullName": "torch.package.file_structure_representation.GlobPattern",
    "signature": "(*args, **kwargs)",
    "description": "No description available."
  },
  "2419": {
    "name": "find_first_use_of_broken_modules",
    "module": "torch.package.analyze",
    "fullName": "torch.package.analyze.find_first_use_of_broken_modules",
    "signature": "(exc: torch.package.package_exporter.PackagingError) -> Dict[str, List[str]]",
    "description": "Find all broken modules in a PackagingError, and for each one, return the"
  },
  "2420": {
    "name": "trace_dependencies",
    "module": "torch.package.analyze",
    "fullName": "torch.package.analyze.trace_dependencies",
    "signature": "(callable: Callable[[Any], Any], inputs: Iterable[Tuple[Any, ...]]) -> List[str]",
    "description": "Trace the execution of a callable in order to determine which modules it uses."
  },
  "2421": {
    "name": "is_from_package",
    "module": "torch.package.analyze.is_from_package",
    "fullName": "torch.package.analyze.is_from_package.is_from_package",
    "signature": "(obj: Any) -> bool",
    "description": "Return whether an object was loaded from a package."
  },
  "2422": {
    "name": "is_mangled",
    "module": "torch.package.analyze.is_from_package",
    "fullName": "torch.package.analyze.is_from_package.is_mangled",
    "signature": "(name: str) -> bool",
    "description": "No description available."
  },
  "2423": {
    "name": "enable_torch_function_mode",
    "module": "torch.overrides",
    "fullName": "torch.overrides.enable_torch_function_mode",
    "signature": "(mode, *, replace=None, ignore_preexisting=False) -> Iterator[NoneType]",
    "description": "Context manager that sets the current :class:`TorchFunctionMode`; see the"
  },
  "2424": {
    "name": "get_default_nowrap_functions",
    "module": "torch.overrides",
    "fullName": "torch.overrides.get_default_nowrap_functions",
    "signature": "() -> Set[Callable]",
    "description": "Return public functions that do not wrap in a subclass when invoked by"
  },
  "2425": {
    "name": "get_ignored_functions",
    "module": "torch.overrides",
    "fullName": "torch.overrides.get_ignored_functions",
    "signature": "() -> Set[Callable]",
    "description": "Return public functions that cannot be overridden by ``__torch_function__``."
  },
  "2426": {
    "name": "get_overridable_functions",
    "module": "torch.overrides",
    "fullName": "torch.overrides.get_overridable_functions",
    "signature": "() -> Dict[Any, List[Callable]]",
    "description": "List functions that are overridable via __torch_function__"
  },
  "2427": {
    "name": "get_testing_overrides",
    "module": "torch.overrides",
    "fullName": "torch.overrides.get_testing_overrides",
    "signature": "() -> Dict[Callable, Callable]",
    "description": "Return a dict containing dummy overrides for all overridable functions"
  },
  "2428": {
    "name": "handle_torch_function",
    "module": "torch.overrides",
    "fullName": "torch.overrides.handle_torch_function",
    "signature": "(public_api: Callable, relevant_args: Iterable[Any], *args, **kwargs) -> Any",
    "description": "Implement a function with checks for ``__torch_function__`` overrides."
  },
  "2429": {
    "name": "is_tensor_like",
    "module": "torch.overrides",
    "fullName": "torch.overrides.is_tensor_like",
    "signature": "(inp)",
    "description": "Returns ``True`` if the passed-in input is a Tensor-like."
  },
  "2430": {
    "name": "is_tensor_method_or_property",
    "module": "torch.overrides",
    "fullName": "torch.overrides.is_tensor_method_or_property",
    "signature": "(func: Callable) -> bool",
    "description": "Returns True if the function passed in is a handler for a"
  },
  "2431": {
    "name": "push_torch_function_mode",
    "module": "torch.overrides",
    "fullName": "torch.overrides.push_torch_function_mode",
    "signature": "(ctor) -> Iterator[torch.overrides.TorchFunctionMode]",
    "description": "Context manager that pushes a :class:`TorchFunctionMode` onto the current"
  },
  "2432": {
    "name": "resolve_name",
    "module": "torch.overrides",
    "fullName": "torch.overrides.resolve_name",
    "signature": "(f)",
    "description": "Get a human readable string name for a function passed to"
  },
  "2433": {
    "name": "wrap_torch_function",
    "module": "torch.overrides",
    "fullName": "torch.overrides.wrap_torch_function",
    "signature": "(dispatcher: Callable)",
    "description": "Wraps a given function with ``__torch_function__`` -related functionality."
  },
  "2434": {
    "name": "deepcopy",
    "module": "torch.optim.swa_utils",
    "fullName": "torch.optim.swa_utils.deepcopy",
    "signature": "(x, memo=None, _nil=[])",
    "description": "Deep copy operation on arbitrary Python objects."
  },
  "2435": {
    "name": "update_bn",
    "module": "torch.optim.swa_utils",
    "fullName": "torch.optim.swa_utils.update_bn",
    "signature": "(loader, model, device=None)",
    "description": "Updates BatchNorm running_mean, running_var buffers in the model."
  },
  "2436": {
    "name": "wraps",
    "module": "torch.optim.lr_scheduler",
    "fullName": "torch.optim.lr_scheduler.wraps",
    "signature": "(wrapped, assigned=('__module__', '__name__', '__qualname__', '__doc__', '__annotations__'), updated=('__dict__',))",
    "description": "Decorator factory to apply update_wrapper() to a wrapper function"
  },
  "2437": {
    "name": "disable_log",
    "module": "torch.onnx",
    "fullName": "torch.onnx.disable_log",
    "signature": "()",
    "description": "Disables ONNX logging."
  },
  "2438": {
    "name": "enable_log",
    "module": "torch.onnx",
    "fullName": "torch.onnx.enable_log",
    "signature": "()",
    "description": "Enables ONNX logging."
  },
  "2439": {
    "name": "export",
    "module": "torch.onnx",
    "fullName": "torch.onnx.export",
    "signature": "(model, args, f, export_params=True, verbose=False, training=<TrainingMode.EVAL: 0>, input_names=None, output_names=None, operator_export_type=<OperatorExportTypes.ONNX: 0>, opset_version=None, do_constant_folding=True, dynamic_axes=None, keep_initializers_as_inputs=None, custom_opsets=None, export_modules_as_functions=False)",
    "description": "Exports a model into ONNX format. If ``model`` is not a"
  },
  "2440": {
    "name": "export_to_pretty_string",
    "module": "torch.onnx",
    "fullName": "torch.onnx.export_to_pretty_string",
    "signature": "(*args, **kwargs) -> str",
    "description": "Similar to :func:`export`, but returns a text representation of the ONNX"
  },
  "2441": {
    "name": "is_in_onnx_export",
    "module": "torch.onnx",
    "fullName": "torch.onnx.is_in_onnx_export",
    "signature": "()",
    "description": "Returns True iff :func:`export` is running in the current thread"
  },
  "2442": {
    "name": "is_onnx_log_enabled",
    "module": "torch.onnx",
    "fullName": "torch.onnx.is_onnx_log_enabled",
    "signature": "()",
    "description": "Returns True iff ONNX logging is turned on."
  },
  "2443": {
    "name": "log",
    "module": "torch.onnx",
    "fullName": "torch.onnx.log",
    "signature": "(*args)",
    "description": "A simple logging facility for ONNX exporter."
  },
  "2444": {
    "name": "register_custom_op_symbolic",
    "module": "torch.onnx",
    "fullName": "torch.onnx.register_custom_op_symbolic",
    "signature": "(symbolic_name, symbolic_fn, opset_version)",
    "description": "Registers ``symbolic_fn`` to handle ``symbolic_name``. See"
  },
  "2445": {
    "name": "select_model_mode_for_export",
    "module": "torch.onnx",
    "fullName": "torch.onnx.select_model_mode_for_export",
    "signature": "(model, mode)",
    "description": "A context manager to temporarily set the training mode of ``model``"
  },
  "2446": {
    "name": "set_log_stream",
    "module": "torch.onnx",
    "fullName": "torch.onnx.set_log_stream",
    "signature": "(stream_name='stdout')",
    "description": "Set output stream for ONNX logging."
  },
  "2447": {
    "name": "unregister_custom_op_symbolic",
    "module": "torch.onnx",
    "fullName": "torch.onnx.unregister_custom_op_symbolic",
    "signature": "(symbolic_name, opset_version)",
    "description": "Unregisters ``symbolic_name``. See"
  },
  "2448": {
    "name": "factory_kwargs",
    "module": "torch.nn",
    "fullName": "torch.nn.factory_kwargs",
    "signature": "(kwargs)",
    "description": "Given kwargs, returns a canonicalized dict of factory kwargs that can be directly passed"
  },
  "2449": {
    "name": "clip_grad_norm_",
    "module": "torch.nn.utils",
    "fullName": "torch.nn.utils.clip_grad_norm_",
    "signature": "(parameters: Union[torch.Tensor, Iterable[torch.Tensor]], max_norm: float, norm_type: float = 2.0, error_if_nonfinite: bool = False) -> torch.Tensor",
    "description": "Clips gradient norm of an iterable of parameters."
  },
  "2450": {
    "name": "clip_grad_value_",
    "module": "torch.nn.utils",
    "fullName": "torch.nn.utils.clip_grad_value_",
    "signature": "(parameters: Union[torch.Tensor, Iterable[torch.Tensor]], clip_value: float) -> None",
    "description": "Clips gradient of an iterable of parameters at specified value."
  },
  "2451": {
    "name": "convert_conv2d_weight_memory_format",
    "module": "torch.nn.utils",
    "fullName": "torch.nn.utils.convert_conv2d_weight_memory_format",
    "signature": "(module, memory_format)",
    "description": "Convert ``memory_format`` of ``nn.Conv2d.weight`` to ``memory_format``"
  },
  "2452": {
    "name": "fuse_conv_bn_eval",
    "module": "torch.nn.utils",
    "fullName": "torch.nn.utils.fuse_conv_bn_eval",
    "signature": "(conv, bn, transpose=False)",
    "description": "No description available."
  },
  "2453": {
    "name": "fuse_conv_bn_weights",
    "module": "torch.nn.utils",
    "fullName": "torch.nn.utils.fuse_conv_bn_weights",
    "signature": "(conv_w, conv_b, bn_rm, bn_rv, bn_eps, bn_w, bn_b, transpose=False)",
    "description": "No description available."
  },
  "2454": {
    "name": "parameters_to_vector",
    "module": "torch.nn.utils",
    "fullName": "torch.nn.utils.parameters_to_vector",
    "signature": "(parameters: Iterable[torch.Tensor]) -> torch.Tensor",
    "description": "Convert parameters to one vector"
  },
  "2455": {
    "name": "remove_spectral_norm",
    "module": "torch.nn.utils",
    "fullName": "torch.nn.utils.remove_spectral_norm",
    "signature": "(module: ~T_module, name: str = 'weight') -> ~T_module",
    "description": "Removes the spectral normalization reparameterization from a module."
  },
  "2456": {
    "name": "remove_weight_norm",
    "module": "torch.nn.utils",
    "fullName": "torch.nn.utils.remove_weight_norm",
    "signature": "(module: ~T_module, name: str = 'weight') -> ~T_module",
    "description": "Removes the weight normalization reparameterization from a module."
  },
  "2457": {
    "name": "skip_init",
    "module": "torch.nn.utils",
    "fullName": "torch.nn.utils.skip_init",
    "signature": "(module_cls, *args, **kwargs)",
    "description": "Given a module class object and args / kwargs, instantiates the module without initializing"
  },
  "2458": {
    "name": "vector_to_parameters",
    "module": "torch.nn.utils",
    "fullName": "torch.nn.utils.vector_to_parameters",
    "signature": "(vec: torch.Tensor, parameters: Iterable[torch.Tensor]) -> None",
    "description": "Convert one vector to the parameters"
  },
  "2459": {
    "name": "weight_norm",
    "module": "torch.nn.utils",
    "fullName": "torch.nn.utils.weight_norm",
    "signature": "(module: ~T_module, name: str = 'weight', dim: int = 0) -> ~T_module",
    "description": "Applies weight normalization to a parameter in the given module."
  },
  "2460": {
    "name": "functional_call",
    "module": "torch.nn.utils.stateless",
    "fullName": "torch.nn.utils.stateless.functional_call",
    "signature": "(module: 'torch.nn.Module', parameters_and_buffers: Dict[str, torch.Tensor], args: Tuple, kwargs: Dict[str, Any] = None)",
    "description": "Performs a functional call on the module by replacing the module parameters"
  },
  "2461": {
    "name": "bind",
    "module": "torch.nn.utils.rnn",
    "fullName": "torch.nn.utils.rnn.bind",
    "signature": "(optional, fn)",
    "description": "No description available."
  },
  "2462": {
    "name": "invert_permutation",
    "module": "torch.nn.utils.rnn",
    "fullName": "torch.nn.utils.rnn.invert_permutation",
    "signature": "(permutation: Optional[torch.Tensor]) -> Optional[torch.Tensor]",
    "description": "No description available."
  },
  "2463": {
    "name": "namedtuple",
    "module": "torch.nn.utils.rnn",
    "fullName": "torch.nn.utils.rnn.namedtuple",
    "signature": "(typename, field_names, *, rename=False, defaults=None, module=None)",
    "description": "Returns a new subclass of tuple with named fields."
  },
  "2464": {
    "name": "pack_padded_sequence",
    "module": "torch.nn.utils.rnn",
    "fullName": "torch.nn.utils.rnn.pack_padded_sequence",
    "signature": "(input: torch.Tensor, lengths: torch.Tensor, batch_first: bool = False, enforce_sorted: bool = True) -> torch.nn.utils.rnn.PackedSequence",
    "description": "Packs a Tensor containing padded sequences of variable length."
  },
  "2465": {
    "name": "pack_sequence",
    "module": "torch.nn.utils.rnn",
    "fullName": "torch.nn.utils.rnn.pack_sequence",
    "signature": "(sequences: List[torch.Tensor], enforce_sorted: bool = True) -> torch.nn.utils.rnn.PackedSequence",
    "description": "Packs a list of variable length Tensors"
  },
  "2466": {
    "name": "pad_packed_sequence",
    "module": "torch.nn.utils.rnn",
    "fullName": "torch.nn.utils.rnn.pad_packed_sequence",
    "signature": "(sequence: torch.nn.utils.rnn.PackedSequence, batch_first: bool = False, padding_value: float = 0.0, total_length: Optional[int] = None) -> Tuple[torch.Tensor, torch.Tensor]",
    "description": "Pads a packed batch of variable length sequences."
  },
  "2467": {
    "name": "pad_sequence",
    "module": "torch.nn.utils.rnn",
    "fullName": "torch.nn.utils.rnn.pad_sequence",
    "signature": "(sequences: Union[torch.Tensor, List[torch.Tensor]], batch_first: bool = False, padding_value: float = 0.0) -> torch.Tensor",
    "description": "Pad a list of variable length Tensors with ``padding_value``"
  },
  "2468": {
    "name": "unpack_sequence",
    "module": "torch.nn.utils.rnn",
    "fullName": "torch.nn.utils.rnn.unpack_sequence",
    "signature": "(packed_sequences: torch.nn.utils.rnn.PackedSequence) -> List[torch.Tensor]",
    "description": "Unpacks PackedSequence into a list of variable length Tensors"
  },
  "2469": {
    "name": "unpad_sequence",
    "module": "torch.nn.utils.rnn",
    "fullName": "torch.nn.utils.rnn.unpad_sequence",
    "signature": "(padded_sequences: torch.Tensor, lengths: torch.Tensor, batch_first: bool = False) -> List[torch.Tensor]",
    "description": "Unpad padded Tensor into a list of variable length Tensors"
  },
  "2470": {
    "name": "cached",
    "module": "torch.nn.utils.parametrize",
    "fullName": "torch.nn.utils.parametrize.cached",
    "signature": "()",
    "description": "Context manager that enables the caching system within parametrizations"
  },
  "2471": {
    "name": "contextmanager",
    "module": "torch.nn.utils.parametrize",
    "fullName": "torch.nn.utils.parametrize.contextmanager",
    "signature": "(func)",
    "description": "@contextmanager decorator."
  },
  "2472": {
    "name": "is_parametrized",
    "module": "torch.nn.utils.parametrize",
    "fullName": "torch.nn.utils.parametrize.is_parametrized",
    "signature": "(module: torch.nn.modules.module.Module, tensor_name: Optional[str] = None) -> bool",
    "description": "Returns ``True`` if module has an active parametrization."
  },
  "2473": {
    "name": "register_parametrization",
    "module": "torch.nn.utils.parametrize",
    "fullName": "torch.nn.utils.parametrize.register_parametrization",
    "signature": "(module: torch.nn.modules.module.Module, tensor_name: str, parametrization: torch.nn.modules.module.Module, *, unsafe: bool = False) -> torch.nn.modules.module.Module",
    "description": "Adds a parametrization to a tensor in a module."
  },
  "2474": {
    "name": "remove_parametrizations",
    "module": "torch.nn.utils.parametrize",
    "fullName": "torch.nn.utils.parametrize.remove_parametrizations",
    "signature": "(module: torch.nn.modules.module.Module, tensor_name: str, leave_parametrized: bool = True) -> torch.nn.modules.module.Module",
    "description": "Removes the parametrizations on a tensor in a module."
  },
  "2475": {
    "name": "transfer_parametrizations_and_params",
    "module": "torch.nn.utils.parametrize",
    "fullName": "torch.nn.utils.parametrize.transfer_parametrizations_and_params",
    "signature": "(from_module: torch.nn.modules.module.Module, to_module: torch.nn.modules.module.Module, tensor_name: Optional[str] = None) -> torch.nn.modules.module.Module",
    "description": "Transfers parametrizations and the parameters they parametrize from from_module"
  },
  "2476": {
    "name": "type_before_parametrizations",
    "module": "torch.nn.utils.parametrize",
    "fullName": "torch.nn.utils.parametrize.type_before_parametrizations",
    "signature": "(module: torch.nn.modules.module.Module) -> type",
    "description": "Returns the module type before parametrizations were applied and if not,"
  },
  "2477": {
    "name": "orthogonal",
    "module": "torch.nn.utils.parametrizations",
    "fullName": "torch.nn.utils.parametrizations.orthogonal",
    "signature": "(module: torch.nn.modules.module.Module, name: str = 'weight', orthogonal_map: Optional[str] = None, *, use_trivialization: bool = True) -> torch.nn.modules.module.Module",
    "description": "Applies an orthogonal or unitary parametrization to a matrix or a batch of matrices."
  },
  "2478": {
    "name": "spectral_norm",
    "module": "torch.nn.utils.parametrizations",
    "fullName": "torch.nn.utils.parametrizations.spectral_norm",
    "signature": "(module: torch.nn.modules.module.Module, name: str = 'weight', n_power_iterations: int = 1, eps: float = 1e-12, dim: Optional[int] = None) -> torch.nn.modules.module.Module",
    "description": "Applies spectral normalization to a parameter in the given module."
  },
  "2479": {
    "name": "adaptive_avg_pool2d",
    "module": "torch.nn.utils.parametrizations.F",
    "fullName": "torch.nn.utils.parametrizations.F.adaptive_avg_pool2d",
    "signature": "(input: torch.Tensor, output_size: None) -> torch.Tensor",
    "description": "Applies a 2D adaptive average pooling over an input signal composed of"
  },
  "2480": {
    "name": "adaptive_avg_pool3d",
    "module": "torch.nn.utils.parametrizations.F",
    "fullName": "torch.nn.utils.parametrizations.F.adaptive_avg_pool3d",
    "signature": "(input: torch.Tensor, output_size: None) -> torch.Tensor",
    "description": "Applies a 3D adaptive average pooling over an input signal composed of"
  },
  "2481": {
    "name": "adaptive_max_pool1d",
    "module": "torch.nn.utils.parametrizations.F",
    "fullName": "torch.nn.utils.parametrizations.F.adaptive_max_pool1d",
    "signature": "(*args, **kwargs)",
    "description": "Applies a 1D adaptive max pooling over an input signal composed of"
  },
  "2482": {
    "name": "adaptive_max_pool1d_with_indices",
    "module": "torch.nn.utils.parametrizations.F",
    "fullName": "torch.nn.utils.parametrizations.F.adaptive_max_pool1d_with_indices",
    "signature": "(input: torch.Tensor, output_size: None, return_indices: bool = False) -> Tuple[torch.Tensor, torch.Tensor]",
    "description": "Applies a 1D adaptive max pooling over an input signal composed of"
  },
  "2483": {
    "name": "adaptive_max_pool2d",
    "module": "torch.nn.utils.parametrizations.F",
    "fullName": "torch.nn.utils.parametrizations.F.adaptive_max_pool2d",
    "signature": "(*args, **kwargs)",
    "description": "Applies a 2D adaptive max pooling over an input signal composed of"
  },
  "2484": {
    "name": "adaptive_max_pool2d_with_indices",
    "module": "torch.nn.utils.parametrizations.F",
    "fullName": "torch.nn.utils.parametrizations.F.adaptive_max_pool2d_with_indices",
    "signature": "(input: torch.Tensor, output_size: None, return_indices: bool = False) -> Tuple[torch.Tensor, torch.Tensor]",
    "description": "Applies a 2D adaptive max pooling over an input signal composed of"
  },
  "2485": {
    "name": "adaptive_max_pool3d",
    "module": "torch.nn.utils.parametrizations.F",
    "fullName": "torch.nn.utils.parametrizations.F.adaptive_max_pool3d",
    "signature": "(*args, **kwargs)",
    "description": "Applies a 3D adaptive max pooling over an input signal composed of"
  },
  "2486": {
    "name": "adaptive_max_pool3d_with_indices",
    "module": "torch.nn.utils.parametrizations.F",
    "fullName": "torch.nn.utils.parametrizations.F.adaptive_max_pool3d_with_indices",
    "signature": "(input: torch.Tensor, output_size: None, return_indices: bool = False) -> Tuple[torch.Tensor, torch.Tensor]",
    "description": "Applies a 3D adaptive max pooling over an input signal composed of"
  },
  "2487": {
    "name": "affine_grid",
    "module": "torch.nn.utils.parametrizations.F",
    "fullName": "torch.nn.utils.parametrizations.F.affine_grid",
    "signature": "(theta: torch.Tensor, size: List[int], align_corners: Optional[bool] = None) -> torch.Tensor",
    "description": "Generates a 2D or 3D flow field (sampling grid), given a batch of"
  },
  "2488": {
    "name": "alpha_dropout",
    "module": "torch.nn.utils.parametrizations.F",
    "fullName": "torch.nn.utils.parametrizations.F.alpha_dropout",
    "signature": "(input: torch.Tensor, p: float = 0.5, training: bool = False, inplace: bool = False) -> torch.Tensor",
    "description": "Applies alpha dropout to the input."
  },
  "2489": {
    "name": "assert_int_or_pair",
    "module": "torch.nn.utils.parametrizations.F",
    "fullName": "torch.nn.utils.parametrizations.F.assert_int_or_pair",
    "signature": "(arg: List[int], arg_name: str, message: str) -> None",
    "description": "No description available."
  },
  "2490": {
    "name": "batch_norm",
    "module": "torch.nn.utils.parametrizations.F",
    "fullName": "torch.nn.utils.parametrizations.F.batch_norm",
    "signature": "(input: torch.Tensor, running_mean: Optional[torch.Tensor], running_var: Optional[torch.Tensor], weight: Optional[torch.Tensor] = None, bias: Optional[torch.Tensor] = None, training: bool = False, momentum: float = 0.1, eps: float = 1e-05) -> torch.Tensor",
    "description": "Applies Batch Normalization for each channel across a batch of data."
  },
  "2491": {
    "name": "boolean_dispatch",
    "module": "torch.nn.utils.parametrizations.F",
    "fullName": "torch.nn.utils.parametrizations.F.boolean_dispatch",
    "signature": "(arg_name, arg_index, default, if_true, if_false, module_name, func_name)",
    "description": "Dispatches to either of 2 script functions based on a boolean argument."
  },
  "2492": {
    "name": "celu",
    "module": "torch.nn.utils.parametrizations.F",
    "fullName": "torch.nn.utils.parametrizations.F.celu",
    "signature": "(input: torch.Tensor, alpha: float = 1.0, inplace: bool = False) -> torch.Tensor",
    "description": "celu(input, alpha=1., inplace=False) -> Tensor"
  },
  "2493": {
    "name": "cosine_embedding_loss",
    "module": "torch.nn.utils.parametrizations.F",
    "fullName": "torch.nn.utils.parametrizations.F.cosine_embedding_loss",
    "signature": "(input1: torch.Tensor, input2: torch.Tensor, target: torch.Tensor, margin: float = 0, size_average: Optional[bool] = None, reduce: Optional[bool] = None, reduction: str = 'mean') -> torch.Tensor",
    "description": "cosine_embedding_loss(input1, input2, target, margin=0, size_average=None, reduce=None, reduction='mean') -> Tensor"
  },
  "2494": {
    "name": "ctc_loss",
    "module": "torch.nn.utils.parametrizations.F",
    "fullName": "torch.nn.utils.parametrizations.F.ctc_loss",
    "signature": "(log_probs: torch.Tensor, targets: torch.Tensor, input_lengths: torch.Tensor, target_lengths: torch.Tensor, blank: int = 0, reduction: str = 'mean', zero_infinity: bool = False) -> torch.Tensor",
    "description": "The Connectionist Temporal Classification loss."
  },
  "2495": {
    "name": "dropout",
    "module": "torch.nn.utils.parametrizations.F",
    "fullName": "torch.nn.utils.parametrizations.F.dropout",
    "signature": "(input: torch.Tensor, p: float = 0.5, training: bool = True, inplace: bool = False) -> torch.Tensor",
    "description": "During training, randomly zeroes some of the elements of the input"
  },
  "2496": {
    "name": "dropout1d",
    "module": "torch.nn.utils.parametrizations.F",
    "fullName": "torch.nn.utils.parametrizations.F.dropout1d",
    "signature": "(input: torch.Tensor, p: float = 0.5, training: bool = True, inplace: bool = False) -> torch.Tensor",
    "description": "Randomly zero out entire channels (a channel is a 1D feature map,"
  },
  "2497": {
    "name": "dropout2d",
    "module": "torch.nn.utils.parametrizations.F",
    "fullName": "torch.nn.utils.parametrizations.F.dropout2d",
    "signature": "(input: torch.Tensor, p: float = 0.5, training: bool = True, inplace: bool = False) -> torch.Tensor",
    "description": "Randomly zero out entire channels (a channel is a 2D feature map,"
  },
  "2498": {
    "name": "dropout3d",
    "module": "torch.nn.utils.parametrizations.F",
    "fullName": "torch.nn.utils.parametrizations.F.dropout3d",
    "signature": "(input: torch.Tensor, p: float = 0.5, training: bool = True, inplace: bool = False) -> torch.Tensor",
    "description": "Randomly zero out entire channels (a channel is a 3D feature map,"
  },
  "2499": {
    "name": "elu",
    "module": "torch.nn.utils.parametrizations.F",
    "fullName": "torch.nn.utils.parametrizations.F.elu",
    "signature": "(input: torch.Tensor, alpha: float = 1.0, inplace: bool = False) -> torch.Tensor",
    "description": "Applies element-wise,"
  },
  "2500": {
    "name": "embedding",
    "module": "torch.nn.utils.parametrizations.F",
    "fullName": "torch.nn.utils.parametrizations.F.embedding",
    "signature": "(input: torch.Tensor, weight: torch.Tensor, padding_idx: Optional[int] = None, max_norm: Optional[float] = None, norm_type: float = 2.0, scale_grad_by_freq: bool = False, sparse: bool = False) -> torch.Tensor",
    "description": "A simple lookup table that looks up embeddings in a fixed dictionary and size."
  },
  "2501": {
    "name": "embedding_bag",
    "module": "torch.nn.utils.parametrizations.F",
    "fullName": "torch.nn.utils.parametrizations.F.embedding_bag",
    "signature": "(input: torch.Tensor, weight: torch.Tensor, offsets: Optional[torch.Tensor] = None, max_norm: Optional[float] = None, norm_type: float = 2, scale_grad_by_freq: bool = False, mode: str = 'mean', sparse: bool = False, per_sample_weights: Optional[torch.Tensor] = None, include_last_offset: bool = False, padding_idx: Optional[int] = None) -> torch.Tensor",
    "description": "Computes sums, means or maxes of `bags` of embeddings, without instantiating the"
  },
  "2502": {
    "name": "feature_alpha_dropout",
    "module": "torch.nn.utils.parametrizations.F",
    "fullName": "torch.nn.utils.parametrizations.F.feature_alpha_dropout",
    "signature": "(input: torch.Tensor, p: float = 0.5, training: bool = False, inplace: bool = False) -> torch.Tensor",
    "description": "Randomly masks out entire channels (a channel is a feature map,"
  },
  "2503": {
    "name": "fold",
    "module": "torch.nn.utils.parametrizations.F",
    "fullName": "torch.nn.utils.parametrizations.F.fold",
    "signature": "(input: torch.Tensor, output_size: None, kernel_size: None, dilation: None = 1, padding: None = 0, stride: None = 1) -> torch.Tensor",
    "description": "Combines an array of sliding local blocks into a large containing"
  },
  "2504": {
    "name": "fractional_max_pool2d",
    "module": "torch.nn.utils.parametrizations.F",
    "fullName": "torch.nn.utils.parametrizations.F.fractional_max_pool2d",
    "signature": "(*args, **kwargs)",
    "description": "Applies 2D fractional max pooling over an input signal composed of several input planes."
  },
  "2505": {
    "name": "fractional_max_pool2d_with_indices",
    "module": "torch.nn.utils.parametrizations.F",
    "fullName": "torch.nn.utils.parametrizations.F.fractional_max_pool2d_with_indices",
    "signature": "(input: torch.Tensor, kernel_size: None, output_size: NoneType = None, output_ratio: NoneType = None, return_indices: bool = False, _random_samples: Optional[torch.Tensor] = None) -> Tuple[torch.Tensor, torch.Tensor]",
    "description": "Applies 2D fractional max pooling over an input signal composed of several input planes."
  },
  "2506": {
    "name": "fractional_max_pool3d",
    "module": "torch.nn.utils.parametrizations.F",
    "fullName": "torch.nn.utils.parametrizations.F.fractional_max_pool3d",
    "signature": "(*args, **kwargs)",
    "description": "Applies 3D fractional max pooling over an input signal composed of several input planes."
  },
  "2507": {
    "name": "fractional_max_pool3d_with_indices",
    "module": "torch.nn.utils.parametrizations.F",
    "fullName": "torch.nn.utils.parametrizations.F.fractional_max_pool3d_with_indices",
    "signature": "(input: torch.Tensor, kernel_size: None, output_size: NoneType = None, output_ratio: NoneType = None, return_indices: bool = False, _random_samples: Optional[torch.Tensor] = None) -> Tuple[torch.Tensor, torch.Tensor]",
    "description": "Applies 3D fractional max pooling over an input signal composed of several input planes."
  },
  "2508": {
    "name": "gaussian_nll_loss",
    "module": "torch.nn.utils.parametrizations.F",
    "fullName": "torch.nn.utils.parametrizations.F.gaussian_nll_loss",
    "signature": "(input: torch.Tensor, target: torch.Tensor, var: torch.Tensor, full: bool = False, eps: float = 1e-06, reduction: str = 'mean') -> torch.Tensor",
    "description": "Gaussian negative log likelihood loss."
  },
  "2509": {
    "name": "glu",
    "module": "torch.nn.utils.parametrizations.F",
    "fullName": "torch.nn.utils.parametrizations.F.glu",
    "signature": "(input: torch.Tensor, dim: int = -1) -> torch.Tensor",
    "description": "glu(input, dim=-1) -> Tensor"
  },
  "2510": {
    "name": "grid_sample",
    "module": "torch.nn.utils.parametrizations.F",
    "fullName": "torch.nn.utils.parametrizations.F.grid_sample",
    "signature": "(input: torch.Tensor, grid: torch.Tensor, mode: str = 'bilinear', padding_mode: str = 'zeros', align_corners: Optional[bool] = None) -> torch.Tensor",
    "description": "Given an :attr:`input` and a flow-field :attr:`grid`, computes the"
  },
  "2511": {
    "name": "group_norm",
    "module": "torch.nn.utils.parametrizations.F",
    "fullName": "torch.nn.utils.parametrizations.F.group_norm",
    "signature": "(input: torch.Tensor, num_groups: int, weight: Optional[torch.Tensor] = None, bias: Optional[torch.Tensor] = None, eps: float = 1e-05) -> torch.Tensor",
    "description": "Applies Group Normalization for last certain number of dimensions."
  },
  "2512": {
    "name": "gumbel_softmax",
    "module": "torch.nn.utils.parametrizations.F",
    "fullName": "torch.nn.utils.parametrizations.F.gumbel_softmax",
    "signature": "(logits: torch.Tensor, tau: float = 1, hard: bool = False, eps: float = 1e-10, dim: int = -1) -> torch.Tensor",
    "description": "Samples from the Gumbel-Softmax distribution (`Link 1`_  `Link 2`_) and optionally discretizes."
  },
  "2513": {
    "name": "handle_torch_function",
    "module": "torch.nn.utils.parametrizations.F",
    "fullName": "torch.nn.utils.parametrizations.F.handle_torch_function",
    "signature": "(public_api: Callable, relevant_args: Iterable[Any], *args, **kwargs) -> Any",
    "description": "Implement a function with checks for ``__torch_function__`` overrides."
  },
  "2514": {
    "name": "hardsigmoid",
    "module": "torch.nn.utils.parametrizations.F",
    "fullName": "torch.nn.utils.parametrizations.F.hardsigmoid",
    "signature": "(input: torch.Tensor, inplace: bool = False) -> torch.Tensor",
    "description": "Applies the element-wise function"
  },
  "2515": {
    "name": "hardswish",
    "module": "torch.nn.utils.parametrizations.F",
    "fullName": "torch.nn.utils.parametrizations.F.hardswish",
    "signature": "(input: torch.Tensor, inplace: bool = False) -> torch.Tensor",
    "description": "Applies the hardswish function, element-wise, as described in the paper:"
  },
  "2516": {
    "name": "hardtanh",
    "module": "torch.nn.utils.parametrizations.F",
    "fullName": "torch.nn.utils.parametrizations.F.hardtanh",
    "signature": "(input: torch.Tensor, min_val: float = -1.0, max_val: float = 1.0, inplace: bool = False) -> torch.Tensor",
    "description": "hardtanh(input, min_val=-1., max_val=1., inplace=False) -> Tensor"
  },
  "2517": {
    "name": "hinge_embedding_loss",
    "module": "torch.nn.utils.parametrizations.F",
    "fullName": "torch.nn.utils.parametrizations.F.hinge_embedding_loss",
    "signature": "(input: torch.Tensor, target: torch.Tensor, margin: float = 1.0, size_average: Optional[bool] = None, reduce: Optional[bool] = None, reduction: str = 'mean') -> torch.Tensor",
    "description": "hinge_embedding_loss(input, target, margin=1.0, size_average=None, reduce=None, reduction='mean') -> Tensor"
  },
  "2518": {
    "name": "huber_loss",
    "module": "torch.nn.utils.parametrizations.F",
    "fullName": "torch.nn.utils.parametrizations.F.huber_loss",
    "signature": "(input: torch.Tensor, target: torch.Tensor, reduction: str = 'mean', delta: float = 1.0) -> torch.Tensor",
    "description": "Function that uses a squared term if the absolute"
  },
  "2519": {
    "name": "instance_norm",
    "module": "torch.nn.utils.parametrizations.F",
    "fullName": "torch.nn.utils.parametrizations.F.instance_norm",
    "signature": "(input: torch.Tensor, running_mean: Optional[torch.Tensor] = None, running_var: Optional[torch.Tensor] = None, weight: Optional[torch.Tensor] = None, bias: Optional[torch.Tensor] = None, use_input_stats: bool = True, momentum: float = 0.1, eps: float = 1e-05) -> torch.Tensor",
    "description": "Applies Instance Normalization for each channel in each data sample in a"
  },
  "2520": {
    "name": "interpolate",
    "module": "torch.nn.utils.parametrizations.F",
    "fullName": "torch.nn.utils.parametrizations.F.interpolate",
    "signature": "(input: torch.Tensor, size: Optional[int] = None, scale_factor: Optional[List[float]] = None, mode: str = 'nearest', align_corners: Optional[bool] = None, recompute_scale_factor: Optional[bool] = None, antialias: bool = False) -> torch.Tensor",
    "description": "Down/up samples the input to either the given :attr:`size` or the given"
  },
  "2521": {
    "name": "l1_loss",
    "module": "torch.nn.utils.parametrizations.F",
    "fullName": "torch.nn.utils.parametrizations.F.l1_loss",
    "signature": "(input: torch.Tensor, target: torch.Tensor, size_average: Optional[bool] = None, reduce: Optional[bool] = None, reduction: str = 'mean') -> torch.Tensor",
    "description": "l1_loss(input, target, size_average=None, reduce=None, reduction='mean') -> Tensor"
  },
  "2522": {
    "name": "layer_norm",
    "module": "torch.nn.utils.parametrizations.F",
    "fullName": "torch.nn.utils.parametrizations.F.layer_norm",
    "signature": "(input: torch.Tensor, normalized_shape: List[int], weight: Optional[torch.Tensor] = None, bias: Optional[torch.Tensor] = None, eps: float = 1e-05) -> torch.Tensor",
    "description": "Applies Layer Normalization for last certain number of dimensions."
  },
  "2523": {
    "name": "leaky_relu",
    "module": "torch.nn.utils.parametrizations.F",
    "fullName": "torch.nn.utils.parametrizations.F.leaky_relu",
    "signature": "(input: torch.Tensor, negative_slope: float = 0.01, inplace: bool = False) -> torch.Tensor",
    "description": "leaky_relu(input, negative_slope=0.01, inplace=False) -> Tensor"
  },
  "2524": {
    "name": "local_response_norm",
    "module": "torch.nn.utils.parametrizations.F",
    "fullName": "torch.nn.utils.parametrizations.F.local_response_norm",
    "signature": "(input: torch.Tensor, size: int, alpha: float = 0.0001, beta: float = 0.75, k: float = 1.0) -> torch.Tensor",
    "description": "Applies local response normalization over an input signal composed of"
  },
  "2525": {
    "name": "log_softmax",
    "module": "torch.nn.utils.parametrizations.F",
    "fullName": "torch.nn.utils.parametrizations.F.log_softmax",
    "signature": "(input: torch.Tensor, dim: Optional[int] = None, _stacklevel: int = 3, dtype: Optional[int] = None) -> torch.Tensor",
    "description": "Applies a softmax followed by a logarithm."
  },
  "2526": {
    "name": "lp_pool1d",
    "module": "torch.nn.utils.parametrizations.F",
    "fullName": "torch.nn.utils.parametrizations.F.lp_pool1d",
    "signature": "(input: torch.Tensor, norm_type: Union[int, float], kernel_size: int, stride: NoneType = None, ceil_mode: bool = False) -> torch.Tensor",
    "description": "Applies a 1D power-average pooling over an input signal composed of"
  },
  "2527": {
    "name": "lp_pool2d",
    "module": "torch.nn.utils.parametrizations.F",
    "fullName": "torch.nn.utils.parametrizations.F.lp_pool2d",
    "signature": "(input: torch.Tensor, norm_type: Union[int, float], kernel_size: None, stride: NoneType = None, ceil_mode: bool = False) -> torch.Tensor",
    "description": "Applies a 2D power-average pooling over an input signal composed of"
  },
  "2528": {
    "name": "margin_ranking_loss",
    "module": "torch.nn.utils.parametrizations.F",
    "fullName": "torch.nn.utils.parametrizations.F.margin_ranking_loss",
    "signature": "(input1: torch.Tensor, input2: torch.Tensor, target: torch.Tensor, margin: float = 0, size_average: Optional[bool] = None, reduce: Optional[bool] = None, reduction: str = 'mean') -> torch.Tensor",
    "description": "margin_ranking_loss(input1, input2, target, margin=0, size_average=None, reduce=None, reduction='mean') -> Tensor"
  },
  "2529": {
    "name": "max_pool1d",
    "module": "torch.nn.utils.parametrizations.F",
    "fullName": "torch.nn.utils.parametrizations.F.max_pool1d",
    "signature": "(*args, **kwargs)",
    "description": "max_pool1d(input, kernel_size, stride=None, padding=0, dilation=1, ceil_mode=False, return_indices=False)"
  },
  "2530": {
    "name": "max_pool1d_with_indices",
    "module": "torch.nn.utils.parametrizations.F",
    "fullName": "torch.nn.utils.parametrizations.F.max_pool1d_with_indices",
    "signature": "(input: torch.Tensor, kernel_size: None, stride: NoneType = None, padding: None = 0, dilation: None = 1, ceil_mode: bool = False, return_indices: bool = False) -> Tuple[torch.Tensor, torch.Tensor]",
    "description": "max_pool1d(input, kernel_size, stride=None, padding=0, dilation=1, ceil_mode=False, return_indices=False)"
  },
  "2531": {
    "name": "max_pool2d",
    "module": "torch.nn.utils.parametrizations.F",
    "fullName": "torch.nn.utils.parametrizations.F.max_pool2d",
    "signature": "(*args, **kwargs)",
    "description": "max_pool2d(input, kernel_size, stride=None, padding=0, dilation=1, ceil_mode=False, return_indices=False)"
  },
  "2532": {
    "name": "max_pool2d_with_indices",
    "module": "torch.nn.utils.parametrizations.F",
    "fullName": "torch.nn.utils.parametrizations.F.max_pool2d_with_indices",
    "signature": "(input: torch.Tensor, kernel_size: None, stride: NoneType = None, padding: None = 0, dilation: None = 1, ceil_mode: bool = False, return_indices: bool = False) -> Tuple[torch.Tensor, torch.Tensor]",
    "description": "max_pool2d(input, kernel_size, stride=None, padding=0, dilation=1, ceil_mode=False, return_indices=False)"
  },
  "2533": {
    "name": "max_pool3d",
    "module": "torch.nn.utils.parametrizations.F",
    "fullName": "torch.nn.utils.parametrizations.F.max_pool3d",
    "signature": "(*args, **kwargs)",
    "description": "max_pool3d(input, kernel_size, stride=None, padding=0, dilation=1, ceil_mode=False, return_indices=False)"
  },
  "2534": {
    "name": "max_pool3d_with_indices",
    "module": "torch.nn.utils.parametrizations.F",
    "fullName": "torch.nn.utils.parametrizations.F.max_pool3d_with_indices",
    "signature": "(input: torch.Tensor, kernel_size: None, stride: NoneType = None, padding: None = 0, dilation: None = 1, ceil_mode: bool = False, return_indices: bool = False) -> Tuple[torch.Tensor, torch.Tensor]",
    "description": "max_pool3d(input, kernel_size, stride=None, padding=0, dilation=1, ceil_mode=False, return_indices=False)"
  },
  "2535": {
    "name": "max_unpool1d",
    "module": "torch.nn.utils.parametrizations.F",
    "fullName": "torch.nn.utils.parametrizations.F.max_unpool1d",
    "signature": "(input: torch.Tensor, indices: torch.Tensor, kernel_size: None, stride: NoneType = None, padding: None = 0, output_size: NoneType = None) -> torch.Tensor",
    "description": "Computes a partial inverse of :class:`MaxPool1d`."
  },
  "2536": {
    "name": "max_unpool2d",
    "module": "torch.nn.utils.parametrizations.F",
    "fullName": "torch.nn.utils.parametrizations.F.max_unpool2d",
    "signature": "(input: torch.Tensor, indices: torch.Tensor, kernel_size: None, stride: NoneType = None, padding: None = 0, output_size: NoneType = None) -> torch.Tensor",
    "description": "Computes a partial inverse of :class:`MaxPool2d`."
  },
  "2537": {
    "name": "max_unpool3d",
    "module": "torch.nn.utils.parametrizations.F",
    "fullName": "torch.nn.utils.parametrizations.F.max_unpool3d",
    "signature": "(input: torch.Tensor, indices: torch.Tensor, kernel_size: None, stride: NoneType = None, padding: None = 0, output_size: NoneType = None) -> torch.Tensor",
    "description": "Computes a partial inverse of :class:`MaxPool3d`."
  },
  "2538": {
    "name": "mish",
    "module": "torch.nn.utils.parametrizations.F",
    "fullName": "torch.nn.utils.parametrizations.F.mish",
    "signature": "(input: torch.Tensor, inplace: bool = False) -> torch.Tensor",
    "description": "Applies the Mish function, element-wise."
  },
  "2539": {
    "name": "mse_loss",
    "module": "torch.nn.utils.parametrizations.F",
    "fullName": "torch.nn.utils.parametrizations.F.mse_loss",
    "signature": "(input: torch.Tensor, target: torch.Tensor, size_average: Optional[bool] = None, reduce: Optional[bool] = None, reduction: str = 'mean') -> torch.Tensor",
    "description": "mse_loss(input, target, size_average=None, reduce=None, reduction='mean') -> Tensor"
  },
  "2540": {
    "name": "multi_head_attention_forward",
    "module": "torch.nn.utils.parametrizations.F",
    "fullName": "torch.nn.utils.parametrizations.F.multi_head_attention_forward",
    "signature": "(query: torch.Tensor, key: torch.Tensor, value: torch.Tensor, embed_dim_to_check: int, num_heads: int, in_proj_weight: Optional[torch.Tensor], in_proj_bias: Optional[torch.Tensor], bias_k: Optional[torch.Tensor], bias_v: Optional[torch.Tensor], add_zero_attn: bool, dropout_p: float, out_proj_weight: torch.Tensor, out_proj_bias: Optional[torch.Tensor], training: bool = True, key_padding_mask: Optional[torch.Tensor] = None, need_weights: bool = True, attn_mask: Optional[torch.Tensor] = None, use_separate_proj_weight: bool = False, q_proj_weight: Optional[torch.Tensor] = None, k_proj_weight: Optional[torch.Tensor] = None, v_proj_weight: Optional[torch.Tensor] = None, static_k: Optional[torch.Tensor] = None, static_v: Optional[torch.Tensor] = None, average_attn_weights: bool = True) -> Tuple[torch.Tensor, Optional[torch.Tensor]]",
    "description": "Args:"
  },
  "2541": {
    "name": "multi_margin_loss",
    "module": "torch.nn.utils.parametrizations.F",
    "fullName": "torch.nn.utils.parametrizations.F.multi_margin_loss",
    "signature": "(input: torch.Tensor, target: torch.Tensor, p: int = 1, margin: float = 1.0, weight: Optional[torch.Tensor] = None, size_average: Optional[bool] = None, reduce: Optional[bool] = None, reduction: str = 'mean') -> torch.Tensor",
    "description": "multi_margin_loss(input, target, p=1, margin=1, weight=None, size_average=None, reduce=None, reduction='mean') -> Tensor"
  },
  "2542": {
    "name": "multilabel_margin_loss",
    "module": "torch.nn.utils.parametrizations.F",
    "fullName": "torch.nn.utils.parametrizations.F.multilabel_margin_loss",
    "signature": "(input: torch.Tensor, target: torch.Tensor, size_average: Optional[bool] = None, reduce: Optional[bool] = None, reduction: str = 'mean') -> torch.Tensor",
    "description": "multilabel_margin_loss(input, target, size_average=None, reduce=None, reduction='mean') -> Tensor"
  },
  "2543": {
    "name": "multilabel_soft_margin_loss",
    "module": "torch.nn.utils.parametrizations.F",
    "fullName": "torch.nn.utils.parametrizations.F.multilabel_soft_margin_loss",
    "signature": "(input: torch.Tensor, target: torch.Tensor, weight: Optional[torch.Tensor] = None, size_average: Optional[bool] = None, reduce: Optional[bool] = None, reduction: str = 'mean') -> torch.Tensor",
    "description": "multilabel_soft_margin_loss(input, target, weight=None, size_average=None, reduce=None, reduction='mean') -> Tensor"
  },
  "2544": {
    "name": "normalize",
    "module": "torch.nn.utils.parametrizations.F",
    "fullName": "torch.nn.utils.parametrizations.F.normalize",
    "signature": "(input: torch.Tensor, p: float = 2.0, dim: int = 1, eps: float = 1e-12, out: Optional[torch.Tensor] = None) -> torch.Tensor",
    "description": "Performs :math:`L_p` normalization of inputs over specified dimension."
  },
  "2545": {
    "name": "relu",
    "module": "torch.nn.utils.parametrizations.F",
    "fullName": "torch.nn.utils.parametrizations.F.relu",
    "signature": "(input: torch.Tensor, inplace: bool = False) -> torch.Tensor",
    "description": "relu(input, inplace=False) -> Tensor"
  },
  "2546": {
    "name": "relu6",
    "module": "torch.nn.utils.parametrizations.F",
    "fullName": "torch.nn.utils.parametrizations.F.relu6",
    "signature": "(input: torch.Tensor, inplace: bool = False) -> torch.Tensor",
    "description": "relu6(input, inplace=False) -> Tensor"
  },
  "2547": {
    "name": "rrelu",
    "module": "torch.nn.utils.parametrizations.F",
    "fullName": "torch.nn.utils.parametrizations.F.rrelu",
    "signature": "(input: torch.Tensor, lower: float = 0.125, upper: float = 0.3333333333333333, training: bool = False, inplace: bool = False) -> torch.Tensor",
    "description": "rrelu(input, lower=1./8, upper=1./3, training=False, inplace=False) -> Tensor"
  },
  "2548": {
    "name": "selu",
    "module": "torch.nn.utils.parametrizations.F",
    "fullName": "torch.nn.utils.parametrizations.F.selu",
    "signature": "(input: torch.Tensor, inplace: bool = False) -> torch.Tensor",
    "description": "selu(input, inplace=False) -> Tensor"
  },
  "2549": {
    "name": "sigmoid",
    "module": "torch.nn.utils.parametrizations.F",
    "fullName": "torch.nn.utils.parametrizations.F.sigmoid",
    "signature": "(input)",
    "description": "sigmoid(input) -> Tensor"
  },
  "2550": {
    "name": "silu",
    "module": "torch.nn.utils.parametrizations.F",
    "fullName": "torch.nn.utils.parametrizations.F.silu",
    "signature": "(input: torch.Tensor, inplace: bool = False) -> torch.Tensor",
    "description": "Applies the Sigmoid Linear Unit (SiLU) function, element-wise."
  },
  "2551": {
    "name": "smooth_l1_loss",
    "module": "torch.nn.utils.parametrizations.F",
    "fullName": "torch.nn.utils.parametrizations.F.smooth_l1_loss",
    "signature": "(input: torch.Tensor, target: torch.Tensor, size_average: Optional[bool] = None, reduce: Optional[bool] = None, reduction: str = 'mean', beta: float = 1.0) -> torch.Tensor",
    "description": "Function that uses a squared term if the absolute"
  },
  "2552": {
    "name": "soft_margin_loss",
    "module": "torch.nn.utils.parametrizations.F",
    "fullName": "torch.nn.utils.parametrizations.F.soft_margin_loss",
    "signature": "(input: torch.Tensor, target: torch.Tensor, size_average: Optional[bool] = None, reduce: Optional[bool] = None, reduction: str = 'mean') -> torch.Tensor",
    "description": "soft_margin_loss(input, target, size_average=None, reduce=None, reduction='mean') -> Tensor"
  },
  "2553": {
    "name": "softmax",
    "module": "torch.nn.utils.parametrizations.F",
    "fullName": "torch.nn.utils.parametrizations.F.softmax",
    "signature": "(input: torch.Tensor, dim: Optional[int] = None, _stacklevel: int = 3, dtype: Optional[int] = None) -> torch.Tensor",
    "description": "Applies a softmax function."
  },
  "2554": {
    "name": "softmin",
    "module": "torch.nn.utils.parametrizations.F",
    "fullName": "torch.nn.utils.parametrizations.F.softmin",
    "signature": "(input: torch.Tensor, dim: Optional[int] = None, _stacklevel: int = 3, dtype: Optional[int] = None) -> torch.Tensor",
    "description": "Applies a softmin function."
  },
  "2555": {
    "name": "softsign",
    "module": "torch.nn.utils.parametrizations.F",
    "fullName": "torch.nn.utils.parametrizations.F.softsign",
    "signature": "(input)",
    "description": "softsign(input) -> Tensor"
  },
  "2556": {
    "name": "tanh",
    "module": "torch.nn.utils.parametrizations.F",
    "fullName": "torch.nn.utils.parametrizations.F.tanh",
    "signature": "(input)",
    "description": "tanh(input) -> Tensor"
  },
  "2557": {
    "name": "tanhshrink",
    "module": "torch.nn.utils.parametrizations.F",
    "fullName": "torch.nn.utils.parametrizations.F.tanhshrink",
    "signature": "(input)",
    "description": "tanhshrink(input) -> Tensor"
  },
  "2558": {
    "name": "threshold",
    "module": "torch.nn.utils.parametrizations.F",
    "fullName": "torch.nn.utils.parametrizations.F.threshold",
    "signature": "(input: torch.Tensor, threshold: float, value: float, inplace: bool = False) -> torch.Tensor",
    "description": "Thresholds each element of the input Tensor."
  },
  "2559": {
    "name": "triplet_margin_loss",
    "module": "torch.nn.utils.parametrizations.F",
    "fullName": "torch.nn.utils.parametrizations.F.triplet_margin_loss",
    "signature": "(anchor: torch.Tensor, positive: torch.Tensor, negative: torch.Tensor, margin: float = 1.0, p: float = 2, eps: float = 1e-06, swap: bool = False, size_average: Optional[bool] = None, reduce: Optional[bool] = None, reduction: str = 'mean') -> torch.Tensor",
    "description": "See :class:`~torch.nn.TripletMarginLoss` for details"
  },
  "2560": {
    "name": "triplet_margin_with_distance_loss",
    "module": "torch.nn.utils.parametrizations.F",
    "fullName": "torch.nn.utils.parametrizations.F.triplet_margin_with_distance_loss",
    "signature": "(anchor: torch.Tensor, positive: torch.Tensor, negative: torch.Tensor, *, distance_function: Optional[Callable[[torch.Tensor, torch.Tensor], torch.Tensor]] = None, margin: float = 1.0, swap: bool = False, reduction: str = 'mean') -> torch.Tensor",
    "description": "See :class:`~torch.nn.TripletMarginWithDistanceLoss` for details."
  },
  "2561": {
    "name": "unfold",
    "module": "torch.nn.utils.parametrizations.F",
    "fullName": "torch.nn.utils.parametrizations.F.unfold",
    "signature": "(input: torch.Tensor, kernel_size: None, dilation: None = 1, padding: None = 0, stride: None = 1) -> torch.Tensor",
    "description": "Extracts sliding local blocks from a batched input tensor."
  },
  "2562": {
    "name": "consume_prefix_in_state_dict_if_present",
    "module": "torch.nn.utils.parametrizations.F.utils",
    "fullName": "torch.nn.utils.parametrizations.F.utils.consume_prefix_in_state_dict_if_present",
    "signature": "(state_dict: Dict[str, Any], prefix: str) -> None",
    "description": "Strip the prefix in state_dict in place, if any."
  },
  "2563": {
    "name": "conv1d_input",
    "module": "torch.nn.utils.parametrizations.F.grad",
    "fullName": "torch.nn.utils.parametrizations.F.grad.conv1d_input",
    "signature": "(input_size, weight, grad_output, stride=1, padding=0, dilation=1, groups=1)",
    "description": "Computes the gradient of conv1d with respect to the input of the convolution."
  },
  "2564": {
    "name": "conv1d_weight",
    "module": "torch.nn.utils.parametrizations.F.grad",
    "fullName": "torch.nn.utils.parametrizations.F.grad.conv1d_weight",
    "signature": "(input, weight_size, grad_output, stride=1, padding=0, dilation=1, groups=1)",
    "description": "Computes the gradient of conv1d with respect to the weight of the convolution."
  },
  "2565": {
    "name": "conv2d_input",
    "module": "torch.nn.utils.parametrizations.F.grad",
    "fullName": "torch.nn.utils.parametrizations.F.grad.conv2d_input",
    "signature": "(input_size, weight, grad_output, stride=1, padding=0, dilation=1, groups=1)",
    "description": "Computes the gradient of conv2d with respect to the input of the convolution."
  },
  "2566": {
    "name": "conv2d_weight",
    "module": "torch.nn.utils.parametrizations.F.grad",
    "fullName": "torch.nn.utils.parametrizations.F.grad.conv2d_weight",
    "signature": "(input, weight_size, grad_output, stride=1, padding=0, dilation=1, groups=1)",
    "description": "Computes the gradient of conv2d with respect to the weight of the convolution."
  },
  "2567": {
    "name": "conv3d_input",
    "module": "torch.nn.utils.parametrizations.F.grad",
    "fullName": "torch.nn.utils.parametrizations.F.grad.conv3d_input",
    "signature": "(input_size, weight, grad_output, stride=1, padding=0, dilation=1, groups=1)",
    "description": "Computes the gradient of conv3d with respect to the input of the convolution."
  },
  "2568": {
    "name": "conv3d_weight",
    "module": "torch.nn.utils.parametrizations.F.grad",
    "fullName": "torch.nn.utils.parametrizations.F.grad.conv3d_weight",
    "signature": "(input, weight_size, grad_output, stride=1, padding=0, dilation=1, groups=1)",
    "description": "Computes the gradient of conv3d with respect to the weight of the convolution."
  },
  "2569": {
    "name": "convert_conv2d_weight_memory_format",
    "module": "torch.nn.utils.memory_format",
    "fullName": "torch.nn.utils.memory_format.convert_conv2d_weight_memory_format",
    "signature": "(module, memory_format)",
    "description": "Convert ``memory_format`` of ``nn.Conv2d.weight`` to ``memory_format``"
  },
  "2570": {
    "name": "skip_init",
    "module": "torch.nn.utils.init",
    "fullName": "torch.nn.utils.init.skip_init",
    "signature": "(module_cls, *args, **kwargs)",
    "description": "Given a module class object and args / kwargs, instantiates the module without initializing"
  },
  "2571": {
    "name": "fuse_conv_bn_eval",
    "module": "torch.nn.utils.fusion",
    "fullName": "torch.nn.utils.fusion.fuse_conv_bn_eval",
    "signature": "(conv, bn, transpose=False)",
    "description": "No description available."
  },
  "2572": {
    "name": "fuse_conv_bn_weights",
    "module": "torch.nn.utils.fusion",
    "fullName": "torch.nn.utils.fusion.fuse_conv_bn_weights",
    "signature": "(conv_w, conv_b, bn_rm, bn_rv, bn_eps, bn_w, bn_b, transpose=False)",
    "description": "No description available."
  },
  "2573": {
    "name": "fuse_linear_bn_eval",
    "module": "torch.nn.utils.fusion",
    "fullName": "torch.nn.utils.fusion.fuse_linear_bn_eval",
    "signature": "(linear, bn)",
    "description": "No description available."
  },
  "2574": {
    "name": "fuse_linear_bn_weights",
    "module": "torch.nn.utils.fusion",
    "fullName": "torch.nn.utils.fusion.fuse_linear_bn_weights",
    "signature": "(linear_w, linear_b, bn_rm, bn_rv, bn_eps, bn_w, bn_b)",
    "description": "No description available."
  },
  "2575": {
    "name": "parameters_to_vector",
    "module": "torch.nn.utils.convert_parameters",
    "fullName": "torch.nn.utils.convert_parameters.parameters_to_vector",
    "signature": "(parameters: Iterable[torch.Tensor]) -> torch.Tensor",
    "description": "Convert parameters to one vector"
  },
  "2576": {
    "name": "vector_to_parameters",
    "module": "torch.nn.utils.convert_parameters",
    "fullName": "torch.nn.utils.convert_parameters.vector_to_parameters",
    "signature": "(vec: torch.Tensor, parameters: Iterable[torch.Tensor]) -> None",
    "description": "Convert one vector to the parameters"
  },
  "2577": {
    "name": "clip_grad_norm_",
    "module": "torch.nn.utils.clip_grad",
    "fullName": "torch.nn.utils.clip_grad.clip_grad_norm_",
    "signature": "(parameters: Union[torch.Tensor, Iterable[torch.Tensor]], max_norm: float, norm_type: float = 2.0, error_if_nonfinite: bool = False) -> torch.Tensor",
    "description": "Clips gradient norm of an iterable of parameters."
  },
  "2578": {
    "name": "clip_grad_value_",
    "module": "torch.nn.utils.clip_grad",
    "fullName": "torch.nn.utils.clip_grad.clip_grad_value_",
    "signature": "(parameters: Union[torch.Tensor, Iterable[torch.Tensor]], clip_value: float) -> None",
    "description": "Clips gradient of an iterable of parameters at specified value."
  },
  "2579": {
    "name": "hide_packed_params_repr",
    "module": "torch.nn.quantized.modules.utils",
    "fullName": "torch.nn.quantized.modules.utils.hide_packed_params_repr",
    "signature": "(self, params)",
    "description": "No description available."
  },
  "2580": {
    "name": "fuse_linear_bn_weights",
    "module": "torch.nn.quantized.modules.linear",
    "fullName": "torch.nn.quantized.modules.linear.fuse_linear_bn_weights",
    "signature": "(linear_w, linear_b, bn_rm, bn_rv, bn_eps, bn_w, bn_b)",
    "description": "No description available."
  },
  "2581": {
    "name": "hide_packed_params_repr",
    "module": "torch.nn.quantized.modules.linear",
    "fullName": "torch.nn.quantized.modules.linear.hide_packed_params_repr",
    "signature": "(self, params)",
    "description": "No description available."
  },
  "2582": {
    "name": "type_before_parametrizations",
    "module": "torch.nn.quantized.modules.linear",
    "fullName": "torch.nn.quantized.modules.linear.type_before_parametrizations",
    "signature": "(module: torch.nn.modules.module.Module) -> type",
    "description": "Returns the module type before parametrizations were applied and if not,"
  },
  "2583": {
    "name": "freeze_bn_stats",
    "module": "torch.nn.quantized.modules.linear.nniqat",
    "fullName": "torch.nn.quantized.modules.linear.nniqat.freeze_bn_stats",
    "signature": "(mod)",
    "description": "No description available."
  },
  "2584": {
    "name": "update_bn_stats",
    "module": "torch.nn.quantized.modules.linear.nniqat",
    "fullName": "torch.nn.quantized.modules.linear.nniqat.update_bn_stats",
    "signature": "(mod)",
    "description": "No description available."
  },
  "2585": {
    "name": "freeze_bn_stats",
    "module": "torch.nn.quantized.modules.linear.nniqat.modules",
    "fullName": "torch.nn.quantized.modules.linear.nniqat.modules.freeze_bn_stats",
    "signature": "(mod)",
    "description": "No description available."
  },
  "2586": {
    "name": "update_bn_stats",
    "module": "torch.nn.quantized.modules.linear.nniqat.modules",
    "fullName": "torch.nn.quantized.modules.linear.nniqat.modules.update_bn_stats",
    "signature": "(mod)",
    "description": "No description available."
  },
  "2587": {
    "name": "is_parametrized",
    "module": "torch.nn.quantized.modules.linear.nniqat.modules.linear_relu.nnqat.modules.linear",
    "fullName": "torch.nn.quantized.modules.linear.nniqat.modules.linear_relu.nnqat.modules.linear.is_parametrized",
    "signature": "(module: torch.nn.modules.module.Module, tensor_name: Optional[str] = None) -> bool",
    "description": "Returns ``True`` if module has an active parametrization."
  },
  "2588": {
    "name": "transfer_parametrizations_and_params",
    "module": "torch.nn.quantized.modules.linear.nniqat.modules.linear_relu.nnqat.modules.linear",
    "fullName": "torch.nn.quantized.modules.linear.nniqat.modules.linear_relu.nnqat.modules.linear.transfer_parametrizations_and_params",
    "signature": "(from_module: torch.nn.modules.module.Module, to_module: torch.nn.modules.module.Module, tensor_name: Optional[str] = None) -> torch.nn.modules.module.Module",
    "description": "Transfers parametrizations and the parameters they parametrize from from_module"
  },
  "2589": {
    "name": "type_before_parametrizations",
    "module": "torch.nn.quantized.modules.linear.nniqat.modules.linear_relu.nnqat.modules.linear",
    "fullName": "torch.nn.quantized.modules.linear.nniqat.modules.linear_relu.nnqat.modules.linear.type_before_parametrizations",
    "signature": "(module: torch.nn.modules.module.Module) -> type",
    "description": "Returns the module type before parametrizations were applied and if not,"
  },
  "2590": {
    "name": "activation_is_memoryless",
    "module": "torch.nn.quantized.modules.linear.nniqat.modules.linear_relu.nnqat.dynamic.modules.linear",
    "fullName": "torch.nn.quantized.modules.linear.nniqat.modules.linear_relu.nnqat.dynamic.modules.linear.activation_is_memoryless",
    "signature": "(qconfig: torch.ao.quantization.qconfig.QConfig)",
    "description": "Return whether the observer for activations defined in the given QConfig is memoryless."
  },
  "2591": {
    "name": "fuse_conv_bn_weights",
    "module": "torch.nn.quantized.modules.linear.nniqat.modules.linear_relu.nni.quantized.modules.conv_relu",
    "fullName": "torch.nn.quantized.modules.linear.nniqat.modules.linear_relu.nni.quantized.modules.conv_relu.fuse_conv_bn_weights",
    "signature": "(conv_w, conv_b, bn_rm, bn_rv, bn_eps, bn_w, bn_b, transpose=False)",
    "description": "No description available."
  },
  "2592": {
    "name": "apply_permutation",
    "module": "torch.nn.quantized.modules.linear.nniqat.modules.linear_relu.nni.quantized.dynamic.modules.linear_relu.nnqd.modules.rnn",
    "fullName": "torch.nn.quantized.modules.linear.nniqat.modules.linear_relu.nni.quantized.dynamic.modules.linear_relu.nnqd.modules.rnn.apply_permutation",
    "signature": "(tensor: torch.Tensor, permutation: torch.Tensor, dim: int = 1) -> torch.Tensor",
    "description": "No description available."
  },
  "2593": {
    "name": "pack_weight_bias",
    "module": "torch.nn.quantized.modules.linear.nniqat.modules.linear_relu.nni.quantized.dynamic.modules.linear_relu.nnqd.modules.rnn",
    "fullName": "torch.nn.quantized.modules.linear.nniqat.modules.linear_relu.nni.quantized.dynamic.modules.linear_relu.nnqd.modules.rnn.pack_weight_bias",
    "signature": "(qweight, bias, dtype)",
    "description": "No description available."
  },
  "2594": {
    "name": "type_before_parametrizations",
    "module": "torch.nn.quantized.modules.linear.nniqat.modules.linear_relu.nni.modules.fused",
    "fullName": "torch.nn.quantized.modules.linear.nniqat.modules.linear_relu.nni.modules.fused.type_before_parametrizations",
    "signature": "(module: torch.nn.modules.module.Module) -> type",
    "description": "Returns the module type before parametrizations were applied and if not,"
  },
  "2595": {
    "name": "fuse_linear_bn_weights",
    "module": "torch.nn.quantized.modules.linear.nniqat.modules.linear_fused",
    "fullName": "torch.nn.quantized.modules.linear.nniqat.modules.linear_fused.fuse_linear_bn_weights",
    "signature": "(linear_w, linear_b, bn_rm, bn_rv, bn_eps, bn_w, bn_b)",
    "description": "No description available."
  },
  "2596": {
    "name": "calculate_gain",
    "module": "torch.nn.quantized.modules.linear.nniqat.modules.linear_fused.init",
    "fullName": "torch.nn.quantized.modules.linear.nniqat.modules.linear_fused.init.calculate_gain",
    "signature": "(nonlinearity, param=None)",
    "description": "Return the recommended gain value for the given nonlinearity function."
  },
  "2597": {
    "name": "constant_",
    "module": "torch.nn.quantized.modules.linear.nniqat.modules.linear_fused.init",
    "fullName": "torch.nn.quantized.modules.linear.nniqat.modules.linear_fused.init.constant_",
    "signature": "(tensor: torch.Tensor, val: float) -> torch.Tensor",
    "description": "Fills the input Tensor with the value :math:`\\text{val}`."
  },
  "2598": {
    "name": "dirac_",
    "module": "torch.nn.quantized.modules.linear.nniqat.modules.linear_fused.init",
    "fullName": "torch.nn.quantized.modules.linear.nniqat.modules.linear_fused.init.dirac_",
    "signature": "(tensor, groups=1)",
    "description": "Fills the {3, 4, 5}-dimensional input `Tensor` with the Dirac"
  },
  "2599": {
    "name": "eye_",
    "module": "torch.nn.quantized.modules.linear.nniqat.modules.linear_fused.init",
    "fullName": "torch.nn.quantized.modules.linear.nniqat.modules.linear_fused.init.eye_",
    "signature": "(tensor)",
    "description": "Fills the 2-dimensional input `Tensor` with the identity"
  },
  "2600": {
    "name": "kaiming_normal_",
    "module": "torch.nn.quantized.modules.linear.nniqat.modules.linear_fused.init",
    "fullName": "torch.nn.quantized.modules.linear.nniqat.modules.linear_fused.init.kaiming_normal_",
    "signature": "(tensor: torch.Tensor, a: float = 0, mode: str = 'fan_in', nonlinearity: str = 'leaky_relu')",
    "description": "Fills the input `Tensor` with values according to the method"
  },
  "2601": {
    "name": "kaiming_uniform_",
    "module": "torch.nn.quantized.modules.linear.nniqat.modules.linear_fused.init",
    "fullName": "torch.nn.quantized.modules.linear.nniqat.modules.linear_fused.init.kaiming_uniform_",
    "signature": "(tensor: torch.Tensor, a: float = 0, mode: str = 'fan_in', nonlinearity: str = 'leaky_relu')",
    "description": "Fills the input `Tensor` with values according to the method"
  },
  "2602": {
    "name": "normal_",
    "module": "torch.nn.quantized.modules.linear.nniqat.modules.linear_fused.init",
    "fullName": "torch.nn.quantized.modules.linear.nniqat.modules.linear_fused.init.normal_",
    "signature": "(tensor: torch.Tensor, mean: float = 0.0, std: float = 1.0) -> torch.Tensor",
    "description": "Fills the input Tensor with values drawn from the normal"
  },
  "2603": {
    "name": "ones_",
    "module": "torch.nn.quantized.modules.linear.nniqat.modules.linear_fused.init",
    "fullName": "torch.nn.quantized.modules.linear.nniqat.modules.linear_fused.init.ones_",
    "signature": "(tensor: torch.Tensor) -> torch.Tensor",
    "description": "Fills the input Tensor with the scalar value `1`."
  },
  "2604": {
    "name": "orthogonal_",
    "module": "torch.nn.quantized.modules.linear.nniqat.modules.linear_fused.init",
    "fullName": "torch.nn.quantized.modules.linear.nniqat.modules.linear_fused.init.orthogonal_",
    "signature": "(tensor, gain=1)",
    "description": "Fills the input `Tensor` with a (semi) orthogonal matrix, as"
  },
  "2605": {
    "name": "sparse_",
    "module": "torch.nn.quantized.modules.linear.nniqat.modules.linear_fused.init",
    "fullName": "torch.nn.quantized.modules.linear.nniqat.modules.linear_fused.init.sparse_",
    "signature": "(tensor, sparsity, std=0.01)",
    "description": "Fills the 2D input `Tensor` as a sparse matrix, where the"
  },
  "2606": {
    "name": "trunc_normal_",
    "module": "torch.nn.quantized.modules.linear.nniqat.modules.linear_fused.init",
    "fullName": "torch.nn.quantized.modules.linear.nniqat.modules.linear_fused.init.trunc_normal_",
    "signature": "(tensor: torch.Tensor, mean: float = 0.0, std: float = 1.0, a: float = -2.0, b: float = 2.0) -> torch.Tensor",
    "description": "Fills the input Tensor with values drawn from a truncated"
  },
  "2607": {
    "name": "uniform_",
    "module": "torch.nn.quantized.modules.linear.nniqat.modules.linear_fused.init",
    "fullName": "torch.nn.quantized.modules.linear.nniqat.modules.linear_fused.init.uniform_",
    "signature": "(tensor: torch.Tensor, a: float = 0.0, b: float = 1.0) -> torch.Tensor",
    "description": "Fills the input Tensor with values drawn from the uniform"
  },
  "2608": {
    "name": "xavier_normal_",
    "module": "torch.nn.quantized.modules.linear.nniqat.modules.linear_fused.init",
    "fullName": "torch.nn.quantized.modules.linear.nniqat.modules.linear_fused.init.xavier_normal_",
    "signature": "(tensor: torch.Tensor, gain: float = 1.0) -> torch.Tensor",
    "description": "Fills the input `Tensor` with values according to the method"
  },
  "2609": {
    "name": "xavier_uniform_",
    "module": "torch.nn.quantized.modules.linear.nniqat.modules.linear_fused.init",
    "fullName": "torch.nn.quantized.modules.linear.nniqat.modules.linear_fused.init.xavier_uniform_",
    "signature": "(tensor: torch.Tensor, gain: float = 1.0) -> torch.Tensor",
    "description": "Fills the input `Tensor` with values according to the method"
  },
  "2610": {
    "name": "zeros_",
    "module": "torch.nn.quantized.modules.linear.nniqat.modules.linear_fused.init",
    "fullName": "torch.nn.quantized.modules.linear.nniqat.modules.linear_fused.init.zeros_",
    "signature": "(tensor: torch.Tensor) -> torch.Tensor",
    "description": "Fills the input Tensor with the scalar value `0`."
  },
  "2611": {
    "name": "freeze_bn_stats",
    "module": "torch.nn.quantized.modules.linear.nniqat.modules.conv_fused",
    "fullName": "torch.nn.quantized.modules.linear.nniqat.modules.conv_fused.freeze_bn_stats",
    "signature": "(mod)",
    "description": "No description available."
  },
  "2612": {
    "name": "fuse_conv_bn_weights",
    "module": "torch.nn.quantized.modules.linear.nniqat.modules.conv_fused",
    "fullName": "torch.nn.quantized.modules.linear.nniqat.modules.conv_fused.fuse_conv_bn_weights",
    "signature": "(conv_w, conv_b, bn_rm, bn_rv, bn_eps, bn_w, bn_b, transpose=False)",
    "description": "No description available."
  },
  "2613": {
    "name": "update_bn_stats",
    "module": "torch.nn.quantized.modules.linear.nniqat.modules.conv_fused",
    "fullName": "torch.nn.quantized.modules.linear.nniqat.modules.conv_fused.update_bn_stats",
    "signature": "(mod)",
    "description": "No description available."
  },
  "2614": {
    "name": "hide_packed_params_repr",
    "module": "torch.nn.quantized.modules.embedding_ops",
    "fullName": "torch.nn.quantized.modules.embedding_ops.hide_packed_params_repr",
    "signature": "(self, params)",
    "description": "No description available."
  },
  "2615": {
    "name": "fuse_conv_bn_weights",
    "module": "torch.nn.quantized.modules.conv",
    "fullName": "torch.nn.quantized.modules.conv.fuse_conv_bn_weights",
    "signature": "(conv_w, conv_b, bn_rm, bn_rv, bn_eps, bn_w, bn_b, transpose=False)",
    "description": "No description available."
  },
  "2616": {
    "name": "adaptive_avg_pool2d",
    "module": "torch.nn.quantized.functional",
    "fullName": "torch.nn.quantized.functional.adaptive_avg_pool2d",
    "signature": "(input: torch.Tensor, output_size: None) -> torch.Tensor",
    "description": "Applies a 2D adaptive average pooling over a quantized input signal composed"
  },
  "2617": {
    "name": "adaptive_avg_pool3d",
    "module": "torch.nn.quantized.functional",
    "fullName": "torch.nn.quantized.functional.adaptive_avg_pool3d",
    "signature": "(input: torch.Tensor, output_size: None) -> torch.Tensor",
    "description": "Applies a 3D adaptive average pooling over a quantized input signal composed"
  },
  "2618": {
    "name": "avg_pool2d",
    "module": "torch.nn.quantized.functional",
    "fullName": "torch.nn.quantized.functional.avg_pool2d",
    "signature": "(input, kernel_size, stride=None, padding=0, ceil_mode=False, count_include_pad=True, divisor_override=None)",
    "description": "Applies 2D average-pooling operation in :math:`kH \\times kW` regions by step size"
  },
  "2619": {
    "name": "avg_pool3d",
    "module": "torch.nn.quantized.functional",
    "fullName": "torch.nn.quantized.functional.avg_pool3d",
    "signature": "(input, kernel_size, stride=None, padding=0, ceil_mode=False, count_include_pad=True, divisor_override=None)",
    "description": "Applies 3D average-pooling operation in :math:`kD \\ times kH \\times kW` regions by step size"
  },
  "2620": {
    "name": "celu",
    "module": "torch.nn.quantized.functional",
    "fullName": "torch.nn.quantized.functional.celu",
    "signature": "(input: torch.Tensor, scale: float, zero_point: int, alpha: float = 1.0) -> torch.Tensor",
    "description": "celu(input, scale, zero_point, alpha=1.) -> Tensor"
  },
  "2621": {
    "name": "clamp",
    "module": "torch.nn.quantized.functional",
    "fullName": "torch.nn.quantized.functional.clamp",
    "signature": "(input: torch.Tensor, min_: float, max_: float) -> torch.Tensor",
    "description": "float(input, min\\_, max\\_) -> Tensor"
  },
  "2622": {
    "name": "conv1d",
    "module": "torch.nn.quantized.functional",
    "fullName": "torch.nn.quantized.functional.conv1d",
    "signature": "(input, weight, bias, stride=1, padding=0, dilation=1, groups=1, padding_mode='zeros', scale=1.0, zero_point=0, dtype=torch.quint8)",
    "description": "Applies a 1D convolution over a quantized 1D input composed of several input"
  },
  "2623": {
    "name": "conv2d",
    "module": "torch.nn.quantized.functional",
    "fullName": "torch.nn.quantized.functional.conv2d",
    "signature": "(input, weight, bias, stride=1, padding=0, dilation=1, groups=1, padding_mode='zeros', scale=1.0, zero_point=0, dtype=torch.quint8)",
    "description": "Applies a 2D convolution over a quantized 2D input composed of several input"
  },
  "2624": {
    "name": "conv3d",
    "module": "torch.nn.quantized.functional",
    "fullName": "torch.nn.quantized.functional.conv3d",
    "signature": "(input, weight, bias, stride=1, padding=0, dilation=1, groups=1, padding_mode='zeros', scale=1.0, zero_point=0, dtype=torch.quint8)",
    "description": "Applies a 3D convolution over a quantized 3D input composed of several input"
  },
  "2625": {
    "name": "elu",
    "module": "torch.nn.quantized.functional",
    "fullName": "torch.nn.quantized.functional.elu",
    "signature": "(input: torch.Tensor, scale: float, zero_point: int, alpha: float = 1.0) -> torch.Tensor",
    "description": "This is the quantized version of :func:`~torch.nn.functional.elu`."
  },
  "2626": {
    "name": "hardsigmoid",
    "module": "torch.nn.quantized.functional",
    "fullName": "torch.nn.quantized.functional.hardsigmoid",
    "signature": "(input: torch.Tensor, inplace: bool = False) -> torch.Tensor",
    "description": "This is the quantized version of :func:`~torch.nn.functional.hardsigmoid`."
  },
  "2627": {
    "name": "hardswish",
    "module": "torch.nn.quantized.functional",
    "fullName": "torch.nn.quantized.functional.hardswish",
    "signature": "(input: torch.Tensor, scale: float, zero_point: int) -> torch.Tensor",
    "description": "This is the quantized version of :func:`~torch.nn.functional.hardswish`."
  },
  "2628": {
    "name": "hardtanh",
    "module": "torch.nn.quantized.functional",
    "fullName": "torch.nn.quantized.functional.hardtanh",
    "signature": "(input: torch.Tensor, min_val: float = -1.0, max_val: float = 1.0, inplace: bool = False) -> torch.Tensor",
    "description": "This is the quantized version of :func:`~torch.nn.functional.hardtanh`."
  },
  "2629": {
    "name": "interpolate",
    "module": "torch.nn.quantized.functional",
    "fullName": "torch.nn.quantized.functional.interpolate",
    "signature": "(input, size=None, scale_factor=None, mode='nearest', align_corners=None)",
    "description": "Down/up samples the input to either the given :attr:`size` or the given"
  },
  "2630": {
    "name": "leaky_relu",
    "module": "torch.nn.quantized.functional",
    "fullName": "torch.nn.quantized.functional.leaky_relu",
    "signature": "(input: torch.Tensor, negative_slope: float = 0.01, inplace: bool = False, scale: Optional[float] = None, zero_point: Optional[int] = None)",
    "description": "Quantized version of the."
  },
  "2631": {
    "name": "linear",
    "module": "torch.nn.quantized.functional",
    "fullName": "torch.nn.quantized.functional.linear",
    "signature": "(input: torch.Tensor, weight: torch.Tensor, bias: Optional[torch.Tensor] = None, scale: Optional[float] = None, zero_point: Optional[int] = None) -> torch.Tensor",
    "description": "Applies a linear transformation to the incoming quantized data:"
  },
  "2632": {
    "name": "max_pool1d",
    "module": "torch.nn.quantized.functional",
    "fullName": "torch.nn.quantized.functional.max_pool1d",
    "signature": "(input, kernel_size, stride=None, padding=0, dilation=1, ceil_mode=False, return_indices=False)",
    "description": "Applies a 1D max pooling over a quantized input signal composed of"
  },
  "2633": {
    "name": "max_pool2d",
    "module": "torch.nn.quantized.functional",
    "fullName": "torch.nn.quantized.functional.max_pool2d",
    "signature": "(input, kernel_size, stride=None, padding=0, dilation=1, ceil_mode=False, return_indices=False)",
    "description": "Applies a 2D max pooling over a quantized input signal composed of"
  },
  "2634": {
    "name": "threshold",
    "module": "torch.nn.quantized.functional",
    "fullName": "torch.nn.quantized.functional.threshold",
    "signature": "(input: torch.Tensor, threshold: float, value: float) -> torch.Tensor",
    "description": "Applies the quantized version of the threshold function element-wise:"
  },
  "2635": {
    "name": "is_lazy",
    "module": "torch.nn.parameter",
    "fullName": "torch.nn.parameter.is_lazy",
    "signature": "(param)",
    "description": "No description available."
  },
  "2636": {
    "name": "DistributedDataParallelCPU",
    "module": "torch.nn.parallel",
    "fullName": "torch.nn.parallel.DistributedDataParallelCPU",
    "signature": "(*args, **kwargs)",
    "description": "No description available."
  },
  "2637": {
    "name": "data_parallel",
    "module": "torch.nn.parallel",
    "fullName": "torch.nn.parallel.data_parallel",
    "signature": "(module, inputs, device_ids=None, output_device=None, dim=0, module_kwargs=None)",
    "description": "Evaluates module(input) in parallel across the GPUs given in device_ids."
  },
  "2638": {
    "name": "gather",
    "module": "torch.nn.parallel",
    "fullName": "torch.nn.parallel.gather",
    "signature": "(outputs, target_device, dim=0)",
    "description": "Gathers tensors from different GPUs on a specified device."
  },
  "2639": {
    "name": "parallel_apply",
    "module": "torch.nn.parallel",
    "fullName": "torch.nn.parallel.parallel_apply",
    "signature": "(modules, inputs, kwargs_tup=None, devices=None)",
    "description": "Applies each `module` in :attr:`modules` in parallel on arguments"
  },
  "2640": {
    "name": "replicate",
    "module": "torch.nn.parallel",
    "fullName": "torch.nn.parallel.replicate",
    "signature": "(network, devices, detach=False)",
    "description": "No description available."
  },
  "2641": {
    "name": "scatter",
    "module": "torch.nn.parallel",
    "fullName": "torch.nn.parallel.scatter",
    "signature": "(inputs, target_gpus, dim=0)",
    "description": "Slices tensors into approximately equal chunks and"
  },
  "2642": {
    "name": "gather",
    "module": "torch.nn.parallel.scatter_gather",
    "fullName": "torch.nn.parallel.scatter_gather.gather",
    "signature": "(outputs, target_device, dim=0)",
    "description": "Gathers tensors from different GPUs on a specified device."
  },
  "2643": {
    "name": "is_namedtuple",
    "module": "torch.nn.parallel.scatter_gather",
    "fullName": "torch.nn.parallel.scatter_gather.is_namedtuple",
    "signature": "(obj)",
    "description": "No description available."
  },
  "2644": {
    "name": "scatter",
    "module": "torch.nn.parallel.scatter_gather",
    "fullName": "torch.nn.parallel.scatter_gather.scatter",
    "signature": "(inputs, target_gpus, dim=0)",
    "description": "Slices tensors into approximately equal chunks and"
  },
  "2645": {
    "name": "scatter_kwargs",
    "module": "torch.nn.parallel.scatter_gather",
    "fullName": "torch.nn.parallel.scatter_gather.scatter_kwargs",
    "signature": "(inputs, kwargs, target_gpus, dim=0)",
    "description": "Scatter with support for kwargs dictionary"
  },
  "2646": {
    "name": "contextmanager",
    "module": "torch.nn.parallel.distributed",
    "fullName": "torch.nn.parallel.distributed.contextmanager",
    "signature": "(func)",
    "description": "@contextmanager decorator."
  },
  "2647": {
    "name": "dataclass",
    "module": "torch.nn.parallel.distributed",
    "fullName": "torch.nn.parallel.distributed.dataclass",
    "signature": "(cls=None, /, *, init=True, repr=True, eq=True, order=False, unsafe_hash=False, frozen=False)",
    "description": "Returns the same class as was passed in, with dunder methods"
  },
  "2648": {
    "name": "gather",
    "module": "torch.nn.parallel.distributed",
    "fullName": "torch.nn.parallel.distributed.gather",
    "signature": "(outputs, target_device, dim=0)",
    "description": "Gathers tensors from different GPUs on a specified device."
  },
  "2649": {
    "name": "is_namedtuple",
    "module": "torch.nn.parallel.distributed",
    "fullName": "torch.nn.parallel.distributed.is_namedtuple",
    "signature": "(obj)",
    "description": "No description available."
  },
  "2650": {
    "name": "scatter_kwargs",
    "module": "torch.nn.parallel.distributed",
    "fullName": "torch.nn.parallel.distributed.scatter_kwargs",
    "signature": "(inputs, kwargs, target_gpus, dim=0)",
    "description": "Scatter with support for kwargs dictionary"
  },
  "2651": {
    "name": "tree_flatten",
    "module": "torch.nn.parallel.distributed",
    "fullName": "torch.nn.parallel.distributed.tree_flatten",
    "signature": "(pytree: Any) -> Tuple[List[Any], torch.utils._pytree.TreeSpec]",
    "description": "Flattens a pytree into a list of values and a TreeSpec that can be used"
  },
  "2652": {
    "name": "tree_unflatten",
    "module": "torch.nn.parallel.distributed",
    "fullName": "torch.nn.parallel.distributed.tree_unflatten",
    "signature": "(values: List[Any], spec: torch.utils._pytree.TreeSpec) -> Any",
    "description": "Given a list of values and a TreeSpec, builds a pytree."
  },
  "2653": {
    "name": "broadcast",
    "module": "torch.nn.parallel.comm",
    "fullName": "torch.nn.parallel.comm.broadcast",
    "signature": "(tensor, devices=None, *, out=None)",
    "description": "Broadcasts a tensor to specified GPU devices."
  },
  "2654": {
    "name": "broadcast_coalesced",
    "module": "torch.nn.parallel.comm",
    "fullName": "torch.nn.parallel.comm.broadcast_coalesced",
    "signature": "(tensors, devices, buffer_size=10485760)",
    "description": "Broadcasts a sequence tensors to the specified GPUs."
  },
  "2655": {
    "name": "gather",
    "module": "torch.nn.parallel.comm",
    "fullName": "torch.nn.parallel.comm.gather",
    "signature": "(tensors, dim=0, destination=None, *, out=None)",
    "description": "Gathers tensors from multiple GPU devices."
  },
  "2656": {
    "name": "reduce_add",
    "module": "torch.nn.parallel.comm",
    "fullName": "torch.nn.parallel.comm.reduce_add",
    "signature": "(inputs, destination=None)",
    "description": "Sums tensors from multiple GPUs."
  },
  "2657": {
    "name": "reduce_add_coalesced",
    "module": "torch.nn.parallel.comm",
    "fullName": "torch.nn.parallel.comm.reduce_add_coalesced",
    "signature": "(inputs, destination=None, buffer_size=10485760)",
    "description": "Sums tensors from multiple GPUs."
  },
  "2658": {
    "name": "scatter",
    "module": "torch.nn.parallel.comm",
    "fullName": "torch.nn.parallel.comm.scatter",
    "signature": "(tensor, devices=None, chunk_sizes=None, dim=0, streams=None, *, out=None)",
    "description": "Scatters tensor across multiple GPUs."
  },
  "2659": {
    "name": "all_gather",
    "module": "torch.nn.parallel.comm.nccl",
    "fullName": "torch.nn.parallel.comm.nccl.all_gather",
    "signature": "(inputs: Sequence[torch.Tensor], outputs: Sequence[torch.Tensor], streams=None, comms=None) -> None",
    "description": "No description available."
  },
  "2660": {
    "name": "all_reduce",
    "module": "torch.nn.parallel.comm.nccl",
    "fullName": "torch.nn.parallel.comm.nccl.all_reduce",
    "signature": "(inputs, outputs=None, op=0, streams=None, comms=None)",
    "description": "No description available."
  },
  "2661": {
    "name": "broadcast",
    "module": "torch.nn.parallel.comm.nccl",
    "fullName": "torch.nn.parallel.comm.nccl.broadcast",
    "signature": "(inputs: Sequence[torch.Tensor], root: int = 0, streams=None, comms=None) -> None",
    "description": "No description available."
  },
  "2662": {
    "name": "init_rank",
    "module": "torch.nn.parallel.comm.nccl",
    "fullName": "torch.nn.parallel.comm.nccl.init_rank",
    "signature": "(num_ranks, uid, rank)",
    "description": "No description available."
  },
  "2663": {
    "name": "is_available",
    "module": "torch.nn.parallel.comm.nccl",
    "fullName": "torch.nn.parallel.comm.nccl.is_available",
    "signature": "(tensors)",
    "description": "No description available."
  },
  "2664": {
    "name": "reduce",
    "module": "torch.nn.parallel.comm.nccl",
    "fullName": "torch.nn.parallel.comm.nccl.reduce",
    "signature": "(inputs: Sequence[torch.Tensor], output: Union[torch.Tensor, Sequence[torch.Tensor], NoneType] = None, root: int = 0, op: int = 0, streams: Optional[Sequence[torch.cuda.streams.Stream]] = None, comms=None, *, outputs: Optional[Sequence[torch.Tensor]] = None) -> None",
    "description": "No description available."
  },
  "2665": {
    "name": "reduce_scatter",
    "module": "torch.nn.parallel.comm.nccl",
    "fullName": "torch.nn.parallel.comm.nccl.reduce_scatter",
    "signature": "(inputs: Sequence[torch.Tensor], outputs: Sequence[torch.Tensor], op: int = 0, streams=None, comms=None) -> None",
    "description": "No description available."
  },
  "2666": {
    "name": "unique_id",
    "module": "torch.nn.parallel.comm.nccl",
    "fullName": "torch.nn.parallel.comm.nccl.unique_id",
    "signature": "()",
    "description": "No description available."
  },
  "2667": {
    "name": "version",
    "module": "torch.nn.parallel.comm.nccl",
    "fullName": "torch.nn.parallel.comm.nccl.version",
    "signature": "()",
    "description": "No description available."
  },
  "2668": {
    "name": "xavier_uniform_",
    "module": "torch.nn.modules.transformer",
    "fullName": "torch.nn.modules.transformer.xavier_uniform_",
    "signature": "(tensor: torch.Tensor, gain: float = 1.0) -> torch.Tensor",
    "description": "Fills the input `Tensor` with values according to the method"
  },
  "2669": {
    "name": "apply_permutation",
    "module": "torch.nn.modules.rnn",
    "fullName": "torch.nn.modules.rnn.apply_permutation",
    "signature": "(tensor: torch.Tensor, permutation: torch.Tensor, dim: int = 1) -> torch.Tensor",
    "description": "No description available."
  },
  "2670": {
    "name": "overload",
    "module": "torch.nn.modules.rnn",
    "fullName": "torch.nn.modules.rnn.overload",
    "signature": "(func)",
    "description": "Decorator for overloaded functions/methods."
  },
  "2671": {
    "name": "namedtuple",
    "module": "torch.nn.modules.module",
    "fullName": "torch.nn.modules.module.namedtuple",
    "signature": "(typename, field_names, *, rename=False, defaults=None, module=None)",
    "description": "Returns a new subclass of tuple with named fields."
  },
  "2672": {
    "name": "overload",
    "module": "torch.nn.modules.module",
    "fullName": "torch.nn.modules.module.overload",
    "signature": "(func)",
    "description": "Decorator for overloaded functions/methods."
  },
  "2673": {
    "name": "register_module_forward_hook",
    "module": "torch.nn.modules.module",
    "fullName": "torch.nn.modules.module.register_module_forward_hook",
    "signature": "(hook: Callable[..., NoneType]) -> torch.utils.hooks.RemovableHandle",
    "description": "Registers a global forward hook for all the modules"
  },
  "2674": {
    "name": "register_module_forward_pre_hook",
    "module": "torch.nn.modules.module",
    "fullName": "torch.nn.modules.module.register_module_forward_pre_hook",
    "signature": "(hook: Callable[..., NoneType]) -> torch.utils.hooks.RemovableHandle",
    "description": "Registers a forward pre-hook common to all modules."
  },
  "2675": {
    "name": "register_module_full_backward_hook",
    "module": "torch.nn.modules.module",
    "fullName": "torch.nn.modules.module.register_module_full_backward_hook",
    "signature": "(hook: Callable[[ForwardRef('Module'), Union[Tuple[torch.Tensor, ...], torch.Tensor], Union[Tuple[torch.Tensor, ...], torch.Tensor]], Optional[torch.Tensor]]) -> torch.utils.hooks.RemovableHandle",
    "description": "Registers a backward hook common to all the modules."
  },
  "2676": {
    "name": "is_lazy",
    "module": "torch.nn.modules.lazy",
    "fullName": "torch.nn.modules.lazy.is_lazy",
    "signature": "(param)",
    "description": "No description available."
  },
  "2677": {
    "name": "overload",
    "module": "torch.nn.modules.container",
    "fullName": "torch.nn.modules.container.overload",
    "signature": "(func)",
    "description": "Decorator for overloaded functions/methods."
  },
  "2678": {
    "name": "log_softmax",
    "module": "torch.nn.modules.adaptive",
    "fullName": "torch.nn.modules.adaptive.log_softmax",
    "signature": "(input: torch.Tensor, dim: Optional[int] = None, _stacklevel: int = 3, dtype: Optional[int] = None) -> torch.Tensor",
    "description": "Applies a softmax followed by a logarithm."
  },
  "2679": {
    "name": "namedtuple",
    "module": "torch.nn.modules.adaptive",
    "fullName": "torch.nn.modules.adaptive.namedtuple",
    "signature": "(typename, field_names, *, rename=False, defaults=None, module=None)",
    "description": "Returns a new subclass of tuple with named fields."
  },
  "2680": {
    "name": "constant_",
    "module": "torch.nn.modules.activation",
    "fullName": "torch.nn.modules.activation.constant_",
    "signature": "(tensor: torch.Tensor, val: float) -> torch.Tensor",
    "description": "Fills the input Tensor with the value :math:`\\text{val}`."
  },
  "2681": {
    "name": "xavier_normal_",
    "module": "torch.nn.modules.activation",
    "fullName": "torch.nn.modules.activation.xavier_normal_",
    "signature": "(tensor: torch.Tensor, gain: float = 1.0) -> torch.Tensor",
    "description": "Fills the input `Tensor` with values according to the method"
  },
  "2682": {
    "name": "xavier_uniform_",
    "module": "torch.nn.modules.activation",
    "fullName": "torch.nn.modules.activation.xavier_uniform_",
    "signature": "(tensor: torch.Tensor, gain: float = 1.0) -> torch.Tensor",
    "description": "Fills the input `Tensor` with values according to the method"
  },
  "2683": {
    "name": "define",
    "module": "torch.library",
    "fullName": "torch.library.define",
    "signature": "(lib, schema, alias_analysis='')",
    "description": "No description available."
  },
  "2684": {
    "name": "impl",
    "module": "torch.library",
    "fullName": "torch.library.impl",
    "signature": "(lib, name, dispatch_key='')",
    "description": "No description available."
  },
  "2685": {
    "name": "annotate",
    "module": "torch.jit",
    "fullName": "torch.jit.annotate",
    "signature": "(the_type, the_value)",
    "description": "This method is a pass-through function that returns `the_value`, used to hint TorchScript"
  },
  "2686": {
    "name": "contextmanager",
    "module": "torch.jit",
    "fullName": "torch.jit.contextmanager",
    "signature": "(func)",
    "description": "@contextmanager decorator."
  },
  "2687": {
    "name": "enable_onednn_fusion",
    "module": "torch.jit",
    "fullName": "torch.jit.enable_onednn_fusion",
    "signature": "(enabled: bool)",
    "description": "Enables or disables onednn JIT fusion based on the parameter `enabled`."
  },
  "2688": {
    "name": "export",
    "module": "torch.jit",
    "fullName": "torch.jit.export",
    "signature": "(fn)",
    "description": "This decorator indicates that a method on an ``nn.Module`` is used as an entry point into a"
  },
  "2689": {
    "name": "export_opnames",
    "module": "torch.jit",
    "fullName": "torch.jit.export_opnames",
    "signature": "(m)",
    "description": "Generates new bytecode for a Script module and returns what the op list"
  },
  "2690": {
    "name": "fork",
    "module": "torch.jit",
    "fullName": "torch.jit.fork",
    "signature": "(func, *args, **kwargs)",
    "description": "Creates an asynchronous task executing `func` and a reference to the value"
  },
  "2691": {
    "name": "freeze",
    "module": "torch.jit",
    "fullName": "torch.jit.freeze",
    "signature": "(mod, preserved_attrs: Optional[List[str]] = None, optimize_numerics: bool = True)",
    "description": "Freezing a :class:`ScriptModule` will clone it and attempt to inline the cloned"
  },
  "2692": {
    "name": "fuser",
    "module": "torch.jit",
    "fullName": "torch.jit.fuser",
    "signature": "(name)",
    "description": "A context manager that facilitates switching between"
  },
  "2693": {
    "name": "ignore",
    "module": "torch.jit",
    "fullName": "torch.jit.ignore",
    "signature": "(drop=False, **kwargs)",
    "description": "This decorator indicates to the compiler that a function or method should"
  },
  "2694": {
    "name": "interface",
    "module": "torch.jit",
    "fullName": "torch.jit.interface",
    "signature": "(obj)",
    "description": "No description available."
  },
  "2695": {
    "name": "is_scripting",
    "module": "torch.jit",
    "fullName": "torch.jit.is_scripting",
    "signature": "() -> bool",
    "description": "Function that returns True when in compilation and False otherwise. This"
  },
  "2696": {
    "name": "is_tracing",
    "module": "torch.jit",
    "fullName": "torch.jit.is_tracing",
    "signature": "()",
    "description": "Returns ``True`` in tracing (if a function is called during the tracing of"
  },
  "2697": {
    "name": "isinstance",
    "module": "torch.jit",
    "fullName": "torch.jit.isinstance",
    "signature": "(obj, target_type)",
    "description": "This function provides for conatiner type refinement in TorchScript. It can refine"
  },
  "2698": {
    "name": "jit_module_from_flatbuffer",
    "module": "torch.jit",
    "fullName": "torch.jit.jit_module_from_flatbuffer",
    "signature": "(f)",
    "description": "No description available."
  },
  "2699": {
    "name": "load",
    "module": "torch.jit",
    "fullName": "torch.jit.load",
    "signature": "(f, map_location=None, _extra_files=None)",
    "description": "Load a :class:`ScriptModule` or :class:`ScriptFunction` previously"
  },
  "2700": {
    "name": "onednn_fusion_enabled",
    "module": "torch.jit",
    "fullName": "torch.jit.onednn_fusion_enabled",
    "signature": "()",
    "description": "Returns whether onednn JIT fusion is enabled"
  },
  "2701": {
    "name": "optimize_for_inference",
    "module": "torch.jit",
    "fullName": "torch.jit.optimize_for_inference",
    "signature": "(mod: torch.jit._script.ScriptModule, other_methods: Optional[List[str]] = None) -> torch.jit._script.ScriptModule",
    "description": "Performs a set of optimization passes to optimize a model for the"
  },
  "2702": {
    "name": "optimized_execution",
    "module": "torch.jit",
    "fullName": "torch.jit.optimized_execution",
    "signature": "(should_optimize)",
    "description": "A context manager that controls whether the JIT's executor will run"
  },
  "2703": {
    "name": "run_frozen_optimizations",
    "module": "torch.jit",
    "fullName": "torch.jit.run_frozen_optimizations",
    "signature": "(mod, optimize_numerics: bool = True, preserved_methods: Optional[List[str]] = None)",
    "description": "Runs a series of optimizations looking for patterns that occur in frozen graphs."
  },
  "2704": {
    "name": "save",
    "module": "torch.jit",
    "fullName": "torch.jit.save",
    "signature": "(m, f, _extra_files=None)",
    "description": "Save an offline version of this module for use in a separate process. The"
  },
  "2705": {
    "name": "save_jit_module_to_flatbuffer",
    "module": "torch.jit",
    "fullName": "torch.jit.save_jit_module_to_flatbuffer",
    "signature": "(m, f, _extra_files=None)",
    "description": "Save an offline version of this module for use in a separate process. The"
  },
  "2706": {
    "name": "script",
    "module": "torch.jit",
    "fullName": "torch.jit.script",
    "signature": "(obj, optimize=None, _frames_up=0, _rcb=None, example_inputs: Union[List[Tuple], Dict[Callable, List[Tuple]], NoneType] = None)",
    "description": "Scripting a function or ``nn.Module`` will inspect the source code, compile"
  },
  "2707": {
    "name": "script_if_tracing",
    "module": "torch.jit",
    "fullName": "torch.jit.script_if_tracing",
    "signature": "(fn)",
    "description": "Compiles ``fn`` when it is first called during tracing. ``torch.jit.script``"
  },
  "2708": {
    "name": "script_method",
    "module": "torch.jit",
    "fullName": "torch.jit.script_method",
    "signature": "(fn)",
    "description": "No description available."
  },
  "2709": {
    "name": "set_fusion_strategy",
    "module": "torch.jit",
    "fullName": "torch.jit.set_fusion_strategy",
    "signature": "(strategy: List[Tuple[str, int]])",
    "description": "Sets the type and number of specializations that can occur during fusion."
  },
  "2710": {
    "name": "set_module",
    "module": "torch.jit",
    "fullName": "torch.jit.set_module",
    "signature": "(obj, mod)",
    "description": "No description available."
  },
  "2711": {
    "name": "trace",
    "module": "torch.jit",
    "fullName": "torch.jit.trace",
    "signature": "(func, example_inputs, optimize=None, check_trace=True, check_inputs=None, check_tolerance=1e-05, strict=True, _force_outplace=False, _module_class=None, _compilation_unit=<torch.jit.CompilationUnit object at 0x7fce1efe5730>)",
    "description": "Trace a function and return an executable  or :class:`ScriptFunction`"
  },
  "2712": {
    "name": "trace_module",
    "module": "torch.jit",
    "fullName": "torch.jit.trace_module",
    "signature": "(mod, inputs, optimize=None, check_trace=True, check_inputs=None, check_tolerance=1e-05, strict=True, _force_outplace=False, _module_class=None, _compilation_unit=<torch.jit.CompilationUnit object at 0x7fce1efe5730>)",
    "description": "Trace a module and return an executable :class:`ScriptModule` that will be optimized"
  },
  "2713": {
    "name": "unused",
    "module": "torch.jit",
    "fullName": "torch.jit.unused",
    "signature": "(fn)",
    "description": "This decorator indicates to the compiler that a function or method should"
  },
  "2714": {
    "name": "wait",
    "module": "torch.jit",
    "fullName": "torch.jit.wait",
    "signature": "(future)",
    "description": "Forces completion of a `torch.jit.Future[T]` asynchronous task, returning the"
  },
  "2715": {
    "name": "build_class_def",
    "module": "torch.jit.frontend",
    "fullName": "torch.jit.frontend.build_class_def",
    "signature": "(ctx, py_def, methods, properties, self_name, assigns)",
    "description": "No description available."
  },
  "2716": {
    "name": "build_def",
    "module": "torch.jit.frontend",
    "fullName": "torch.jit.frontend.build_def",
    "signature": "(ctx, py_def, type_line, def_name, self_name=None, pdt_arg_types=None)",
    "description": "No description available."
  },
  "2717": {
    "name": "build_ignore_context_manager",
    "module": "torch.jit.frontend",
    "fullName": "torch.jit.frontend.build_ignore_context_manager",
    "signature": "(ctx, stmt)",
    "description": "No description available."
  },
  "2718": {
    "name": "build_param",
    "module": "torch.jit.frontend",
    "fullName": "torch.jit.frontend.build_param",
    "signature": "(ctx, py_arg, self_name, kwarg_only, pdt_arg_type=None)",
    "description": "No description available."
  },
  "2719": {
    "name": "build_param_list",
    "module": "torch.jit.frontend",
    "fullName": "torch.jit.frontend.build_param_list",
    "signature": "(ctx, py_args, self_name, pdt_arg_types=None)",
    "description": "No description available."
  },
  "2720": {
    "name": "build_stmts",
    "module": "torch.jit.frontend",
    "fullName": "torch.jit.frontend.build_stmts",
    "signature": "(ctx, stmts)",
    "description": "No description available."
  },
  "2721": {
    "name": "build_withitems",
    "module": "torch.jit.frontend",
    "fullName": "torch.jit.frontend.build_withitems",
    "signature": "(ctx, items)",
    "description": "No description available."
  },
  "2722": {
    "name": "dedent",
    "module": "torch.jit.frontend",
    "fullName": "torch.jit.frontend.dedent",
    "signature": "(text)",
    "description": "Remove any common leading whitespace from every line in `text`."
  },
  "2723": {
    "name": "find_before",
    "module": "torch.jit.frontend",
    "fullName": "torch.jit.frontend.find_before",
    "signature": "(ctx, pos, substr, offsets=(0, 0))",
    "description": "No description available."
  },
  "2724": {
    "name": "get_class_assigns",
    "module": "torch.jit.frontend",
    "fullName": "torch.jit.frontend.get_class_assigns",
    "signature": "(ctx, cls_ast)",
    "description": "No description available."
  },
  "2725": {
    "name": "get_class_properties",
    "module": "torch.jit.frontend",
    "fullName": "torch.jit.frontend.get_class_properties",
    "signature": "(cls, self_name)",
    "description": "Get a list of Property objects representing the properties of a class."
  },
  "2726": {
    "name": "get_default_args",
    "module": "torch.jit.frontend",
    "fullName": "torch.jit.frontend.get_default_args",
    "signature": "(fn)",
    "description": "No description available."
  },
  "2727": {
    "name": "get_default_args_for_class",
    "module": "torch.jit.frontend",
    "fullName": "torch.jit.frontend.get_default_args_for_class",
    "signature": "(cls)",
    "description": "Get default arguments for all methods in a class (except for static methods)."
  },
  "2728": {
    "name": "get_jit_class_def",
    "module": "torch.jit.frontend",
    "fullName": "torch.jit.frontend.get_jit_class_def",
    "signature": "(cls, self_name)",
    "description": "No description available."
  },
  "2729": {
    "name": "get_jit_def",
    "module": "torch.jit.frontend",
    "fullName": "torch.jit.frontend.get_jit_def",
    "signature": "(fn, def_name, self_name=None, is_classmethod=False)",
    "description": "Build a JIT AST (TreeView) from the given function."
  },
  "2730": {
    "name": "get_qualified_name",
    "module": "torch.jit.frontend",
    "fullName": "torch.jit.frontend.get_qualified_name",
    "signature": "(func)",
    "description": "No description available."
  },
  "2731": {
    "name": "get_source_lines_and_file",
    "module": "torch.jit.frontend",
    "fullName": "torch.jit.frontend.get_source_lines_and_file",
    "signature": "(obj: Any, error_msg: Optional[str] = None) -> Tuple[List[str], int, Optional[str]]",
    "description": "Wrapper around inspect.getsourcelines and inspect.getsourcefile."
  },
  "2732": {
    "name": "is_reserved_name",
    "module": "torch.jit.frontend",
    "fullName": "torch.jit.frontend.is_reserved_name",
    "signature": "(name)",
    "description": "No description available."
  },
  "2733": {
    "name": "is_static_fn",
    "module": "torch.jit.frontend",
    "fullName": "torch.jit.frontend.is_static_fn",
    "signature": "(cls, fn) -> bool",
    "description": "No description available."
  },
  "2734": {
    "name": "is_torch_jit_ignore_context_manager",
    "module": "torch.jit.frontend",
    "fullName": "torch.jit.frontend.is_torch_jit_ignore_context_manager",
    "signature": "(stmt)",
    "description": "No description available."
  },
  "2735": {
    "name": "make_source_context",
    "module": "torch.jit.frontend",
    "fullName": "torch.jit.frontend.make_source_context",
    "signature": "(*args)",
    "description": "No description available."
  },
  "2736": {
    "name": "namedtuple",
    "module": "torch.jit.frontend",
    "fullName": "torch.jit.frontend.namedtuple",
    "signature": "(typename, field_names, *, rename=False, defaults=None, module=None)",
    "description": "Returns a new subclass of tuple with named fields."
  },
  "2737": {
    "name": "parse_def",
    "module": "torch.jit.frontend",
    "fullName": "torch.jit.frontend.parse_def",
    "signature": "(fn)",
    "description": "No description available."
  },
  "2738": {
    "name": "should_drop",
    "module": "torch.jit.frontend",
    "fullName": "torch.jit.frontend.should_drop",
    "signature": "(fn) -> bool",
    "description": "No description available."
  },
  "2739": {
    "name": "capwords",
    "module": "torch.jit.frontend.string",
    "fullName": "torch.jit.frontend.string.capwords",
    "signature": "(s, sep=None)",
    "description": "capwords(s [,sep]) -> string"
  },
  "2740": {
    "name": "dump",
    "module": "torch.jit.frontend.astunparse",
    "fullName": "torch.jit.frontend.astunparse.dump",
    "signature": "(tree)",
    "description": "No description available."
  },
  "2741": {
    "name": "unparse",
    "module": "torch.jit.frontend.astunparse",
    "fullName": "torch.jit.frontend.astunparse.unparse",
    "signature": "(tree)",
    "description": "No description available."
  },
  "2742": {
    "name": "interleave",
    "module": "torch.jit.frontend.astunparse.unparser",
    "fullName": "torch.jit.frontend.astunparse.unparser.interleave",
    "signature": "(inter, f, seq)",
    "description": "Call f on each item in seq, calling inter() in between."
  },
  "2743": {
    "name": "main",
    "module": "torch.jit.frontend.astunparse.unparser",
    "fullName": "torch.jit.frontend.astunparse.unparser.main",
    "signature": "(args)",
    "description": "No description available."
  },
  "2744": {
    "name": "roundtrip",
    "module": "torch.jit.frontend.astunparse.unparser",
    "fullName": "torch.jit.frontend.astunparse.unparser.roundtrip",
    "signature": "(filename, output=<_io.TextIOWrapper name='<stdout>' mode='w' encoding='utf-8'>)",
    "description": "No description available."
  },
  "2745": {
    "name": "testdir",
    "module": "torch.jit.frontend.astunparse.unparser",
    "fullName": "torch.jit.frontend.astunparse.unparser.testdir",
    "signature": "(a)",
    "description": "No description available."
  },
  "2746": {
    "name": "add_metaclass",
    "module": "torch.jit.frontend.astunparse.unparser.six",
    "fullName": "torch.jit.frontend.astunparse.unparser.six.add_metaclass",
    "signature": "(metaclass)",
    "description": "Class decorator for creating a class with a metaclass."
  },
  "2747": {
    "name": "add_move",
    "module": "torch.jit.frontend.astunparse.unparser.six",
    "fullName": "torch.jit.frontend.astunparse.unparser.six.add_move",
    "signature": "(move)",
    "description": "Add an item to six.moves."
  },
  "2748": {
    "name": "assertCountEqual",
    "module": "torch.jit.frontend.astunparse.unparser.six",
    "fullName": "torch.jit.frontend.astunparse.unparser.six.assertCountEqual",
    "signature": "(self, *args, **kwargs)",
    "description": "No description available."
  },
  "2749": {
    "name": "assertNotRegex",
    "module": "torch.jit.frontend.astunparse.unparser.six",
    "fullName": "torch.jit.frontend.astunparse.unparser.six.assertNotRegex",
    "signature": "(self, *args, **kwargs)",
    "description": "No description available."
  },
  "2750": {
    "name": "assertRaisesRegex",
    "module": "torch.jit.frontend.astunparse.unparser.six",
    "fullName": "torch.jit.frontend.astunparse.unparser.six.assertRaisesRegex",
    "signature": "(self, *args, **kwargs)",
    "description": "No description available."
  },
  "2751": {
    "name": "assertRegex",
    "module": "torch.jit.frontend.astunparse.unparser.six",
    "fullName": "torch.jit.frontend.astunparse.unparser.six.assertRegex",
    "signature": "(self, *args, **kwargs)",
    "description": "No description available."
  },
  "2752": {
    "name": "b",
    "module": "torch.jit.frontend.astunparse.unparser.six",
    "fullName": "torch.jit.frontend.astunparse.unparser.six.b",
    "signature": "(s)",
    "description": "Byte literal"
  },
  "2753": {
    "name": "create_unbound_method",
    "module": "torch.jit.frontend.astunparse.unparser.six",
    "fullName": "torch.jit.frontend.astunparse.unparser.six.create_unbound_method",
    "signature": "(func, cls)",
    "description": "No description available."
  },
  "2754": {
    "name": "ensure_binary",
    "module": "torch.jit.frontend.astunparse.unparser.six",
    "fullName": "torch.jit.frontend.astunparse.unparser.six.ensure_binary",
    "signature": "(s, encoding='utf-8', errors='strict')",
    "description": "Coerce **s** to six.binary_type."
  },
  "2755": {
    "name": "ensure_str",
    "module": "torch.jit.frontend.astunparse.unparser.six",
    "fullName": "torch.jit.frontend.astunparse.unparser.six.ensure_str",
    "signature": "(s, encoding='utf-8', errors='strict')",
    "description": "Coerce *s* to `str`."
  },
  "2756": {
    "name": "ensure_text",
    "module": "torch.jit.frontend.astunparse.unparser.six",
    "fullName": "torch.jit.frontend.astunparse.unparser.six.ensure_text",
    "signature": "(s, encoding='utf-8', errors='strict')",
    "description": "Coerce *s* to six.text_type."
  },
  "2757": {
    "name": "get_unbound_function",
    "module": "torch.jit.frontend.astunparse.unparser.six",
    "fullName": "torch.jit.frontend.astunparse.unparser.six.get_unbound_function",
    "signature": "(unbound)",
    "description": "Get the function out of a possibly unbound function"
  },
  "2758": {
    "name": "iteritems",
    "module": "torch.jit.frontend.astunparse.unparser.six",
    "fullName": "torch.jit.frontend.astunparse.unparser.six.iteritems",
    "signature": "(d, **kw)",
    "description": "Return an iterator over the (key, value) pairs of a dictionary."
  },
  "2759": {
    "name": "iterkeys",
    "module": "torch.jit.frontend.astunparse.unparser.six",
    "fullName": "torch.jit.frontend.astunparse.unparser.six.iterkeys",
    "signature": "(d, **kw)",
    "description": "Return an iterator over the keys of a dictionary."
  },
  "2760": {
    "name": "iterlists",
    "module": "torch.jit.frontend.astunparse.unparser.six",
    "fullName": "torch.jit.frontend.astunparse.unparser.six.iterlists",
    "signature": "(d, **kw)",
    "description": "Return an iterator over the (key, [values]) pairs of a dictionary."
  },
  "2761": {
    "name": "itervalues",
    "module": "torch.jit.frontend.astunparse.unparser.six",
    "fullName": "torch.jit.frontend.astunparse.unparser.six.itervalues",
    "signature": "(d, **kw)",
    "description": "Return an iterator over the values of a dictionary."
  },
  "2762": {
    "name": "python_2_unicode_compatible",
    "module": "torch.jit.frontend.astunparse.unparser.six",
    "fullName": "torch.jit.frontend.astunparse.unparser.six.python_2_unicode_compatible",
    "signature": "(klass)",
    "description": "A class decorator that defines __unicode__ and __str__ methods under Python 2."
  },
  "2763": {
    "name": "raise_from",
    "module": "torch.jit.frontend.astunparse.unparser.six",
    "fullName": "torch.jit.frontend.astunparse.unparser.six.raise_from",
    "signature": "(value, from_value)",
    "description": "No description available."
  },
  "2764": {
    "name": "remove_move",
    "module": "torch.jit.frontend.astunparse.unparser.six",
    "fullName": "torch.jit.frontend.astunparse.unparser.six.remove_move",
    "signature": "(name)",
    "description": "Remove item from six.moves."
  },
  "2765": {
    "name": "reraise",
    "module": "torch.jit.frontend.astunparse.unparser.six",
    "fullName": "torch.jit.frontend.astunparse.unparser.six.reraise",
    "signature": "(tp, value, tb=None)",
    "description": "Reraise an exception."
  },
  "2766": {
    "name": "spec_from_loader",
    "module": "torch.jit.frontend.astunparse.unparser.six",
    "fullName": "torch.jit.frontend.astunparse.unparser.six.spec_from_loader",
    "signature": "(name, loader, *, origin=None, is_package=None)",
    "description": "Return a module spec based on various loader methods."
  },
  "2767": {
    "name": "u",
    "module": "torch.jit.frontend.astunparse.unparser.six",
    "fullName": "torch.jit.frontend.astunparse.unparser.six.u",
    "signature": "(s)",
    "description": "Text literal"
  },
  "2768": {
    "name": "with_metaclass",
    "module": "torch.jit.frontend.astunparse.unparser.six",
    "fullName": "torch.jit.frontend.astunparse.unparser.six.with_metaclass",
    "signature": "(meta, *bases)",
    "description": "Create a base class with a metaclass."
  },
  "2769": {
    "name": "wraps",
    "module": "torch.jit.frontend.astunparse.unparser.six",
    "fullName": "torch.jit.frontend.astunparse.unparser.six.wraps",
    "signature": "(wrapped, assigned=('__module__', '__name__', '__qualname__', '__doc__', '__annotations__'), updated=('__dict__',))",
    "description": "Decorator factory to apply update_wrapper() to a wrapper function"
  },
  "2770": {
    "name": "ann_to_type",
    "module": "torch.jit.annotations",
    "fullName": "torch.jit.annotations.ann_to_type",
    "signature": "(ann, loc)",
    "description": "No description available."
  },
  "2771": {
    "name": "check_fn",
    "module": "torch.jit.annotations",
    "fullName": "torch.jit.annotations.check_fn",
    "signature": "(fn, loc)",
    "description": "No description available."
  },
  "2772": {
    "name": "dedent",
    "module": "torch.jit.annotations",
    "fullName": "torch.jit.annotations.dedent",
    "signature": "(text)",
    "description": "Remove any common leading whitespace from every line in `text`."
  },
  "2773": {
    "name": "get_enum_value_type",
    "module": "torch.jit.annotations",
    "fullName": "torch.jit.annotations.get_enum_value_type",
    "signature": "(e: Type[enum.Enum], loc)",
    "description": "No description available."
  },
  "2774": {
    "name": "get_param_names",
    "module": "torch.jit.annotations",
    "fullName": "torch.jit.annotations.get_param_names",
    "signature": "(fn, n_args)",
    "description": "No description available."
  },
  "2775": {
    "name": "get_signature",
    "module": "torch.jit.annotations",
    "fullName": "torch.jit.annotations.get_signature",
    "signature": "(fn, rcb, loc, is_method)",
    "description": "No description available."
  },
  "2776": {
    "name": "get_source_lines_and_file",
    "module": "torch.jit.annotations",
    "fullName": "torch.jit.annotations.get_source_lines_and_file",
    "signature": "(obj: Any, error_msg: Optional[str] = None) -> Tuple[List[str], int, Optional[str]]",
    "description": "Wrapper around inspect.getsourcelines and inspect.getsourcefile."
  },
  "2777": {
    "name": "get_type_line",
    "module": "torch.jit.annotations",
    "fullName": "torch.jit.annotations.get_type_line",
    "signature": "(source)",
    "description": "Tries to find the line containing a comment with the type annotation."
  },
  "2778": {
    "name": "is_dict",
    "module": "torch.jit.annotations",
    "fullName": "torch.jit.annotations.is_dict",
    "signature": "(ann) -> bool",
    "description": "No description available."
  },
  "2779": {
    "name": "is_function_or_method",
    "module": "torch.jit.annotations",
    "fullName": "torch.jit.annotations.is_function_or_method",
    "signature": "(the_callable)",
    "description": "No description available."
  },
  "2780": {
    "name": "is_future",
    "module": "torch.jit.annotations",
    "fullName": "torch.jit.annotations.is_future",
    "signature": "(ann) -> bool",
    "description": "No description available."
  },
  "2781": {
    "name": "is_ignored_fn",
    "module": "torch.jit.annotations",
    "fullName": "torch.jit.annotations.is_ignored_fn",
    "signature": "(fn) -> bool",
    "description": "No description available."
  },
  "2782": {
    "name": "is_list",
    "module": "torch.jit.annotations",
    "fullName": "torch.jit.annotations.is_list",
    "signature": "(ann) -> bool",
    "description": "No description available."
  },
  "2783": {
    "name": "is_optional",
    "module": "torch.jit.annotations",
    "fullName": "torch.jit.annotations.is_optional",
    "signature": "(ann)",
    "description": "No description available."
  },
  "2784": {
    "name": "is_rref",
    "module": "torch.jit.annotations",
    "fullName": "torch.jit.annotations.is_rref",
    "signature": "(ann) -> bool",
    "description": "No description available."
  },
  "2785": {
    "name": "is_tensor",
    "module": "torch.jit.annotations",
    "fullName": "torch.jit.annotations.is_tensor",
    "signature": "(ann)",
    "description": "No description available."
  },
  "2786": {
    "name": "is_tuple",
    "module": "torch.jit.annotations",
    "fullName": "torch.jit.annotations.is_tuple",
    "signature": "(ann) -> bool",
    "description": "No description available."
  },
  "2787": {
    "name": "is_union",
    "module": "torch.jit.annotations",
    "fullName": "torch.jit.annotations.is_union",
    "signature": "(ann)",
    "description": "No description available."
  },
  "2788": {
    "name": "is_vararg",
    "module": "torch.jit.annotations",
    "fullName": "torch.jit.annotations.is_vararg",
    "signature": "(the_callable)",
    "description": "No description available."
  },
  "2789": {
    "name": "parse_type_line",
    "module": "torch.jit.annotations",
    "fullName": "torch.jit.annotations.parse_type_line",
    "signature": "(type_line, rcb, loc)",
    "description": "Parses a type annotation specified as a comment."
  },
  "2790": {
    "name": "split_type_line",
    "module": "torch.jit.annotations",
    "fullName": "torch.jit.annotations.split_type_line",
    "signature": "(type_line)",
    "description": "Splits the comment with the type annotation into parts for argument and return types."
  },
  "2791": {
    "name": "try_ann_to_type",
    "module": "torch.jit.annotations",
    "fullName": "torch.jit.annotations.try_ann_to_type",
    "signature": "(ann, loc)",
    "description": "No description available."
  },
  "2792": {
    "name": "try_real_annotations",
    "module": "torch.jit.annotations",
    "fullName": "torch.jit.annotations.try_real_annotations",
    "signature": "(fn, loc)",
    "description": "Tries to use the Py3.5+ annotation syntax to get the type."
  },
  "2793": {
    "name": "download_url_to_file",
    "module": "torch.hub",
    "fullName": "torch.hub.download_url_to_file",
    "signature": "(url, dst, hash_prefix=None, progress=True)",
    "description": "Download object at the given URL to a local path."
  },
  "2794": {
    "name": "get_dir",
    "module": "torch.hub",
    "fullName": "torch.hub.get_dir",
    "signature": "()",
    "description": "Get the Torch Hub cache directory used for storing downloaded models & weights."
  },
  "2795": {
    "name": "help",
    "module": "torch.hub",
    "fullName": "torch.hub.help",
    "signature": "(github, model, force_reload=False, skip_validation=False, trust_repo=None)",
    "description": "Show the docstring of entrypoint ``model``."
  },
  "2796": {
    "name": "list",
    "module": "torch.hub",
    "fullName": "torch.hub.list",
    "signature": "(github, force_reload=False, skip_validation=False, trust_repo=None)",
    "description": "List all callable entrypoints available in the repo specified by ``github``."
  },
  "2797": {
    "name": "load",
    "module": "torch.hub",
    "fullName": "torch.hub.load",
    "signature": "(repo_or_dir, model, *args, source='github', trust_repo=None, force_reload=False, verbose=True, skip_validation=False, **kwargs)",
    "description": "Load a model from a github repo or a local directory."
  },
  "2798": {
    "name": "load_state_dict_from_url",
    "module": "torch.hub",
    "fullName": "torch.hub.load_state_dict_from_url",
    "signature": "(url: str, model_dir: Optional[str] = None, map_location: Union[Callable[[str], str], Dict[str, str], NoneType] = None, progress: bool = True, check_hash: bool = False, file_name: Optional[str] = None) -> Dict[str, Any]",
    "description": "Loads the Torch serialized object at the given URL."
  },
  "2799": {
    "name": "set_dir",
    "module": "torch.hub",
    "fullName": "torch.hub.set_dir",
    "signature": "(d)",
    "description": "Optionally set the Torch Hub directory used to save downloaded models & weights."
  },
  "2800": {
    "name": "urlopen",
    "module": "torch.hub",
    "fullName": "torch.hub.urlopen",
    "signature": "(url, data=None, timeout=<object object at 0x7fcdd1b24220>, *, cafile=None, capath=None, cadefault=False, context=None)",
    "description": "Open the URL url, which can be either a string or a Request object."
  },
  "2801": {
    "name": "urlparse",
    "module": "torch.hub",
    "fullName": "torch.hub.urlparse",
    "signature": "(url, scheme='', allow_fragments=True)",
    "description": "Parse a URL into 6 components:"
  },
  "2802": {
    "name": "is_zipfile",
    "module": "torch.hub.zipfile",
    "fullName": "torch.hub.zipfile.is_zipfile",
    "signature": "(filename)",
    "description": "Quickly see if a file is a ZIP file by checking the magic number."
  },
  "2803": {
    "name": "main",
    "module": "torch.hub.zipfile",
    "fullName": "torch.hub.zipfile.main",
    "signature": "(args=None)",
    "description": "No description available."
  },
  "2804": {
    "name": "compress",
    "module": "torch.hub.zipfile.lzma",
    "fullName": "torch.hub.zipfile.lzma.compress",
    "signature": "(data, format=1, check=-1, preset=None, filters=None)",
    "description": "Compress a block of data."
  },
  "2805": {
    "name": "decompress",
    "module": "torch.hub.zipfile.lzma",
    "fullName": "torch.hub.zipfile.lzma.decompress",
    "signature": "(data, format=0, memlimit=None, filters=None)",
    "description": "Decompress a block of data."
  },
  "2806": {
    "name": "open",
    "module": "torch.hub.zipfile.lzma",
    "fullName": "torch.hub.zipfile.lzma.open",
    "signature": "(filename, mode='rb', *, format=None, check=-1, preset=None, filters=None, encoding=None, errors=None, newline=None)",
    "description": "Open an LZMA-compressed file in binary or text mode."
  },
  "2807": {
    "name": "RLock",
    "module": "torch.hub.zipfile.bz2",
    "fullName": "torch.hub.zipfile.bz2.RLock",
    "signature": "(*args, **kwargs)",
    "description": "Factory function that returns a new reentrant lock."
  },
  "2808": {
    "name": "compress",
    "module": "torch.hub.zipfile.bz2",
    "fullName": "torch.hub.zipfile.bz2.compress",
    "signature": "(data, compresslevel=9)",
    "description": "Compress a block of data."
  },
  "2809": {
    "name": "decompress",
    "module": "torch.hub.zipfile.bz2",
    "fullName": "torch.hub.zipfile.bz2.decompress",
    "signature": "(data)",
    "description": "Decompress a block of data."
  },
  "2810": {
    "name": "open",
    "module": "torch.hub.zipfile.bz2",
    "fullName": "torch.hub.zipfile.bz2.open",
    "signature": "(filename, mode='rb', compresslevel=9, encoding=None, errors=None, newline=None)",
    "description": "Open a bzip2-compressed file in binary or text mode."
  },
  "2811": {
    "name": "new",
    "module": "torch.hub.hashlib",
    "fullName": "torch.hub.hashlib.new",
    "signature": "(name, data=b'', **kwargs)",
    "description": "new(name, data=b'') - Return a new hashing object using the named algorithm;"
  },
  "2812": {
    "name": "cast",
    "module": "torch.futures",
    "fullName": "torch.futures.cast",
    "signature": "(typ, val)",
    "description": "Cast a value to a type."
  },
  "2813": {
    "name": "collect_all",
    "module": "torch.futures",
    "fullName": "torch.futures.collect_all",
    "signature": "(futures: 'List[Future]') -> 'Future[List[Future]]'",
    "description": "Collects the provided :class:`~torch.futures.Future` objects into a single"
  },
  "2814": {
    "name": "wait_all",
    "module": "torch.futures",
    "fullName": "torch.futures.wait_all",
    "signature": "(futures: 'List[Future]') -> 'List'",
    "description": "Waits for all provided futures to be complete, and returns"
  },
  "2815": {
    "name": "align_tensors",
    "module": "torch.functional",
    "fullName": "torch.functional.align_tensors",
    "signature": "(*tensors)",
    "description": "No description available."
  },
  "2816": {
    "name": "atleast_1d",
    "module": "torch.functional",
    "fullName": "torch.functional.atleast_1d",
    "signature": "(*tensors)",
    "description": "Returns a 1-dimensional view of each input tensor with zero dimensions."
  },
  "2817": {
    "name": "atleast_2d",
    "module": "torch.functional",
    "fullName": "torch.functional.atleast_2d",
    "signature": "(*tensors)",
    "description": "Returns a 2-dimensional view of each input tensor with zero dimensions."
  },
  "2818": {
    "name": "atleast_3d",
    "module": "torch.functional",
    "fullName": "torch.functional.atleast_3d",
    "signature": "(*tensors)",
    "description": "Returns a 3-dimensional view of each input tensor with zero dimensions."
  },
  "2819": {
    "name": "block_diag",
    "module": "torch.functional",
    "fullName": "torch.functional.block_diag",
    "signature": "(*tensors)",
    "description": "Create a block diagonal matrix from provided tensors."
  },
  "2820": {
    "name": "boolean_dispatch",
    "module": "torch.functional",
    "fullName": "torch.functional.boolean_dispatch",
    "signature": "(arg_name, arg_index, default, if_true, if_false, module_name, func_name)",
    "description": "Dispatches to either of 2 script functions based on a boolean argument."
  },
  "2821": {
    "name": "broadcast_shapes",
    "module": "torch.functional",
    "fullName": "torch.functional.broadcast_shapes",
    "signature": "(*shapes)",
    "description": "broadcast_shapes(*shapes) -> Size"
  },
  "2822": {
    "name": "broadcast_tensors",
    "module": "torch.functional",
    "fullName": "torch.functional.broadcast_tensors",
    "signature": "(*tensors)",
    "description": "broadcast_tensors(*tensors) -> List of Tensors"
  },
  "2823": {
    "name": "cartesian_prod",
    "module": "torch.functional",
    "fullName": "torch.functional.cartesian_prod",
    "signature": "(*tensors)",
    "description": "Do cartesian product of the given sequence of tensors. The behavior is similar to"
  },
  "2824": {
    "name": "cdist",
    "module": "torch.functional",
    "fullName": "torch.functional.cdist",
    "signature": "(x1, x2, p=2.0, compute_mode='use_mm_for_euclid_dist_if_necessary')",
    "description": "Computes batched the p-norm distance between each pair of the two collections of row vectors."
  },
  "2825": {
    "name": "einsum",
    "module": "torch.functional",
    "fullName": "torch.functional.einsum",
    "signature": "(*args: Any) -> torch.Tensor",
    "description": "einsum(equation, *operands) -> Tensor"
  },
  "2826": {
    "name": "handle_torch_function",
    "module": "torch.functional",
    "fullName": "torch.functional.handle_torch_function",
    "signature": "(public_api: Callable, relevant_args: Iterable[Any], *args, **kwargs) -> Any",
    "description": "Implement a function with checks for ``__torch_function__`` overrides."
  },
  "2827": {
    "name": "lu",
    "module": "torch.functional",
    "fullName": "torch.functional.lu",
    "signature": "(*args, **kwargs)",
    "description": "Computes the LU factorization of a matrix or batches of matrices"
  },
  "2828": {
    "name": "meshgrid",
    "module": "torch.functional",
    "fullName": "torch.functional.meshgrid",
    "signature": "(*tensors, indexing: Optional[str] = None) -> Tuple[torch.Tensor, ...]",
    "description": "Creates grids of coordinates specified by the 1D inputs in `attr`:tensors."
  },
  "2829": {
    "name": "overload",
    "module": "torch.functional",
    "fullName": "torch.functional.overload",
    "signature": "(func)",
    "description": "No description available."
  },
  "2830": {
    "name": "pca_lowrank",
    "module": "torch.functional",
    "fullName": "torch.functional.pca_lowrank",
    "signature": "(A: torch.Tensor, q: Optional[int] = None, center: bool = True, niter: int = 2) -> Tuple[torch.Tensor, torch.Tensor, torch.Tensor]",
    "description": "Performs linear Principal Component Analysis (PCA) on a low-rank"
  },
  "2831": {
    "name": "split",
    "module": "torch.functional",
    "fullName": "torch.functional.split",
    "signature": "(tensor: torch.Tensor, split_size_or_sections: Union[int, List[int]], dim: int = 0) -> List[torch.Tensor]",
    "description": "Splits the tensor into chunks. Each chunk is a view of the original tensor."
  },
  "2832": {
    "name": "svd_lowrank",
    "module": "torch.functional",
    "fullName": "torch.functional.svd_lowrank",
    "signature": "(A: torch.Tensor, q: Optional[int] = 6, niter: Optional[int] = 2, M: Optional[torch.Tensor] = None) -> Tuple[torch.Tensor, torch.Tensor, torch.Tensor]",
    "description": "Return the singular value decomposition ``(U, S, V)`` of a matrix,"
  },
  "2833": {
    "name": "tensordot",
    "module": "torch.functional",
    "fullName": "torch.functional.tensordot",
    "signature": "(a, b, dims=2, out: Optional[torch.Tensor] = None)",
    "description": "Returns a contraction of a and b over multiple dimensions."
  },
  "2834": {
    "name": "unique",
    "module": "torch.functional",
    "fullName": "torch.functional.unique",
    "signature": "(*args, **kwargs)",
    "description": "unique(input, sorted=True, return_inverse=False, return_counts=False, dim=None) -> Tuple[Tensor, Tensor, Tensor]"
  },
  "2835": {
    "name": "unique_consecutive",
    "module": "torch.functional",
    "fullName": "torch.functional.unique_consecutive",
    "signature": "(*args, **kwargs)",
    "description": "Eliminates all but the first element from every consecutive group of equivalent elements."
  },
  "2836": {
    "name": "kl_divergence",
    "module": "torch.distributions",
    "fullName": "torch.distributions.kl_divergence",
    "signature": "(p, q)",
    "description": "Compute Kullback-Leibler divergence :math:`KL(p \\| q)` between two distributions."
  },
  "2837": {
    "name": "register_kl",
    "module": "torch.distributions",
    "fullName": "torch.distributions.register_kl",
    "signature": "(type_p, type_q)",
    "description": "Decorator to register a pairwise function with :meth:`kl_divergence`."
  },
  "2838": {
    "name": "is_dependent",
    "module": "torch.distributions.wishart.constraints",
    "fullName": "torch.distributions.wishart.constraints.is_dependent",
    "signature": "(constraint)",
    "description": "No description available."
  },
  "2839": {
    "name": "broadcast_all",
    "module": "torch.distributions.weibull",
    "fullName": "torch.distributions.weibull.broadcast_all",
    "signature": "(*values)",
    "description": "Given a list of values (possibly containing numbers), returns a list where each"
  },
  "2840": {
    "name": "broadcast_all",
    "module": "torch.distributions.von_mises",
    "fullName": "torch.distributions.von_mises.broadcast_all",
    "signature": "(*values)",
    "description": "Given a list of values (possibly containing numbers), returns a list where each"
  },
  "2841": {
    "name": "broadcast_all",
    "module": "torch.distributions.utils",
    "fullName": "torch.distributions.utils.broadcast_all",
    "signature": "(*values)",
    "description": "Given a list of values (possibly containing numbers), returns a list where each"
  },
  "2842": {
    "name": "clamp_probs",
    "module": "torch.distributions.utils",
    "fullName": "torch.distributions.utils.clamp_probs",
    "signature": "(probs)",
    "description": "No description available."
  },
  "2843": {
    "name": "is_tensor_like",
    "module": "torch.distributions.utils",
    "fullName": "torch.distributions.utils.is_tensor_like",
    "signature": "(inp)",
    "description": "Returns ``True`` if the passed-in input is a Tensor-like."
  },
  "2844": {
    "name": "logits_to_probs",
    "module": "torch.distributions.utils",
    "fullName": "torch.distributions.utils.logits_to_probs",
    "signature": "(logits, is_binary=False)",
    "description": "Converts a tensor of logits into probabilities. Note that for the"
  },
  "2845": {
    "name": "probs_to_logits",
    "module": "torch.distributions.utils",
    "fullName": "torch.distributions.utils.probs_to_logits",
    "signature": "(probs, is_binary=False)",
    "description": "Converts a tensor of probabilities into logits. For the binary case,"
  },
  "2846": {
    "name": "tril_matrix_to_vec",
    "module": "torch.distributions.utils",
    "fullName": "torch.distributions.utils.tril_matrix_to_vec",
    "signature": "(mat, diag=0)",
    "description": "Convert a `D x D` matrix or a batch of matrices into a (batched) vector"
  },
  "2847": {
    "name": "update_wrapper",
    "module": "torch.distributions.utils",
    "fullName": "torch.distributions.utils.update_wrapper",
    "signature": "(wrapper, wrapped, assigned=('__module__', '__name__', '__qualname__', '__doc__', '__annotations__'), updated=('__dict__',))",
    "description": "Update a wrapper function to look like the wrapped function"
  },
  "2848": {
    "name": "vec_to_tril_matrix",
    "module": "torch.distributions.utils",
    "fullName": "torch.distributions.utils.vec_to_tril_matrix",
    "signature": "(vec, diag=0)",
    "description": "Convert a vector or a batch of vectors into a batched `D x D`"
  },
  "2849": {
    "name": "broadcast_all",
    "module": "torch.distributions.uniform",
    "fullName": "torch.distributions.uniform.broadcast_all",
    "signature": "(*values)",
    "description": "Given a list of values (possibly containing numbers), returns a list where each"
  },
  "2850": {
    "name": "broadcast_all",
    "module": "torch.distributions.transforms",
    "fullName": "torch.distributions.transforms.broadcast_all",
    "signature": "(*values)",
    "description": "Given a list of values (possibly containing numbers), returns a list where each"
  },
  "2851": {
    "name": "tril_matrix_to_vec",
    "module": "torch.distributions.transforms",
    "fullName": "torch.distributions.transforms.tril_matrix_to_vec",
    "signature": "(mat, diag=0)",
    "description": "Convert a `D x D` matrix or a batch of matrices into a (batched) vector"
  },
  "2852": {
    "name": "vec_to_tril_matrix",
    "module": "torch.distributions.transforms",
    "fullName": "torch.distributions.transforms.vec_to_tril_matrix",
    "signature": "(vec, diag=0)",
    "description": "Convert a vector or a batch of vectors into a batched `D x D`"
  },
  "2853": {
    "name": "broadcast_all",
    "module": "torch.distributions.studentT",
    "fullName": "torch.distributions.studentT.broadcast_all",
    "signature": "(*values)",
    "description": "Given a list of values (possibly containing numbers), returns a list where each"
  },
  "2854": {
    "name": "broadcast_all",
    "module": "torch.distributions.relaxed_categorical",
    "fullName": "torch.distributions.relaxed_categorical.broadcast_all",
    "signature": "(*values)",
    "description": "Given a list of values (possibly containing numbers), returns a list where each"
  },
  "2855": {
    "name": "clamp_probs",
    "module": "torch.distributions.relaxed_categorical",
    "fullName": "torch.distributions.relaxed_categorical.clamp_probs",
    "signature": "(probs)",
    "description": "No description available."
  },
  "2856": {
    "name": "broadcast_all",
    "module": "torch.distributions.relaxed_bernoulli",
    "fullName": "torch.distributions.relaxed_bernoulli.broadcast_all",
    "signature": "(*values)",
    "description": "Given a list of values (possibly containing numbers), returns a list where each"
  },
  "2857": {
    "name": "clamp_probs",
    "module": "torch.distributions.relaxed_bernoulli",
    "fullName": "torch.distributions.relaxed_bernoulli.clamp_probs",
    "signature": "(probs)",
    "description": "No description available."
  },
  "2858": {
    "name": "logits_to_probs",
    "module": "torch.distributions.relaxed_bernoulli",
    "fullName": "torch.distributions.relaxed_bernoulli.logits_to_probs",
    "signature": "(logits, is_binary=False)",
    "description": "Converts a tensor of logits into probabilities. Note that for the"
  },
  "2859": {
    "name": "probs_to_logits",
    "module": "torch.distributions.relaxed_bernoulli",
    "fullName": "torch.distributions.relaxed_bernoulli.probs_to_logits",
    "signature": "(probs, is_binary=False)",
    "description": "Converts a tensor of probabilities into logits. For the binary case,"
  },
  "2860": {
    "name": "broadcast_all",
    "module": "torch.distributions.poisson",
    "fullName": "torch.distributions.poisson.broadcast_all",
    "signature": "(*values)",
    "description": "Given a list of values (possibly containing numbers), returns a list where each"
  },
  "2861": {
    "name": "broadcast_all",
    "module": "torch.distributions.pareto",
    "fullName": "torch.distributions.pareto.broadcast_all",
    "signature": "(*values)",
    "description": "Given a list of values (possibly containing numbers), returns a list where each"
  },
  "2862": {
    "name": "broadcast_all",
    "module": "torch.distributions.normal",
    "fullName": "torch.distributions.normal.broadcast_all",
    "signature": "(*values)",
    "description": "Given a list of values (possibly containing numbers), returns a list where each"
  },
  "2863": {
    "name": "broadcast_all",
    "module": "torch.distributions.negative_binomial",
    "fullName": "torch.distributions.negative_binomial.broadcast_all",
    "signature": "(*values)",
    "description": "Given a list of values (possibly containing numbers), returns a list where each"
  },
  "2864": {
    "name": "logits_to_probs",
    "module": "torch.distributions.negative_binomial",
    "fullName": "torch.distributions.negative_binomial.logits_to_probs",
    "signature": "(logits, is_binary=False)",
    "description": "Converts a tensor of logits into probabilities. Note that for the"
  },
  "2865": {
    "name": "probs_to_logits",
    "module": "torch.distributions.negative_binomial",
    "fullName": "torch.distributions.negative_binomial.probs_to_logits",
    "signature": "(probs, is_binary=False)",
    "description": "Converts a tensor of probabilities into logits. For the binary case,"
  },
  "2866": {
    "name": "broadcast_all",
    "module": "torch.distributions.multinomial",
    "fullName": "torch.distributions.multinomial.broadcast_all",
    "signature": "(*values)",
    "description": "Given a list of values (possibly containing numbers), returns a list where each"
  },
  "2867": {
    "name": "broadcast_all",
    "module": "torch.distributions.lkj_cholesky",
    "fullName": "torch.distributions.lkj_cholesky.broadcast_all",
    "signature": "(*values)",
    "description": "Given a list of values (possibly containing numbers), returns a list where each"
  },
  "2868": {
    "name": "broadcast_all",
    "module": "torch.distributions.laplace",
    "fullName": "torch.distributions.laplace.broadcast_all",
    "signature": "(*values)",
    "description": "Given a list of values (possibly containing numbers), returns a list where each"
  },
  "2869": {
    "name": "broadcast_all",
    "module": "torch.distributions.kumaraswamy",
    "fullName": "torch.distributions.kumaraswamy.broadcast_all",
    "signature": "(*values)",
    "description": "Given a list of values (possibly containing numbers), returns a list where each"
  },
  "2870": {
    "name": "kl_divergence",
    "module": "torch.distributions.kl",
    "fullName": "torch.distributions.kl.kl_divergence",
    "signature": "(p, q)",
    "description": "Compute Kullback-Leibler divergence :math:`KL(p \\| q)` between two distributions."
  },
  "2871": {
    "name": "register_kl",
    "module": "torch.distributions.kl",
    "fullName": "torch.distributions.kl.register_kl",
    "signature": "(type_p, type_q)",
    "description": "Decorator to register a pairwise function with :meth:`kl_divergence`."
  },
  "2872": {
    "name": "total_ordering",
    "module": "torch.distributions.kl",
    "fullName": "torch.distributions.kl.total_ordering",
    "signature": "(cls)",
    "description": "Class decorator that fills in missing ordering methods"
  },
  "2873": {
    "name": "broadcast_all",
    "module": "torch.distributions.gumbel",
    "fullName": "torch.distributions.gumbel.broadcast_all",
    "signature": "(*values)",
    "description": "Given a list of values (possibly containing numbers), returns a list where each"
  },
  "2874": {
    "name": "broadcast_all",
    "module": "torch.distributions.geometric",
    "fullName": "torch.distributions.geometric.broadcast_all",
    "signature": "(*values)",
    "description": "Given a list of values (possibly containing numbers), returns a list where each"
  },
  "2875": {
    "name": "logits_to_probs",
    "module": "torch.distributions.geometric",
    "fullName": "torch.distributions.geometric.logits_to_probs",
    "signature": "(logits, is_binary=False)",
    "description": "Converts a tensor of logits into probabilities. Note that for the"
  },
  "2876": {
    "name": "probs_to_logits",
    "module": "torch.distributions.geometric",
    "fullName": "torch.distributions.geometric.probs_to_logits",
    "signature": "(probs, is_binary=False)",
    "description": "Converts a tensor of probabilities into logits. For the binary case,"
  },
  "2877": {
    "name": "broadcast_all",
    "module": "torch.distributions.gamma",
    "fullName": "torch.distributions.gamma.broadcast_all",
    "signature": "(*values)",
    "description": "Given a list of values (possibly containing numbers), returns a list where each"
  },
  "2878": {
    "name": "broadcast_all",
    "module": "torch.distributions.fishersnedecor",
    "fullName": "torch.distributions.fishersnedecor.broadcast_all",
    "signature": "(*values)",
    "description": "Given a list of values (possibly containing numbers), returns a list where each"
  },
  "2879": {
    "name": "broadcast_all",
    "module": "torch.distributions.exponential",
    "fullName": "torch.distributions.exponential.broadcast_all",
    "signature": "(*values)",
    "description": "Given a list of values (possibly containing numbers), returns a list where each"
  },
  "2880": {
    "name": "once_differentiable",
    "module": "torch.distributions.dirichlet",
    "fullName": "torch.distributions.dirichlet.once_differentiable",
    "signature": "(fn)",
    "description": "No description available."
  },
  "2881": {
    "name": "broadcast_all",
    "module": "torch.distributions.continuous_bernoulli",
    "fullName": "torch.distributions.continuous_bernoulli.broadcast_all",
    "signature": "(*values)",
    "description": "Given a list of values (possibly containing numbers), returns a list where each"
  },
  "2882": {
    "name": "clamp_probs",
    "module": "torch.distributions.continuous_bernoulli",
    "fullName": "torch.distributions.continuous_bernoulli.clamp_probs",
    "signature": "(probs)",
    "description": "No description available."
  },
  "2883": {
    "name": "logits_to_probs",
    "module": "torch.distributions.continuous_bernoulli",
    "fullName": "torch.distributions.continuous_bernoulli.logits_to_probs",
    "signature": "(logits, is_binary=False)",
    "description": "Converts a tensor of logits into probabilities. Note that for the"
  },
  "2884": {
    "name": "probs_to_logits",
    "module": "torch.distributions.continuous_bernoulli",
    "fullName": "torch.distributions.continuous_bernoulli.probs_to_logits",
    "signature": "(probs, is_binary=False)",
    "description": "Converts a tensor of probabilities into logits. For the binary case,"
  },
  "2885": {
    "name": "broadcast_all",
    "module": "torch.distributions.cauchy",
    "fullName": "torch.distributions.cauchy.broadcast_all",
    "signature": "(*values)",
    "description": "Given a list of values (possibly containing numbers), returns a list where each"
  },
  "2886": {
    "name": "logits_to_probs",
    "module": "torch.distributions.categorical",
    "fullName": "torch.distributions.categorical.logits_to_probs",
    "signature": "(logits, is_binary=False)",
    "description": "Converts a tensor of logits into probabilities. Note that for the"
  },
  "2887": {
    "name": "probs_to_logits",
    "module": "torch.distributions.categorical",
    "fullName": "torch.distributions.categorical.probs_to_logits",
    "signature": "(probs, is_binary=False)",
    "description": "Converts a tensor of probabilities into logits. For the binary case,"
  },
  "2888": {
    "name": "broadcast_all",
    "module": "torch.distributions.binomial",
    "fullName": "torch.distributions.binomial.broadcast_all",
    "signature": "(*values)",
    "description": "Given a list of values (possibly containing numbers), returns a list where each"
  },
  "2889": {
    "name": "logits_to_probs",
    "module": "torch.distributions.binomial",
    "fullName": "torch.distributions.binomial.logits_to_probs",
    "signature": "(logits, is_binary=False)",
    "description": "Converts a tensor of logits into probabilities. Note that for the"
  },
  "2890": {
    "name": "probs_to_logits",
    "module": "torch.distributions.binomial",
    "fullName": "torch.distributions.binomial.probs_to_logits",
    "signature": "(probs, is_binary=False)",
    "description": "Converts a tensor of probabilities into logits. For the binary case,"
  },
  "2891": {
    "name": "broadcast_all",
    "module": "torch.distributions.beta",
    "fullName": "torch.distributions.beta.broadcast_all",
    "signature": "(*values)",
    "description": "Given a list of values (possibly containing numbers), returns a list where each"
  },
  "2892": {
    "name": "broadcast_all",
    "module": "torch.distributions.bernoulli",
    "fullName": "torch.distributions.bernoulli.broadcast_all",
    "signature": "(*values)",
    "description": "Given a list of values (possibly containing numbers), returns a list where each"
  },
  "2893": {
    "name": "logits_to_probs",
    "module": "torch.distributions.bernoulli",
    "fullName": "torch.distributions.bernoulli.logits_to_probs",
    "signature": "(logits, is_binary=False)",
    "description": "Converts a tensor of logits into probabilities. Note that for the"
  },
  "2894": {
    "name": "probs_to_logits",
    "module": "torch.distributions.bernoulli",
    "fullName": "torch.distributions.bernoulli.probs_to_logits",
    "signature": "(probs, is_binary=False)",
    "description": "Converts a tensor of probabilities into logits. For the binary case,"
  },
  "2895": {
    "name": "Device",
    "module": "torch.cuda",
    "fullName": "torch.cuda.Device",
    "signature": "(*args, **kwargs)",
    "description": "No description available."
  },
  "2896": {
    "name": "caching_allocator_alloc",
    "module": "torch.cuda",
    "fullName": "torch.cuda.caching_allocator_alloc",
    "signature": "(size, device: Union[torch.device, str, NoneType, int] = None, stream=None)",
    "description": "Performs a memory allocation using the CUDA memory allocator."
  },
  "2897": {
    "name": "caching_allocator_delete",
    "module": "torch.cuda",
    "fullName": "torch.cuda.caching_allocator_delete",
    "signature": "(mem_ptr)",
    "description": "Deletes memory allocated using the CUDA memory allocator."
  },
  "2898": {
    "name": "can_device_access_peer",
    "module": "torch.cuda",
    "fullName": "torch.cuda.can_device_access_peer",
    "signature": "(device: Union[torch.device, str, int, NoneType], peer_device: Union[torch.device, str, int, NoneType]) -> bool",
    "description": "Checks if peer access between two devices is possible."
  },
  "2899": {
    "name": "check_error",
    "module": "torch.cuda",
    "fullName": "torch.cuda.check_error",
    "signature": "(res: int) -> None",
    "description": "No description available."
  },
  "2900": {
    "name": "classproperty",
    "module": "torch.cuda",
    "fullName": "torch.cuda.classproperty",
    "signature": "(func)",
    "description": "No description available."
  },
  "2901": {
    "name": "cudart",
    "module": "torch.cuda",
    "fullName": "torch.cuda.cudart",
    "signature": "()",
    "description": "No description available."
  },
  "2902": {
    "name": "current_blas_handle",
    "module": "torch.cuda",
    "fullName": "torch.cuda.current_blas_handle",
    "signature": "()",
    "description": "Returns cublasHandle_t pointer to current cuBLAS handle"
  },
  "2903": {
    "name": "current_device",
    "module": "torch.cuda",
    "fullName": "torch.cuda.current_device",
    "signature": "() -> int",
    "description": "Returns the index of a currently selected device."
  },
  "2904": {
    "name": "current_stream",
    "module": "torch.cuda",
    "fullName": "torch.cuda.current_stream",
    "signature": "(device: Union[torch.device, str, int, NoneType] = None) -> torch.cuda.streams.Stream",
    "description": "Returns the currently selected :class:`Stream` for a given device."
  },
  "2905": {
    "name": "default_stream",
    "module": "torch.cuda",
    "fullName": "torch.cuda.default_stream",
    "signature": "(device: Union[torch.device, str, int, NoneType] = None) -> torch.cuda.streams.Stream",
    "description": "Returns the default :class:`Stream` for a given device."
  },
  "2906": {
    "name": "device_count",
    "module": "torch.cuda",
    "fullName": "torch.cuda.device_count",
    "signature": "() -> int",
    "description": "Returns the number of GPUs available."
  },
  "2907": {
    "name": "empty_cache",
    "module": "torch.cuda",
    "fullName": "torch.cuda.empty_cache",
    "signature": "() -> None",
    "description": "Releases all unoccupied cached memory currently held by the caching"
  },
  "2908": {
    "name": "get_arch_list",
    "module": "torch.cuda",
    "fullName": "torch.cuda.get_arch_list",
    "signature": "() -> List[str]",
    "description": "Returns list CUDA architectures this library was compiled for."
  },
  "2909": {
    "name": "get_device_capability",
    "module": "torch.cuda",
    "fullName": "torch.cuda.get_device_capability",
    "signature": "(device: Union[torch.device, str, int, NoneType] = None) -> Tuple[int, int]",
    "description": "Gets the cuda capability of a device."
  },
  "2910": {
    "name": "get_device_name",
    "module": "torch.cuda",
    "fullName": "torch.cuda.get_device_name",
    "signature": "(device: Union[torch.device, str, int, NoneType] = None) -> str",
    "description": "Gets the name of a device."
  },
  "2911": {
    "name": "get_device_properties",
    "module": "torch.cuda",
    "fullName": "torch.cuda.get_device_properties",
    "signature": "(device: Union[torch.device, str, int, NoneType]) -> torch._C._CudaDeviceProperties",
    "description": "Gets the properties of a device."
  },
  "2912": {
    "name": "get_gencode_flags",
    "module": "torch.cuda",
    "fullName": "torch.cuda.get_gencode_flags",
    "signature": "() -> str",
    "description": "Returns NVCC gencode flags this library was compiled with."
  },
  "2913": {
    "name": "get_rng_state",
    "module": "torch.cuda",
    "fullName": "torch.cuda.get_rng_state",
    "signature": "(device: Union[int, str, torch.device] = 'cuda') -> torch.Tensor",
    "description": "Returns the random number generator state of the specified GPU as a ByteTensor."
  },
  "2914": {
    "name": "get_rng_state_all",
    "module": "torch.cuda",
    "fullName": "torch.cuda.get_rng_state_all",
    "signature": "() -> List[torch.Tensor]",
    "description": "Returns a list of ByteTensor representing the random number states of all devices."
  },
  "2915": {
    "name": "get_sync_debug_mode",
    "module": "torch.cuda",
    "fullName": "torch.cuda.get_sync_debug_mode",
    "signature": "() -> int",
    "description": "Returns current value of debug mode for cuda synchronizing operations."
  },
  "2916": {
    "name": "graph_pool_handle",
    "module": "torch.cuda",
    "fullName": "torch.cuda.graph_pool_handle",
    "signature": "()",
    "description": "Returns an opaque token representing the id of a graph memory pool."
  },
  "2917": {
    "name": "init",
    "module": "torch.cuda",
    "fullName": "torch.cuda.init",
    "signature": "()",
    "description": "Initialize PyTorch's CUDA state.  You may need to call"
  },
  "2918": {
    "name": "initial_seed",
    "module": "torch.cuda",
    "fullName": "torch.cuda.initial_seed",
    "signature": "() -> int",
    "description": "Returns the current random seed of the current GPU."
  },
  "2919": {
    "name": "ipc_collect",
    "module": "torch.cuda",
    "fullName": "torch.cuda.ipc_collect",
    "signature": "()",
    "description": "Force collects GPU memory after it has been released by CUDA IPC."
  },
  "2920": {
    "name": "is_available",
    "module": "torch.cuda",
    "fullName": "torch.cuda.is_available",
    "signature": "() -> bool",
    "description": "Returns a bool indicating if CUDA is currently available."
  },
  "2921": {
    "name": "is_bf16_supported",
    "module": "torch.cuda",
    "fullName": "torch.cuda.is_bf16_supported",
    "signature": "()",
    "description": "Returns a bool indicating if the current CUDA device supports dtype bfloat16"
  },
  "2922": {
    "name": "is_current_stream_capturing",
    "module": "torch.cuda",
    "fullName": "torch.cuda.is_current_stream_capturing",
    "signature": "()",
    "description": "Returns True if CUDA graph capture is underway on the current CUDA stream, False otherwise."
  },
  "2923": {
    "name": "is_initialized",
    "module": "torch.cuda",
    "fullName": "torch.cuda.is_initialized",
    "signature": "()",
    "description": "Returns whether PyTorch's CUDA state has been initialized."
  },
  "2924": {
    "name": "list_gpu_processes",
    "module": "torch.cuda",
    "fullName": "torch.cuda.list_gpu_processes",
    "signature": "(device: Union[torch.device, str, NoneType, int] = None) -> str",
    "description": "Returns a human-readable printout of the running processes"
  },
  "2925": {
    "name": "make_graphed_callables",
    "module": "torch.cuda",
    "fullName": "torch.cuda.make_graphed_callables",
    "signature": "(callables, sample_args)",
    "description": "Accepts callables (functions or :class:`nn.Module<torch.nn.Module>`\\ s)"
  },
  "2926": {
    "name": "manual_seed",
    "module": "torch.cuda",
    "fullName": "torch.cuda.manual_seed",
    "signature": "(seed: int) -> None",
    "description": "Sets the seed for generating random numbers for the current GPU."
  },
  "2927": {
    "name": "manual_seed_all",
    "module": "torch.cuda",
    "fullName": "torch.cuda.manual_seed_all",
    "signature": "(seed: int) -> None",
    "description": "Sets the seed for generating random numbers on all GPUs."
  },
  "2928": {
    "name": "max_memory_allocated",
    "module": "torch.cuda",
    "fullName": "torch.cuda.max_memory_allocated",
    "signature": "(device: Union[torch.device, str, NoneType, int] = None) -> int",
    "description": "Returns the maximum GPU memory occupied by tensors in bytes for a given"
  },
  "2929": {
    "name": "max_memory_reserved",
    "module": "torch.cuda",
    "fullName": "torch.cuda.max_memory_reserved",
    "signature": "(device: Union[torch.device, str, NoneType, int] = None) -> int",
    "description": "Returns the maximum GPU memory managed by the caching allocator in bytes"
  },
  "2930": {
    "name": "mem_get_info",
    "module": "torch.cuda",
    "fullName": "torch.cuda.mem_get_info",
    "signature": "(device: Union[torch.device, str, NoneType, int] = None) -> int",
    "description": "Returns the global free and total GPU memory occupied for a given"
  },
  "2931": {
    "name": "memory_allocated",
    "module": "torch.cuda",
    "fullName": "torch.cuda.memory_allocated",
    "signature": "(device: Union[torch.device, str, NoneType, int] = None) -> int",
    "description": "Returns the current GPU memory occupied by tensors in bytes for a given"
  },
  "2932": {
    "name": "memory_reserved",
    "module": "torch.cuda",
    "fullName": "torch.cuda.memory_reserved",
    "signature": "(device: Union[torch.device, str, NoneType, int] = None) -> int",
    "description": "Returns the current GPU memory managed by the caching allocator in bytes"
  },
  "2933": {
    "name": "memory_snapshot",
    "module": "torch.cuda",
    "fullName": "torch.cuda.memory_snapshot",
    "signature": "()",
    "description": "Returns a snapshot of the CUDA memory allocator state across all devices."
  },
  "2934": {
    "name": "memory_stats",
    "module": "torch.cuda",
    "fullName": "torch.cuda.memory_stats",
    "signature": "(device: Union[torch.device, str, NoneType, int] = None) -> Dict[str, Any]",
    "description": "Returns a dictionary of CUDA memory allocator statistics for a"
  },
  "2935": {
    "name": "memory_stats_as_nested_dict",
    "module": "torch.cuda",
    "fullName": "torch.cuda.memory_stats_as_nested_dict",
    "signature": "(device: Union[torch.device, str, NoneType, int] = None) -> Dict[str, Any]",
    "description": "Returns the result of :func:`~torch.cuda.memory_stats` as a nested dictionary."
  },
  "2936": {
    "name": "memory_summary",
    "module": "torch.cuda",
    "fullName": "torch.cuda.memory_summary",
    "signature": "(device: Union[torch.device, str, NoneType, int] = None, abbreviated: bool = False) -> str",
    "description": "Returns a human-readable printout of the current memory allocator"
  },
  "2937": {
    "name": "memory_usage",
    "module": "torch.cuda",
    "fullName": "torch.cuda.memory_usage",
    "signature": "(device: Union[torch.device, str, int, NoneType] = None) -> int",
    "description": "Returns the percent of time over the past sample period during which global (device)"
  },
  "2938": {
    "name": "reset_accumulated_memory_stats",
    "module": "torch.cuda",
    "fullName": "torch.cuda.reset_accumulated_memory_stats",
    "signature": "(device: Union[torch.device, str, NoneType, int] = None) -> None",
    "description": "Resets the \"accumulated\" (historical) stats tracked by the CUDA memory allocator."
  },
  "2939": {
    "name": "reset_max_memory_allocated",
    "module": "torch.cuda",
    "fullName": "torch.cuda.reset_max_memory_allocated",
    "signature": "(device: Union[torch.device, str, NoneType, int] = None) -> None",
    "description": "Resets the starting point in tracking maximum GPU memory occupied by"
  },
  "2940": {
    "name": "reset_max_memory_cached",
    "module": "torch.cuda",
    "fullName": "torch.cuda.reset_max_memory_cached",
    "signature": "(device: Union[torch.device, str, NoneType, int] = None) -> None",
    "description": "Resets the starting point in tracking maximum GPU memory managed by the"
  },
  "2941": {
    "name": "reset_peak_memory_stats",
    "module": "torch.cuda",
    "fullName": "torch.cuda.reset_peak_memory_stats",
    "signature": "(device: Union[torch.device, str, NoneType, int] = None) -> None",
    "description": "Resets the \"peak\" stats tracked by the CUDA memory allocator."
  },
  "2942": {
    "name": "seed",
    "module": "torch.cuda",
    "fullName": "torch.cuda.seed",
    "signature": "() -> None",
    "description": "Sets the seed for generating random numbers to a random number for the current GPU."
  },
  "2943": {
    "name": "seed_all",
    "module": "torch.cuda",
    "fullName": "torch.cuda.seed_all",
    "signature": "() -> None",
    "description": "Sets the seed for generating random numbers to a random number on all GPUs."
  },
  "2944": {
    "name": "set_device",
    "module": "torch.cuda",
    "fullName": "torch.cuda.set_device",
    "signature": "(device: Union[torch.device, str, int, NoneType]) -> None",
    "description": "Sets the current device."
  },
  "2945": {
    "name": "set_per_process_memory_fraction",
    "module": "torch.cuda",
    "fullName": "torch.cuda.set_per_process_memory_fraction",
    "signature": "(fraction, device: Union[torch.device, str, NoneType, int] = None) -> None",
    "description": "Set memory fraction for a process."
  },
  "2946": {
    "name": "set_rng_state",
    "module": "torch.cuda",
    "fullName": "torch.cuda.set_rng_state",
    "signature": "(new_state: torch.Tensor, device: Union[int, str, torch.device] = 'cuda') -> None",
    "description": "Sets the random number generator state of the specified GPU."
  },
  "2947": {
    "name": "set_rng_state_all",
    "module": "torch.cuda",
    "fullName": "torch.cuda.set_rng_state_all",
    "signature": "(new_states: Iterable[torch.Tensor]) -> None",
    "description": "Sets the random number generator state of all devices."
  },
  "2948": {
    "name": "set_stream",
    "module": "torch.cuda",
    "fullName": "torch.cuda.set_stream",
    "signature": "(stream: torch.cuda.streams.Stream)",
    "description": "Sets the current stream.This is a wrapper API to set the stream."
  },
  "2949": {
    "name": "stream",
    "module": "torch.cuda",
    "fullName": "torch.cuda.stream",
    "signature": "(stream: Optional[ForwardRef('torch.cuda.Stream')]) -> torch.cuda.StreamContext",
    "description": "Wrapper around the Context-manager StreamContext that"
  },
  "2950": {
    "name": "synchronize",
    "module": "torch.cuda",
    "fullName": "torch.cuda.synchronize",
    "signature": "(device: Union[torch.device, str, int, NoneType] = None) -> None",
    "description": "Waits for all kernels in all streams on a CUDA device to complete."
  },
  "2951": {
    "name": "utilization",
    "module": "torch.cuda",
    "fullName": "torch.cuda.utilization",
    "signature": "(device: Union[torch.device, str, int, NoneType] = None) -> int",
    "description": "Returns the percent of time over the past sample period during which one or"
  },
  "2952": {
    "name": "cast",
    "module": "torch.cuda.random",
    "fullName": "torch.cuda.random.cast",
    "signature": "(typ, val)",
    "description": "Cast a value to a type."
  },
  "2953": {
    "name": "current_device",
    "module": "torch.cuda.random",
    "fullName": "torch.cuda.random.current_device",
    "signature": "() -> int",
    "description": "Returns the index of a currently selected device."
  },
  "2954": {
    "name": "device_count",
    "module": "torch.cuda.random",
    "fullName": "torch.cuda.random.device_count",
    "signature": "() -> int",
    "description": "Returns the number of GPUs available."
  },
  "2955": {
    "name": "get_rng_state",
    "module": "torch.cuda.random",
    "fullName": "torch.cuda.random.get_rng_state",
    "signature": "(device: Union[int, str, torch.device] = 'cuda') -> torch.Tensor",
    "description": "Returns the random number generator state of the specified GPU as a ByteTensor."
  },
  "2956": {
    "name": "get_rng_state_all",
    "module": "torch.cuda.random",
    "fullName": "torch.cuda.random.get_rng_state_all",
    "signature": "() -> List[torch.Tensor]",
    "description": "Returns a list of ByteTensor representing the random number states of all devices."
  },
  "2957": {
    "name": "initial_seed",
    "module": "torch.cuda.random",
    "fullName": "torch.cuda.random.initial_seed",
    "signature": "() -> int",
    "description": "Returns the current random seed of the current GPU."
  },
  "2958": {
    "name": "manual_seed",
    "module": "torch.cuda.random",
    "fullName": "torch.cuda.random.manual_seed",
    "signature": "(seed: int) -> None",
    "description": "Sets the seed for generating random numbers for the current GPU."
  },
  "2959": {
    "name": "manual_seed_all",
    "module": "torch.cuda.random",
    "fullName": "torch.cuda.random.manual_seed_all",
    "signature": "(seed: int) -> None",
    "description": "Sets the seed for generating random numbers on all GPUs."
  },
  "2960": {
    "name": "seed",
    "module": "torch.cuda.random",
    "fullName": "torch.cuda.random.seed",
    "signature": "() -> None",
    "description": "Sets the seed for generating random numbers to a random number for the current GPU."
  },
  "2961": {
    "name": "seed_all",
    "module": "torch.cuda.random",
    "fullName": "torch.cuda.random.seed_all",
    "signature": "() -> None",
    "description": "Sets the seed for generating random numbers to a random number on all GPUs."
  },
  "2962": {
    "name": "set_rng_state",
    "module": "torch.cuda.random",
    "fullName": "torch.cuda.random.set_rng_state",
    "signature": "(new_state: torch.Tensor, device: Union[int, str, torch.device] = 'cuda') -> None",
    "description": "Sets the random number generator state of the specified GPU."
  },
  "2963": {
    "name": "set_rng_state_all",
    "module": "torch.cuda.random",
    "fullName": "torch.cuda.random.set_rng_state_all",
    "signature": "(new_states: Iterable[torch.Tensor]) -> None",
    "description": "Sets the random number generator state of all devices."
  },
  "2964": {
    "name": "check_error",
    "module": "torch.cuda.profiler",
    "fullName": "torch.cuda.profiler.check_error",
    "signature": "(res: int) -> None",
    "description": "No description available."
  },
  "2965": {
    "name": "cudart",
    "module": "torch.cuda.profiler",
    "fullName": "torch.cuda.profiler.cudart",
    "signature": "()",
    "description": "No description available."
  },
  "2966": {
    "name": "init",
    "module": "torch.cuda.profiler",
    "fullName": "torch.cuda.profiler.init",
    "signature": "(output_file, flags=None, output_mode='key_value')",
    "description": "No description available."
  },
  "2967": {
    "name": "profile",
    "module": "torch.cuda.profiler",
    "fullName": "torch.cuda.profiler.profile",
    "signature": "()",
    "description": "No description available."
  },
  "2968": {
    "name": "start",
    "module": "torch.cuda.profiler",
    "fullName": "torch.cuda.profiler.start",
    "signature": "()",
    "description": "No description available."
  },
  "2969": {
    "name": "stop",
    "module": "torch.cuda.profiler",
    "fullName": "torch.cuda.profiler.stop",
    "signature": "()",
    "description": "No description available."
  },
  "2970": {
    "name": "contextmanager",
    "module": "torch.cuda.nvtx",
    "fullName": "torch.cuda.nvtx.contextmanager",
    "signature": "(func)",
    "description": "@contextmanager decorator."
  },
  "2971": {
    "name": "mark",
    "module": "torch.cuda.nvtx",
    "fullName": "torch.cuda.nvtx.mark",
    "signature": "(msg)",
    "description": "Describe an instantaneous event that occurred at some point."
  },
  "2972": {
    "name": "range",
    "module": "torch.cuda.nvtx",
    "fullName": "torch.cuda.nvtx.range",
    "signature": "(msg, *args, **kwargs)",
    "description": "Context manager / decorator that pushes an NVTX range at the beginning"
  },
  "2973": {
    "name": "range_end",
    "module": "torch.cuda.nvtx",
    "fullName": "torch.cuda.nvtx.range_end",
    "signature": "(range_id) -> None",
    "description": "Mark the end of a range for a given range_id."
  },
  "2974": {
    "name": "range_pop",
    "module": "torch.cuda.nvtx",
    "fullName": "torch.cuda.nvtx.range_pop",
    "signature": "()",
    "description": "Pops a range off of a stack of nested range spans.  Returns the"
  },
  "2975": {
    "name": "range_push",
    "module": "torch.cuda.nvtx",
    "fullName": "torch.cuda.nvtx.range_push",
    "signature": "(msg)",
    "description": "Pushes a range onto a stack of nested range span.  Returns zero-based"
  },
  "2976": {
    "name": "range_start",
    "module": "torch.cuda.nvtx",
    "fullName": "torch.cuda.nvtx.range_start",
    "signature": "(msg) -> int",
    "description": "Mark the start of a range with string message. It returns an unique handle"
  },
  "2977": {
    "name": "Device",
    "module": "torch.cuda.memory",
    "fullName": "torch.cuda.memory.Device",
    "signature": "(*args, **kwargs)",
    "description": "No description available."
  },
  "2978": {
    "name": "caching_allocator_alloc",
    "module": "torch.cuda.memory",
    "fullName": "torch.cuda.memory.caching_allocator_alloc",
    "signature": "(size, device: Union[torch.device, str, NoneType, int] = None, stream=None)",
    "description": "Performs a memory allocation using the CUDA memory allocator."
  },
  "2979": {
    "name": "caching_allocator_delete",
    "module": "torch.cuda.memory",
    "fullName": "torch.cuda.memory.caching_allocator_delete",
    "signature": "(mem_ptr)",
    "description": "Deletes memory allocated using the CUDA memory allocator."
  },
  "2980": {
    "name": "empty_cache",
    "module": "torch.cuda.memory",
    "fullName": "torch.cuda.memory.empty_cache",
    "signature": "() -> None",
    "description": "Releases all unoccupied cached memory currently held by the caching"
  },
  "2981": {
    "name": "is_initialized",
    "module": "torch.cuda.memory",
    "fullName": "torch.cuda.memory.is_initialized",
    "signature": "()",
    "description": "Returns whether PyTorch's CUDA state has been initialized."
  },
  "2982": {
    "name": "list_gpu_processes",
    "module": "torch.cuda.memory",
    "fullName": "torch.cuda.memory.list_gpu_processes",
    "signature": "(device: Union[torch.device, str, NoneType, int] = None) -> str",
    "description": "Returns a human-readable printout of the running processes"
  },
  "2983": {
    "name": "max_memory_allocated",
    "module": "torch.cuda.memory",
    "fullName": "torch.cuda.memory.max_memory_allocated",
    "signature": "(device: Union[torch.device, str, NoneType, int] = None) -> int",
    "description": "Returns the maximum GPU memory occupied by tensors in bytes for a given"
  },
  "2984": {
    "name": "max_memory_reserved",
    "module": "torch.cuda.memory",
    "fullName": "torch.cuda.memory.max_memory_reserved",
    "signature": "(device: Union[torch.device, str, NoneType, int] = None) -> int",
    "description": "Returns the maximum GPU memory managed by the caching allocator in bytes"
  },
  "2985": {
    "name": "mem_get_info",
    "module": "torch.cuda.memory",
    "fullName": "torch.cuda.memory.mem_get_info",
    "signature": "(device: Union[torch.device, str, NoneType, int] = None) -> int",
    "description": "Returns the global free and total GPU memory occupied for a given"
  },
  "2986": {
    "name": "memory_allocated",
    "module": "torch.cuda.memory",
    "fullName": "torch.cuda.memory.memory_allocated",
    "signature": "(device: Union[torch.device, str, NoneType, int] = None) -> int",
    "description": "Returns the current GPU memory occupied by tensors in bytes for a given"
  },
  "2987": {
    "name": "memory_reserved",
    "module": "torch.cuda.memory",
    "fullName": "torch.cuda.memory.memory_reserved",
    "signature": "(device: Union[torch.device, str, NoneType, int] = None) -> int",
    "description": "Returns the current GPU memory managed by the caching allocator in bytes"
  },
  "2988": {
    "name": "memory_snapshot",
    "module": "torch.cuda.memory",
    "fullName": "torch.cuda.memory.memory_snapshot",
    "signature": "()",
    "description": "Returns a snapshot of the CUDA memory allocator state across all devices."
  },
  "2989": {
    "name": "memory_stats",
    "module": "torch.cuda.memory",
    "fullName": "torch.cuda.memory.memory_stats",
    "signature": "(device: Union[torch.device, str, NoneType, int] = None) -> Dict[str, Any]",
    "description": "Returns a dictionary of CUDA memory allocator statistics for a"
  },
  "2990": {
    "name": "memory_stats_as_nested_dict",
    "module": "torch.cuda.memory",
    "fullName": "torch.cuda.memory.memory_stats_as_nested_dict",
    "signature": "(device: Union[torch.device, str, NoneType, int] = None) -> Dict[str, Any]",
    "description": "Returns the result of :func:`~torch.cuda.memory_stats` as a nested dictionary."
  },
  "2991": {
    "name": "memory_summary",
    "module": "torch.cuda.memory",
    "fullName": "torch.cuda.memory.memory_summary",
    "signature": "(device: Union[torch.device, str, NoneType, int] = None, abbreviated: bool = False) -> str",
    "description": "Returns a human-readable printout of the current memory allocator"
  },
  "2992": {
    "name": "reset_accumulated_memory_stats",
    "module": "torch.cuda.memory",
    "fullName": "torch.cuda.memory.reset_accumulated_memory_stats",
    "signature": "(device: Union[torch.device, str, NoneType, int] = None) -> None",
    "description": "Resets the \"accumulated\" (historical) stats tracked by the CUDA memory allocator."
  },
  "2993": {
    "name": "reset_max_memory_allocated",
    "module": "torch.cuda.memory",
    "fullName": "torch.cuda.memory.reset_max_memory_allocated",
    "signature": "(device: Union[torch.device, str, NoneType, int] = None) -> None",
    "description": "Resets the starting point in tracking maximum GPU memory occupied by"
  },
  "2994": {
    "name": "reset_max_memory_cached",
    "module": "torch.cuda.memory",
    "fullName": "torch.cuda.memory.reset_max_memory_cached",
    "signature": "(device: Union[torch.device, str, NoneType, int] = None) -> None",
    "description": "Resets the starting point in tracking maximum GPU memory managed by the"
  },
  "2995": {
    "name": "reset_peak_memory_stats",
    "module": "torch.cuda.memory",
    "fullName": "torch.cuda.memory.reset_peak_memory_stats",
    "signature": "(device: Union[torch.device, str, NoneType, int] = None) -> None",
    "description": "Resets the \"peak\" stats tracked by the CUDA memory allocator."
  },
  "2996": {
    "name": "set_per_process_memory_fraction",
    "module": "torch.cuda.memory",
    "fullName": "torch.cuda.memory.set_per_process_memory_fraction",
    "signature": "(fraction, device: Union[torch.device, str, NoneType, int] = None) -> None",
    "description": "Set memory fraction for a process."
  },
  "2997": {
    "name": "graph_pool_handle",
    "module": "torch.cuda.graphs",
    "fullName": "torch.cuda.graphs.graph_pool_handle",
    "signature": "()",
    "description": "Returns an opaque token representing the id of a graph memory pool."
  },
  "2998": {
    "name": "is_current_stream_capturing",
    "module": "torch.cuda.graphs",
    "fullName": "torch.cuda.graphs.is_current_stream_capturing",
    "signature": "()",
    "description": "Returns True if CUDA graph capture is underway on the current CUDA stream, False otherwise."
  },
  "2999": {
    "name": "make_graphed_callables",
    "module": "torch.cuda.graphs",
    "fullName": "torch.cuda.graphs.make_graphed_callables",
    "signature": "(callables, sample_args)",
    "description": "Accepts callables (functions or :class:`nn.Module<torch.nn.Module>`\\ s)"
  },
  "3000": {
    "name": "custom_bwd",
    "module": "torch.cuda.amp",
    "fullName": "torch.cuda.amp.custom_bwd",
    "signature": "(bwd)",
    "description": "Helper decorator for backward methods of custom autograd functions (subclasses of"
  },
  "3001": {
    "name": "custom_fwd",
    "module": "torch.cuda.amp",
    "fullName": "torch.cuda.amp.custom_fwd",
    "signature": "(fwd=None, **kwargs)",
    "description": "Helper decorator for ``forward`` methods of custom autograd functions (subclasses of"
  },
  "3002": {
    "name": "amp_definitely_not_available",
    "module": "torch.cuda.amp.grad_scaler",
    "fullName": "torch.cuda.amp.grad_scaler.amp_definitely_not_available",
    "signature": "()",
    "description": "No description available."
  },
  "3003": {
    "name": "amp_definitely_not_available",
    "module": "torch.cuda.amp.common",
    "fullName": "torch.cuda.amp.common.amp_definitely_not_available",
    "signature": "()",
    "description": "No description available."
  },
  "3004": {
    "name": "find_spec",
    "module": "torch.cuda.amp.common",
    "fullName": "torch.cuda.amp.common.find_spec",
    "signature": "(name, package=None)",
    "description": "Return the spec for the specified module."
  },
  "3005": {
    "name": "custom_bwd",
    "module": "torch.cuda.amp.autocast_mode",
    "fullName": "torch.cuda.amp.autocast_mode.custom_bwd",
    "signature": "(bwd)",
    "description": "Helper decorator for backward methods of custom autograd functions (subclasses of"
  },
  "3006": {
    "name": "custom_fwd",
    "module": "torch.cuda.amp.autocast_mode",
    "fullName": "torch.cuda.amp.autocast_mode.custom_fwd",
    "signature": "(fwd=None, **kwargs)",
    "description": "Helper decorator for ``forward`` methods of custom autograd functions (subclasses of"
  },
  "3007": {
    "name": "contextmanager",
    "module": "torch.backends",
    "fullName": "torch.backends.contextmanager",
    "signature": "(func)",
    "description": "@contextmanager decorator."
  },
  "3008": {
    "name": "disable_global_flags",
    "module": "torch.backends",
    "fullName": "torch.backends.disable_global_flags",
    "signature": "()",
    "description": "No description available."
  },
  "3009": {
    "name": "flags_frozen",
    "module": "torch.backends",
    "fullName": "torch.backends.flags_frozen",
    "signature": "()",
    "description": "No description available."
  },
  "3010": {
    "name": "is_available",
    "module": "torch.backends.openmp",
    "fullName": "torch.backends.openmp.is_available",
    "signature": "()",
    "description": "Returns whether PyTorch is built with OpenMP support."
  },
  "3011": {
    "name": "is_available",
    "module": "torch.backends.mps",
    "fullName": "torch.backends.mps.is_available",
    "signature": "() -> bool",
    "description": "Returns a bool indicating if MPS is currently available."
  },
  "3012": {
    "name": "is_built",
    "module": "torch.backends.mps",
    "fullName": "torch.backends.mps.is_built",
    "signature": "() -> bool",
    "description": "Returns whether PyTorch is built with MPS support. Note that this"
  },
  "3013": {
    "name": "contextmanager",
    "module": "torch.backends.mkldnn.m",
    "fullName": "torch.backends.mkldnn.m.contextmanager",
    "signature": "(func)",
    "description": "@contextmanager decorator."
  },
  "3014": {
    "name": "flags",
    "module": "torch.backends.mkldnn.m",
    "fullName": "torch.backends.mkldnn.m.flags",
    "signature": "(enabled=False)",
    "description": "No description available."
  },
  "3015": {
    "name": "is_available",
    "module": "torch.backends.mkldnn.m",
    "fullName": "torch.backends.mkldnn.m.is_available",
    "signature": "()",
    "description": "Returns whether PyTorch is built with MKL-DNN support."
  },
  "3016": {
    "name": "set_flags",
    "module": "torch.backends.mkldnn.m",
    "fullName": "torch.backends.mkldnn.m.set_flags",
    "signature": "(_enabled)",
    "description": "No description available."
  },
  "3017": {
    "name": "is_available",
    "module": "torch.backends.mkl",
    "fullName": "torch.backends.mkl.is_available",
    "signature": "()",
    "description": "Returns whether PyTorch is built with MKL support."
  },
  "3018": {
    "name": "contextmanager",
    "module": "torch.backends.cudnn.m",
    "fullName": "torch.backends.cudnn.m.contextmanager",
    "signature": "(func)",
    "description": "@contextmanager decorator."
  },
  "3019": {
    "name": "flags",
    "module": "torch.backends.cudnn.m",
    "fullName": "torch.backends.cudnn.m.flags",
    "signature": "(enabled=False, benchmark=False, deterministic=False, allow_tf32=True)",
    "description": "No description available."
  },
  "3020": {
    "name": "is_acceptable",
    "module": "torch.backends.cudnn.m",
    "fullName": "torch.backends.cudnn.m.is_acceptable",
    "signature": "(tensor)",
    "description": "No description available."
  },
  "3021": {
    "name": "is_available",
    "module": "torch.backends.cudnn.m",
    "fullName": "torch.backends.cudnn.m.is_available",
    "signature": "()",
    "description": "Returns a bool indicating if CUDNN is currently available."
  },
  "3022": {
    "name": "set_flags",
    "module": "torch.backends.cudnn.m",
    "fullName": "torch.backends.cudnn.m.set_flags",
    "signature": "(_enabled=None, _benchmark=None, _deterministic=None, _allow_tf32=None)",
    "description": "No description available."
  },
  "3023": {
    "name": "version",
    "module": "torch.backends.cudnn.m",
    "fullName": "torch.backends.cudnn.m.version",
    "signature": "()",
    "description": "Returns the version of cuDNN"
  },
  "3024": {
    "name": "is_built",
    "module": "torch.backends.cuda",
    "fullName": "torch.backends.cuda.is_built",
    "signature": "()",
    "description": "Returns whether PyTorch is built with CUDA support.  Note that this"
  },
  "3025": {
    "name": "backward",
    "module": "torch.autograd",
    "fullName": "torch.autograd.backward",
    "signature": "(tensors: Union[torch.Tensor, Sequence[torch.Tensor]], grad_tensors: Union[torch.Tensor, Sequence[torch.Tensor], NoneType] = None, retain_graph: Optional[bool] = None, create_graph: bool = False, grad_variables: Union[torch.Tensor, Sequence[torch.Tensor], NoneType] = None, inputs: Union[torch.Tensor, Sequence[torch.Tensor], NoneType] = None) -> None",
    "description": "Computes the sum of gradients of given tensors with respect to graph"
  },
  "3026": {
    "name": "cast",
    "module": "torch.autograd",
    "fullName": "torch.autograd.cast",
    "signature": "(typ, val)",
    "description": "Cast a value to a type."
  },
  "3027": {
    "name": "gradcheck",
    "module": "torch.autograd",
    "fullName": "torch.autograd.gradcheck",
    "signature": "(func: Callable[..., Union[torch.Tensor, Sequence[torch.Tensor]]], inputs: Union[torch.Tensor, Sequence[torch.Tensor]], *, eps: float = 1e-06, atol: float = 1e-05, rtol: float = 0.001, raise_exception: bool = True, check_sparse_nnz: bool = False, nondet_tol: float = 0.0, check_undefined_grad: bool = True, check_grad_dtypes: bool = False, check_batched_grad: bool = False, check_batched_forward_grad: bool = False, check_forward_ad: bool = False, check_backward_ad: bool = True, fast_mode: bool = False) -> bool",
    "description": "Check gradients computed via small finite differences against analytical"
  },
  "3028": {
    "name": "gradgradcheck",
    "module": "torch.autograd",
    "fullName": "torch.autograd.gradgradcheck",
    "signature": "(func: Callable[..., Union[torch.Tensor, Sequence[torch.Tensor]]], inputs: Union[torch.Tensor, Sequence[torch.Tensor]], grad_outputs: Union[torch.Tensor, Sequence[torch.Tensor], NoneType] = None, *, eps: float = 1e-06, atol: float = 1e-05, rtol: float = 0.001, gen_non_contig_grad_outputs: bool = False, raise_exception: bool = True, nondet_tol: float = 0.0, check_undefined_grad: bool = True, check_grad_dtypes: bool = False, check_batched_grad: bool = False, check_fwd_over_rev: bool = False, check_rev_over_rev: bool = True, fast_mode: bool = False) -> bool",
    "description": "Check gradients of gradients computed via small finite differences"
  },
  "3029": {
    "name": "handle_torch_function",
    "module": "torch.autograd",
    "fullName": "torch.autograd.handle_torch_function",
    "signature": "(public_api: Callable, relevant_args: Iterable[Any], *args, **kwargs) -> Any",
    "description": "Implement a function with checks for ``__torch_function__`` overrides."
  },
  "3030": {
    "name": "is_tensor_like",
    "module": "torch.autograd",
    "fullName": "torch.autograd.is_tensor_like",
    "signature": "(inp)",
    "description": "Returns ``True`` if the passed-in input is a Tensor-like."
  },
  "3031": {
    "name": "variable",
    "module": "torch.autograd",
    "fullName": "torch.autograd.variable",
    "signature": "(*args, **kwargs)",
    "description": "No description available."
  },
  "3032": {
    "name": "namedtuple",
    "module": "torch.autograd.profiler_util",
    "fullName": "torch.autograd.profiler_util.namedtuple",
    "signature": "(typename, field_names, *, rename=False, defaults=None, module=None)",
    "description": "Returns a new subclass of tuple with named fields."
  },
  "3033": {
    "name": "cast",
    "module": "torch.autograd.grad_mode",
    "fullName": "torch.autograd.grad_mode.cast",
    "signature": "(typ, val)",
    "description": "Cast a value to a type."
  },
  "3034": {
    "name": "hvp",
    "module": "torch.autograd.functional",
    "fullName": "torch.autograd.functional.hvp",
    "signature": "(func, inputs, v=None, create_graph=False, strict=False)",
    "description": "Function that computes the dot product between the Hessian of a given scalar"
  },
  "3035": {
    "name": "jvp",
    "module": "torch.autograd.functional",
    "fullName": "torch.autograd.functional.jvp",
    "signature": "(func, inputs, v=None, create_graph=False, strict=False)",
    "description": "Function that computes the dot product between  the Jacobian of"
  },
  "3036": {
    "name": "vhp",
    "module": "torch.autograd.functional",
    "fullName": "torch.autograd.functional.vhp",
    "signature": "(func, inputs, v=None, create_graph=False, strict=False)",
    "description": "Function that computes the dot product between a vector ``v`` and the"
  },
  "3037": {
    "name": "vjp",
    "module": "torch.autograd.functional",
    "fullName": "torch.autograd.functional.vjp",
    "signature": "(func, inputs, v=None, create_graph=False, strict=False)",
    "description": "Function that computes the dot product between a vector ``v`` and the"
  },
  "3038": {
    "name": "enter_dual_level",
    "module": "torch.autograd.functional.fwAD",
    "fullName": "torch.autograd.functional.fwAD.enter_dual_level",
    "signature": "()",
    "description": "Function that can be used to enter a new forward grad level."
  },
  "3039": {
    "name": "exit_dual_level",
    "module": "torch.autograd.functional.fwAD",
    "fullName": "torch.autograd.functional.fwAD.exit_dual_level",
    "signature": "(*, level=None)",
    "description": "Function that can be used to exit a forward grad level."
  },
  "3040": {
    "name": "make_dual",
    "module": "torch.autograd.functional.fwAD",
    "fullName": "torch.autograd.functional.fwAD.make_dual",
    "signature": "(tensor, tangent, *, level=None)",
    "description": "Associates a tensor value with a forward gradient, the tangent, to create a"
  },
  "3041": {
    "name": "namedtuple",
    "module": "torch.autograd.functional.fwAD",
    "fullName": "torch.autograd.functional.fwAD.namedtuple",
    "signature": "(typename, field_names, *, rename=False, defaults=None, module=None)",
    "description": "Returns a new subclass of tuple with named fields."
  },
  "3042": {
    "name": "unpack_dual",
    "module": "torch.autograd.functional.fwAD",
    "fullName": "torch.autograd.functional.fwAD.unpack_dual",
    "signature": "(tensor, *, level=None)",
    "description": "Unpacks a \"dual tensor\" to get both its Tensor value and its forward AD gradient."
  },
  "3043": {
    "name": "once_differentiable",
    "module": "torch.autograd.function",
    "fullName": "torch.autograd.function.once_differentiable",
    "signature": "(fn)",
    "description": "No description available."
  },
  "3044": {
    "name": "traceable",
    "module": "torch.autograd.function",
    "fullName": "torch.autograd.function.traceable",
    "signature": "(fn_cls)",
    "description": "Marks Function as traceable for the JIT."
  },
  "3045": {
    "name": "with_metaclass",
    "module": "torch.autograd.function",
    "fullName": "torch.autograd.function.with_metaclass",
    "signature": "(meta: type, *bases) -> type",
    "description": "Create a base class with a metaclass."
  },
  "3046": {
    "name": "Pattern",
    "module": "torch.ao.quantization",
    "fullName": "torch.ao.quantization.Pattern",
    "signature": "(*args, **kwargs)",
    "description": "No description available."
  },
  "3047": {
    "name": "QConfigAny",
    "module": "torch.ao.quantization",
    "fullName": "torch.ao.quantization.QConfigAny",
    "signature": "(*args, **kwargs)",
    "description": "No description available."
  },
  "3048": {
    "name": "abstractmethod",
    "module": "torch.ao.quantization",
    "fullName": "torch.ao.quantization.abstractmethod",
    "signature": "(funcobj)",
    "description": "A decorator indicating abstract methods."
  },
  "3049": {
    "name": "activation_is_memoryless",
    "module": "torch.ao.quantization",
    "fullName": "torch.ao.quantization.activation_is_memoryless",
    "signature": "(qconfig: torch.ao.quantization.qconfig.QConfig)",
    "description": "Return whether the observer for activations defined in the given QConfig is memoryless."
  },
  "3050": {
    "name": "add_module_to_qconfig_obs_ctr",
    "module": "torch.ao.quantization",
    "fullName": "torch.ao.quantization.add_module_to_qconfig_obs_ctr",
    "signature": "(qconfig: Optional[torch.ao.quantization.qconfig.QConfig], module: Optional[torch.nn.modules.module.Module]) -> Any",
    "description": "This is a helper function for use in quantization prepare that updates a qconfig so that"
  },
  "3051": {
    "name": "add_observer_",
    "module": "torch.ao.quantization",
    "fullName": "torch.ao.quantization.add_observer_",
    "signature": "(module, qconfig_propagation_list=None, non_leaf_module_list=None, device=None, custom_module_class_mapping=None)",
    "description": "Add observer for the leaf child of the module."
  },
  "3052": {
    "name": "add_quant_dequant",
    "module": "torch.ao.quantization",
    "fullName": "torch.ao.quantization.add_quant_dequant",
    "signature": "(module)",
    "description": "Wrap the leaf child module in QuantWrapper if it has a valid qconfig"
  },
  "3053": {
    "name": "assert_valid_qconfig",
    "module": "torch.ao.quantization",
    "fullName": "torch.ao.quantization.assert_valid_qconfig",
    "signature": "(qconfig: Optional[torch.ao.quantization.qconfig.QConfig], mod: torch.nn.modules.module.Module) -> None",
    "description": "Verifies that this `qconfig` is valid."
  },
  "3054": {
    "name": "calculate_qmin_qmax",
    "module": "torch.ao.quantization",
    "fullName": "torch.ao.quantization.calculate_qmin_qmax",
    "signature": "(quant_min: int, quant_max: int, has_customized_qrange: bool, dtype: torch.dtype, reduce_range: bool) -> Tuple[int, int]",
    "description": "Calculates actual qmin and qmax based on the quantization range,"
  },
  "3055": {
    "name": "check_min_max_valid",
    "module": "torch.ao.quantization",
    "fullName": "torch.ao.quantization.check_min_max_valid",
    "signature": "(min_val: torch.Tensor, max_val: torch.Tensor) -> bool",
    "description": "Checks if the given minimum and maximum values are valid, meaning that"
  },
  "3056": {
    "name": "convert",
    "module": "torch.ao.quantization",
    "fullName": "torch.ao.quantization.convert",
    "signature": "(module, mapping=None, inplace=False, remove_qconfig=True, is_reference=False, convert_custom_config_dict=None)",
    "description": "Converts submodules in input module to a different module according to `mapping`"
  },
  "3057": {
    "name": "convert_dynamic_jit",
    "module": "torch.ao.quantization",
    "fullName": "torch.ao.quantization.convert_dynamic_jit",
    "signature": "(model, inplace=False, debug=False, preserved_attrs=None)",
    "description": "No description available."
  },
  "3058": {
    "name": "convert_jit",
    "module": "torch.ao.quantization",
    "fullName": "torch.ao.quantization.convert_jit",
    "signature": "(model, inplace=False, debug=False, preserved_attrs=None)",
    "description": "No description available."
  },
  "3059": {
    "name": "default_eval_fn",
    "module": "torch.ao.quantization",
    "fullName": "torch.ao.quantization.default_eval_fn",
    "signature": "(model, calib_data)",
    "description": "Default evaluation function takes a torch.utils.data.Dataset or a list of"
  },
  "3060": {
    "name": "disable_fake_quant",
    "module": "torch.ao.quantization",
    "fullName": "torch.ao.quantization.disable_fake_quant",
    "signature": "(mod)",
    "description": "Disable fake quantization for this module, if applicable. Example usage::"
  },
  "3061": {
    "name": "disable_observer",
    "module": "torch.ao.quantization",
    "fullName": "torch.ao.quantization.disable_observer",
    "signature": "(mod)",
    "description": "Disable observation for this module, if applicable. Example usage::"
  },
  "3062": {
    "name": "enable_fake_quant",
    "module": "torch.ao.quantization",
    "fullName": "torch.ao.quantization.enable_fake_quant",
    "signature": "(mod)",
    "description": "Enable fake quantization for this module, if applicable. Example usage::"
  },
  "3063": {
    "name": "enable_observer",
    "module": "torch.ao.quantization",
    "fullName": "torch.ao.quantization.enable_observer",
    "signature": "(mod)",
    "description": "Enable observation for this module, if applicable. Example usage::"
  },
  "3064": {
    "name": "fuse_conv_bn",
    "module": "torch.ao.quantization",
    "fullName": "torch.ao.quantization.fuse_conv_bn",
    "signature": "(is_qat, conv, bn)",
    "description": "Given the conv and bn modules, fuses them and returns the fused module"
  },
  "3065": {
    "name": "fuse_conv_bn_jit",
    "module": "torch.ao.quantization",
    "fullName": "torch.ao.quantization.fuse_conv_bn_jit",
    "signature": "(model, inplace=False)",
    "description": "Fuse conv - bn module"
  },
  "3066": {
    "name": "fuse_conv_bn_relu",
    "module": "torch.ao.quantization",
    "fullName": "torch.ao.quantization.fuse_conv_bn_relu",
    "signature": "(is_qat, conv, bn, relu)",
    "description": "Given the conv and bn modules, fuses them and returns the fused module"
  },
  "3067": {
    "name": "fuse_convtranspose_bn",
    "module": "torch.ao.quantization",
    "fullName": "torch.ao.quantization.fuse_convtranspose_bn",
    "signature": "(is_qat, convt, bn)",
    "description": "Given ConvTranspose and bn modules, fuses them and returns the fused module"
  },
  "3068": {
    "name": "fuse_linear_bn",
    "module": "torch.ao.quantization",
    "fullName": "torch.ao.quantization.fuse_linear_bn",
    "signature": "(is_qat, linear, bn)",
    "description": "Given the linear and bn modules, fuses them and returns the fused module"
  },
  "3069": {
    "name": "fuse_modules",
    "module": "torch.ao.quantization",
    "fullName": "torch.ao.quantization.fuse_modules",
    "signature": "(model, modules_to_fuse, inplace=False, fuser_func=<function fuse_known_modules at 0x7fcda8157160>, fuse_custom_config_dict=None)",
    "description": "Fuses a list of modules into a single module"
  },
  "3070": {
    "name": "fuse_modules_qat",
    "module": "torch.ao.quantization",
    "fullName": "torch.ao.quantization.fuse_modules_qat",
    "signature": "(model, modules_to_fuse, inplace=False, fuser_func=<function fuse_known_modules at 0x7fcda8157160>, fuse_custom_config_dict=None)",
    "description": "QAT version for `fuse_modules`"
  },
  "3071": {
    "name": "get_combined_dict",
    "module": "torch.ao.quantization",
    "fullName": "torch.ao.quantization.get_combined_dict",
    "signature": "(default_dict, additional_dict)",
    "description": "No description available."
  },
  "3072": {
    "name": "get_default_compare_output_module_list",
    "module": "torch.ao.quantization",
    "fullName": "torch.ao.quantization.get_default_compare_output_module_list",
    "signature": "() -> Set[Callable]",
    "description": "Get list of module class types that we will record output"
  },
  "3073": {
    "name": "get_default_dynamic_quant_module_mappings",
    "module": "torch.ao.quantization",
    "fullName": "torch.ao.quantization.get_default_dynamic_quant_module_mappings",
    "signature": "() -> Dict[Callable, Any]",
    "description": "Get module mapping for post training dynamic quantization"
  },
  "3074": {
    "name": "get_default_dynamic_sparse_quant_module_mappings",
    "module": "torch.ao.quantization",
    "fullName": "torch.ao.quantization.get_default_dynamic_sparse_quant_module_mappings",
    "signature": "() -> Dict[Callable, Any]",
    "description": "Get module mapping for post training dynamic sparse quantization"
  },
  "3075": {
    "name": "get_default_float_to_quantized_operator_mappings",
    "module": "torch.ao.quantization",
    "fullName": "torch.ao.quantization.get_default_float_to_quantized_operator_mappings",
    "signature": "() -> Dict[Union[Callable, str], Callable]",
    "description": "No description available."
  },
  "3076": {
    "name": "get_default_qat_module_mappings",
    "module": "torch.ao.quantization",
    "fullName": "torch.ao.quantization.get_default_qat_module_mappings",
    "signature": "() -> Dict[Callable, Any]",
    "description": "Get default module mapping for quantization aware training"
  },
  "3077": {
    "name": "get_default_qat_qconfig",
    "module": "torch.ao.quantization",
    "fullName": "torch.ao.quantization.get_default_qat_qconfig",
    "signature": "(backend='fbgemm', version=1)",
    "description": "Returns the default QAT qconfig for the specified backend."
  },
  "3078": {
    "name": "get_default_qat_qconfig_dict",
    "module": "torch.ao.quantization",
    "fullName": "torch.ao.quantization.get_default_qat_qconfig_dict",
    "signature": "(backend='fbgemm', version=1)",
    "description": "No description available."
  },
  "3079": {
    "name": "get_default_qconfig",
    "module": "torch.ao.quantization",
    "fullName": "torch.ao.quantization.get_default_qconfig",
    "signature": "(backend='fbgemm', version=0)",
    "description": "Returns the default PTQ qconfig for the specified backend."
  },
  "3080": {
    "name": "get_default_qconfig_dict",
    "module": "torch.ao.quantization",
    "fullName": "torch.ao.quantization.get_default_qconfig_dict",
    "signature": "(backend='fbgemm', version=0)",
    "description": "No description available."
  },
  "3081": {
    "name": "get_default_qconfig_propagation_list",
    "module": "torch.ao.quantization",
    "fullName": "torch.ao.quantization.get_default_qconfig_propagation_list",
    "signature": "() -> Set[Callable]",
    "description": "Get the default list of module types that we'll attach qconfig"
  },
  "3082": {
    "name": "get_default_static_quant_module_mappings",
    "module": "torch.ao.quantization",
    "fullName": "torch.ao.quantization.get_default_static_quant_module_mappings",
    "signature": "() -> Dict[Callable, Any]",
    "description": "Get module mapping for post training static quantization"
  },
  "3083": {
    "name": "get_default_static_quant_reference_module_mappings",
    "module": "torch.ao.quantization",
    "fullName": "torch.ao.quantization.get_default_static_quant_reference_module_mappings",
    "signature": "() -> Dict[Callable, Any]",
    "description": "Get reference module mapping for post training static quantization"
  },
  "3084": {
    "name": "get_default_static_sparse_quant_module_mappings",
    "module": "torch.ao.quantization",
    "fullName": "torch.ao.quantization.get_default_static_sparse_quant_module_mappings",
    "signature": "() -> Dict[Callable, Any]",
    "description": "Get module mapping for post training static sparse quantization"
  },
  "3085": {
    "name": "get_dynamic_quant_module_class",
    "module": "torch.ao.quantization",
    "fullName": "torch.ao.quantization.get_dynamic_quant_module_class",
    "signature": "(float_module_class: Callable, additional_dynamic_quant_mapping: Optional[Dict[Callable, Any]] = None) -> Any",
    "description": "n Get the dynamically quantized module class corresponding to"
  },
  "3086": {
    "name": "get_embedding_qat_module_mappings",
    "module": "torch.ao.quantization",
    "fullName": "torch.ao.quantization.get_embedding_qat_module_mappings",
    "signature": "() -> Dict[Callable, Any]",
    "description": "Get module mapping for quantization aware training"
  },
  "3087": {
    "name": "get_embedding_static_quant_module_mappings",
    "module": "torch.ao.quantization",
    "fullName": "torch.ao.quantization.get_embedding_static_quant_module_mappings",
    "signature": "() -> Dict[Callable, Any]",
    "description": "Get module mapping, including mapping for embedding QAT"
  },
  "3088": {
    "name": "get_fuser_method",
    "module": "torch.ao.quantization",
    "fullName": "torch.ao.quantization.get_fuser_method",
    "signature": "(op_list, additional_fuser_method_mapping=None)",
    "description": "Get fuser method for the given list of module types,"
  },
  "3089": {
    "name": "get_fuser_method_new",
    "module": "torch.ao.quantization",
    "fullName": "torch.ao.quantization.get_fuser_method_new",
    "signature": "(op_pattern: Union[Callable, Tuple[Callable, Callable], Tuple[Callable, Tuple[Callable, Callable]], Any], fuser_method_mapping: Optional[Dict[Union[Callable, Tuple[Callable, Callable], Tuple[Callable, Tuple[Callable, Callable]], Any], Union[torch.nn.modules.container.Sequential, Callable]]] = None)",
    "description": "This will be made defult after we deparate the get_fuser_method"
  },
  "3090": {
    "name": "get_observer_dict",
    "module": "torch.ao.quantization",
    "fullName": "torch.ao.quantization.get_observer_dict",
    "signature": "(mod, target_dict, prefix='')",
    "description": "Traverse the modules and save all observers into dict."
  },
  "3091": {
    "name": "get_observer_state_dict",
    "module": "torch.ao.quantization",
    "fullName": "torch.ao.quantization.get_observer_state_dict",
    "signature": "(mod)",
    "description": "Returns the state dict corresponding to the observer stats."
  },
  "3092": {
    "name": "get_qparam_dict",
    "module": "torch.ao.quantization",
    "fullName": "torch.ao.quantization.get_qparam_dict",
    "signature": "(observer_or_fake_quant)",
    "description": "No description available."
  },
  "3093": {
    "name": "get_quantized_operator",
    "module": "torch.ao.quantization",
    "fullName": "torch.ao.quantization.get_quantized_operator",
    "signature": "(float_op: Union[Callable, str]) -> Callable",
    "description": "Get the quantized operator corresponding to the float operator"
  },
  "3094": {
    "name": "get_static_quant_module_class",
    "module": "torch.ao.quantization",
    "fullName": "torch.ao.quantization.get_static_quant_module_class",
    "signature": "(float_module_class: Callable, additional_static_quant_mapping: Optional[Dict[Callable, Any]] = None, is_reference: bool = False) -> Any",
    "description": "n Get the statically quantized module class corresponding to"
  },
  "3095": {
    "name": "get_unique_devices_",
    "module": "torch.ao.quantization",
    "fullName": "torch.ao.quantization.get_unique_devices_",
    "signature": "(module)",
    "description": "No description available."
  },
  "3096": {
    "name": "get_valid_patterns",
    "module": "torch.ao.quantization",
    "fullName": "torch.ao.quantization.get_valid_patterns",
    "signature": "(op_pattern)",
    "description": "Returns a list of valid patterns generated from the op_pattern,"
  },
  "3097": {
    "name": "has_no_children_ignoring_parametrizations",
    "module": "torch.ao.quantization",
    "fullName": "torch.ao.quantization.has_no_children_ignoring_parametrizations",
    "signature": "(module)",
    "description": "Checks if module._modules is empty or"
  },
  "3098": {
    "name": "is_activation_post_process",
    "module": "torch.ao.quantization",
    "fullName": "torch.ao.quantization.is_activation_post_process",
    "signature": "(module)",
    "description": "No description available."
  },
  "3099": {
    "name": "is_reuse_input_qconfig",
    "module": "torch.ao.quantization",
    "fullName": "torch.ao.quantization.is_reuse_input_qconfig",
    "signature": "(qconfig: Optional[torch.ao.quantization.qconfig.QConfig])",
    "description": "No description available."
  },
  "3100": {
    "name": "load_observer_state_dict",
    "module": "torch.ao.quantization",
    "fullName": "torch.ao.quantization.load_observer_state_dict",
    "signature": "(mod, obs_dict)",
    "description": "Given input model and a state_dict containing model observer stats,"
  },
  "3101": {
    "name": "namedtuple",
    "module": "torch.ao.quantization",
    "fullName": "torch.ao.quantization.namedtuple",
    "signature": "(typename, field_names, *, rename=False, defaults=None, module=None)",
    "description": "Returns a new subclass of tuple with named fields."
  },
  "3102": {
    "name": "no_observer_set",
    "module": "torch.ao.quantization",
    "fullName": "torch.ao.quantization.no_observer_set",
    "signature": "() -> Set[Any]",
    "description": "These modules cannot have observers inserted by default."
  },
  "3103": {
    "name": "prepare",
    "module": "torch.ao.quantization",
    "fullName": "torch.ao.quantization.prepare",
    "signature": "(model, inplace=False, allow_list=None, observer_non_leaf_module_list=None, prepare_custom_config_dict=None)",
    "description": "Prepares a copy of the model for quantization calibration or quantization-aware training."
  },
  "3104": {
    "name": "prepare_dynamic_jit",
    "module": "torch.ao.quantization",
    "fullName": "torch.ao.quantization.prepare_dynamic_jit",
    "signature": "(model, qconfig_dict, inplace=False)",
    "description": "No description available."
  },
  "3105": {
    "name": "prepare_jit",
    "module": "torch.ao.quantization",
    "fullName": "torch.ao.quantization.prepare_jit",
    "signature": "(model, qconfig_dict, inplace=False)",
    "description": "No description available."
  },
  "3106": {
    "name": "prepare_qat",
    "module": "torch.ao.quantization",
    "fullName": "torch.ao.quantization.prepare_qat",
    "signature": "(model, mapping=None, inplace=False)",
    "description": "Prepares a copy of the model for quantization calibration or"
  },
  "3107": {
    "name": "propagate_qconfig_",
    "module": "torch.ao.quantization",
    "fullName": "torch.ao.quantization.propagate_qconfig_",
    "signature": "(module, qconfig_dict=None, prepare_custom_config_dict=None)",
    "description": "Propagate qconfig through the module hierarchy and assign `qconfig`"
  },
  "3108": {
    "name": "qconfig_equals",
    "module": "torch.ao.quantization",
    "fullName": "torch.ao.quantization.qconfig_equals",
    "signature": "(q1: Optional[torch.ao.quantization.qconfig.QConfig], q2: Optional[torch.ao.quantization.qconfig.QConfig])",
    "description": "Returns `True` if `q1` equals `q2`, and `False` otherwise."
  },
  "3109": {
    "name": "quant_type_to_str",
    "module": "torch.ao.quantization",
    "fullName": "torch.ao.quantization.quant_type_to_str",
    "signature": "(quant_type)",
    "description": "No description available."
  },
  "3110": {
    "name": "quantize",
    "module": "torch.ao.quantization",
    "fullName": "torch.ao.quantization.quantize",
    "signature": "(model, run_fn, run_args, mapping=None, inplace=False)",
    "description": "Quantize the input float model with post training static quantization."
  },
  "3111": {
    "name": "quantize_dynamic",
    "module": "torch.ao.quantization",
    "fullName": "torch.ao.quantization.quantize_dynamic",
    "signature": "(model, qconfig_spec=None, dtype=torch.qint8, mapping=None, inplace=False)",
    "description": "Converts a float model to dynamic (i.e. weights-only) quantized model."
  },
  "3112": {
    "name": "quantize_dynamic_jit",
    "module": "torch.ao.quantization",
    "fullName": "torch.ao.quantization.quantize_dynamic_jit",
    "signature": "(model, qconfig_dict, inplace=False, debug=False)",
    "description": "Quantize the input float TorchScript model with"
  },
  "3113": {
    "name": "quantize_jit",
    "module": "torch.ao.quantization",
    "fullName": "torch.ao.quantization.quantize_jit",
    "signature": "(model, qconfig_dict, run_fn, run_args, inplace=False, debug=False)",
    "description": "Quantize the input float TorchScript model with"
  },
  "3114": {
    "name": "quantize_qat",
    "module": "torch.ao.quantization",
    "fullName": "torch.ao.quantization.quantize_qat",
    "signature": "(model, run_fn, run_args, inplace=False)",
    "description": "Do quantization aware training and output a quantized model"
  },
  "3115": {
    "name": "register_activation_post_process_hook",
    "module": "torch.ao.quantization",
    "fullName": "torch.ao.quantization.register_activation_post_process_hook",
    "signature": "(module, pre_hook=False)",
    "description": "No description available."
  },
  "3116": {
    "name": "reverse2",
    "module": "torch.ao.quantization",
    "fullName": "torch.ao.quantization.reverse2",
    "signature": "(f)",
    "description": "No description available."
  },
  "3117": {
    "name": "reverse3",
    "module": "torch.ao.quantization",
    "fullName": "torch.ao.quantization.reverse3",
    "signature": "(f)",
    "description": "No description available."
  },
  "3118": {
    "name": "reverse_sequential_wrapper2",
    "module": "torch.ao.quantization",
    "fullName": "torch.ao.quantization.reverse_sequential_wrapper2",
    "signature": "(sequential)",
    "description": "Given a sequential class for two modules, return a function that takes"
  },
  "3119": {
    "name": "script_qconfig",
    "module": "torch.ao.quantization",
    "fullName": "torch.ao.quantization.script_qconfig",
    "signature": "(qconfig)",
    "description": "Instantiate the activation and weight observer modules and script"
  },
  "3120": {
    "name": "script_qconfig_dict",
    "module": "torch.ao.quantization",
    "fullName": "torch.ao.quantization.script_qconfig_dict",
    "signature": "(qconfig_dict)",
    "description": "Helper function used by `prepare_jit`."
  },
  "3121": {
    "name": "sequential_wrapper2",
    "module": "torch.ao.quantization",
    "fullName": "torch.ao.quantization.sequential_wrapper2",
    "signature": "(sequential)",
    "description": "Given a sequential class for two modules, return a function that takes"
  },
  "3122": {
    "name": "swap_module",
    "module": "torch.ao.quantization",
    "fullName": "torch.ao.quantization.swap_module",
    "signature": "(mod, mapping, custom_module_class_mapping)",
    "description": "Swaps the module if it has a quantized counterpart and it has an"
  },
  "3123": {
    "name": "type_before_parametrizations",
    "module": "torch.ao.quantization",
    "fullName": "torch.ao.quantization.type_before_parametrizations",
    "signature": "(module: torch.nn.modules.module.Module) -> type",
    "description": "Returns the module type before parametrizations were applied and if not,"
  },
  "3124": {
    "name": "wrap_cpp_module",
    "module": "torch.ao.quantization",
    "fullName": "torch.ao.quantization.wrap_cpp_module",
    "signature": "(cpp_module)",
    "description": "Wrap this torch._C.ScriptModule in a Python ScriptModule, recursively for all submodules"
  },
  "3125": {
    "name": "Pattern",
    "module": "torch.ao.quantization.utils",
    "fullName": "torch.ao.quantization.utils.Pattern",
    "signature": "(*args, **kwargs)",
    "description": "No description available."
  },
  "3126": {
    "name": "activation_dtype",
    "module": "torch.ao.quantization.utils",
    "fullName": "torch.ao.quantization.utils.activation_dtype",
    "signature": "(qconfig)",
    "description": "No description available."
  },
  "3127": {
    "name": "activation_is_dynamically_quantized",
    "module": "torch.ao.quantization.utils",
    "fullName": "torch.ao.quantization.utils.activation_is_dynamically_quantized",
    "signature": "(qconfig)",
    "description": "Given a qconfig, decide if the activation needs to be"
  },
  "3128": {
    "name": "activation_is_int32_quantized",
    "module": "torch.ao.quantization.utils",
    "fullName": "torch.ao.quantization.utils.activation_is_int32_quantized",
    "signature": "(qconfig)",
    "description": "Given a qconfig, decide if the activation needs to be"
  },
  "3129": {
    "name": "activation_is_int8_quantized",
    "module": "torch.ao.quantization.utils",
    "fullName": "torch.ao.quantization.utils.activation_is_int8_quantized",
    "signature": "(qconfig)",
    "description": "Given a qconfig, decide if the activation needs to be"
  },
  "3130": {
    "name": "activation_is_statically_quantized",
    "module": "torch.ao.quantization.utils",
    "fullName": "torch.ao.quantization.utils.activation_is_statically_quantized",
    "signature": "(qconfig)",
    "description": "Given a qconfig, decide if the activation needs to be"
  },
  "3131": {
    "name": "calculate_qmin_qmax",
    "module": "torch.ao.quantization.utils",
    "fullName": "torch.ao.quantization.utils.calculate_qmin_qmax",
    "signature": "(quant_min: int, quant_max: int, has_customized_qrange: bool, dtype: torch.dtype, reduce_range: bool) -> Tuple[int, int]",
    "description": "Calculates actual qmin and qmax based on the quantization range,"
  },
  "3132": {
    "name": "check_min_max_valid",
    "module": "torch.ao.quantization.utils",
    "fullName": "torch.ao.quantization.utils.check_min_max_valid",
    "signature": "(min_val: torch.Tensor, max_val: torch.Tensor) -> bool",
    "description": "Checks if the given minimum and maximum values are valid, meaning that"
  },
  "3133": {
    "name": "check_node",
    "module": "torch.ao.quantization.utils",
    "fullName": "torch.ao.quantization.utils.check_node",
    "signature": "(node, modules)",
    "description": "No description available."
  },
  "3134": {
    "name": "get_combined_dict",
    "module": "torch.ao.quantization.utils",
    "fullName": "torch.ao.quantization.utils.get_combined_dict",
    "signature": "(default_dict, additional_dict)",
    "description": "No description available."
  },
  "3135": {
    "name": "get_qconfig_dtypes",
    "module": "torch.ao.quantization.utils",
    "fullName": "torch.ao.quantization.utils.get_qconfig_dtypes",
    "signature": "(qconfig)",
    "description": "returns the qconfig tuple for qconfig:"
  },
  "3136": {
    "name": "get_qparam_dict",
    "module": "torch.ao.quantization.utils",
    "fullName": "torch.ao.quantization.utils.get_qparam_dict",
    "signature": "(observer_or_fake_quant)",
    "description": "No description available."
  },
  "3137": {
    "name": "get_quant_type",
    "module": "torch.ao.quantization.utils",
    "fullName": "torch.ao.quantization.utils.get_quant_type",
    "signature": "(qconfig)",
    "description": "No description available."
  },
  "3138": {
    "name": "get_swapped_custom_module_class",
    "module": "torch.ao.quantization.utils",
    "fullName": "torch.ao.quantization.utils.get_swapped_custom_module_class",
    "signature": "(custom_module, custom_module_class_mapping, qconfig)",
    "description": "Get the observed/quantized custom module class that we need"
  },
  "3139": {
    "name": "getattr_from_fqn",
    "module": "torch.ao.quantization.utils",
    "fullName": "torch.ao.quantization.utils.getattr_from_fqn",
    "signature": "(obj: Any, fqn: str) -> Any",
    "description": "Given an obj and a fqn such as \"foo.bar.baz\", returns gm.foo.bar.baz."
  },
  "3140": {
    "name": "has_no_children_ignoring_parametrizations",
    "module": "torch.ao.quantization.utils",
    "fullName": "torch.ao.quantization.utils.has_no_children_ignoring_parametrizations",
    "signature": "(module)",
    "description": "Checks if module._modules is empty or"
  },
  "3141": {
    "name": "is_parametrized",
    "module": "torch.ao.quantization.utils",
    "fullName": "torch.ao.quantization.utils.is_parametrized",
    "signature": "(module: torch.nn.modules.module.Module, tensor_name: Optional[str] = None) -> bool",
    "description": "Returns ``True`` if module has an active parametrization."
  },
  "3142": {
    "name": "is_per_channel",
    "module": "torch.ao.quantization.utils",
    "fullName": "torch.ao.quantization.utils.is_per_channel",
    "signature": "(qscheme)",
    "description": "No description available."
  },
  "3143": {
    "name": "is_per_tensor",
    "module": "torch.ao.quantization.utils",
    "fullName": "torch.ao.quantization.utils.is_per_tensor",
    "signature": "(qscheme)",
    "description": "No description available."
  },
  "3144": {
    "name": "op_is_int8_dynamically_quantized",
    "module": "torch.ao.quantization.utils",
    "fullName": "torch.ao.quantization.utils.op_is_int8_dynamically_quantized",
    "signature": "(qconfig) -> bool",
    "description": "Given a qconfig, returns True if this op is using int8 dynamic"
  },
  "3145": {
    "name": "quant_type_to_str",
    "module": "torch.ao.quantization.utils",
    "fullName": "torch.ao.quantization.utils.quant_type_to_str",
    "signature": "(quant_type)",
    "description": "No description available."
  },
  "3146": {
    "name": "weight_dtype",
    "module": "torch.ao.quantization.utils",
    "fullName": "torch.ao.quantization.utils.weight_dtype",
    "signature": "(qconfig)",
    "description": "No description available."
  },
  "3147": {
    "name": "weight_is_quantized",
    "module": "torch.ao.quantization.utils",
    "fullName": "torch.ao.quantization.utils.weight_is_quantized",
    "signature": "(qconfig)",
    "description": "Given a qconfig, decide if the weight needs to be"
  },
  "3148": {
    "name": "weight_is_statically_quantized",
    "module": "torch.ao.quantization.utils",
    "fullName": "torch.ao.quantization.utils.weight_is_statically_quantized",
    "signature": "(qconfig)",
    "description": "Given a qconfig, decide if the weight needs to be statically"
  },
  "3149": {
    "name": "get_combined_dict",
    "module": "torch.ao.quantization.quantization_mappings",
    "fullName": "torch.ao.quantization.quantization_mappings.get_combined_dict",
    "signature": "(default_dict, additional_dict)",
    "description": "No description available."
  },
  "3150": {
    "name": "get_default_compare_output_module_list",
    "module": "torch.ao.quantization.quantization_mappings",
    "fullName": "torch.ao.quantization.quantization_mappings.get_default_compare_output_module_list",
    "signature": "() -> Set[Callable]",
    "description": "Get list of module class types that we will record output"
  },
  "3151": {
    "name": "get_default_dynamic_quant_module_mappings",
    "module": "torch.ao.quantization.quantization_mappings",
    "fullName": "torch.ao.quantization.quantization_mappings.get_default_dynamic_quant_module_mappings",
    "signature": "() -> Dict[Callable, Any]",
    "description": "Get module mapping for post training dynamic quantization"
  },
  "3152": {
    "name": "get_default_dynamic_sparse_quant_module_mappings",
    "module": "torch.ao.quantization.quantization_mappings",
    "fullName": "torch.ao.quantization.quantization_mappings.get_default_dynamic_sparse_quant_module_mappings",
    "signature": "() -> Dict[Callable, Any]",
    "description": "Get module mapping for post training dynamic sparse quantization"
  },
  "3153": {
    "name": "get_default_float_to_quantized_operator_mappings",
    "module": "torch.ao.quantization.quantization_mappings",
    "fullName": "torch.ao.quantization.quantization_mappings.get_default_float_to_quantized_operator_mappings",
    "signature": "() -> Dict[Union[Callable, str], Callable]",
    "description": "No description available."
  },
  "3154": {
    "name": "get_default_qat_module_mappings",
    "module": "torch.ao.quantization.quantization_mappings",
    "fullName": "torch.ao.quantization.quantization_mappings.get_default_qat_module_mappings",
    "signature": "() -> Dict[Callable, Any]",
    "description": "Get default module mapping for quantization aware training"
  },
  "3155": {
    "name": "get_default_qconfig_propagation_list",
    "module": "torch.ao.quantization.quantization_mappings",
    "fullName": "torch.ao.quantization.quantization_mappings.get_default_qconfig_propagation_list",
    "signature": "() -> Set[Callable]",
    "description": "Get the default list of module types that we'll attach qconfig"
  },
  "3156": {
    "name": "get_default_static_quant_module_mappings",
    "module": "torch.ao.quantization.quantization_mappings",
    "fullName": "torch.ao.quantization.quantization_mappings.get_default_static_quant_module_mappings",
    "signature": "() -> Dict[Callable, Any]",
    "description": "Get module mapping for post training static quantization"
  },
  "3157": {
    "name": "get_default_static_quant_reference_module_mappings",
    "module": "torch.ao.quantization.quantization_mappings",
    "fullName": "torch.ao.quantization.quantization_mappings.get_default_static_quant_reference_module_mappings",
    "signature": "() -> Dict[Callable, Any]",
    "description": "Get reference module mapping for post training static quantization"
  },
  "3158": {
    "name": "get_default_static_sparse_quant_module_mappings",
    "module": "torch.ao.quantization.quantization_mappings",
    "fullName": "torch.ao.quantization.quantization_mappings.get_default_static_sparse_quant_module_mappings",
    "signature": "() -> Dict[Callable, Any]",
    "description": "Get module mapping for post training static sparse quantization"
  },
  "3159": {
    "name": "get_dynamic_quant_module_class",
    "module": "torch.ao.quantization.quantization_mappings",
    "fullName": "torch.ao.quantization.quantization_mappings.get_dynamic_quant_module_class",
    "signature": "(float_module_class: Callable, additional_dynamic_quant_mapping: Optional[Dict[Callable, Any]] = None) -> Any",
    "description": "n Get the dynamically quantized module class corresponding to"
  },
  "3160": {
    "name": "get_embedding_qat_module_mappings",
    "module": "torch.ao.quantization.quantization_mappings",
    "fullName": "torch.ao.quantization.quantization_mappings.get_embedding_qat_module_mappings",
    "signature": "() -> Dict[Callable, Any]",
    "description": "Get module mapping for quantization aware training"
  },
  "3161": {
    "name": "get_embedding_static_quant_module_mappings",
    "module": "torch.ao.quantization.quantization_mappings",
    "fullName": "torch.ao.quantization.quantization_mappings.get_embedding_static_quant_module_mappings",
    "signature": "() -> Dict[Callable, Any]",
    "description": "Get module mapping, including mapping for embedding QAT"
  },
  "3162": {
    "name": "get_quantized_operator",
    "module": "torch.ao.quantization.quantization_mappings",
    "fullName": "torch.ao.quantization.quantization_mappings.get_quantized_operator",
    "signature": "(float_op: Union[Callable, str]) -> Callable",
    "description": "Get the quantized operator corresponding to the float operator"
  },
  "3163": {
    "name": "get_static_quant_module_class",
    "module": "torch.ao.quantization.quantization_mappings",
    "fullName": "torch.ao.quantization.quantization_mappings.get_static_quant_module_class",
    "signature": "(float_module_class: Callable, additional_static_quant_mapping: Optional[Dict[Callable, Any]] = None, is_reference: bool = False) -> Any",
    "description": "n Get the statically quantized module class corresponding to"
  },
  "3164": {
    "name": "no_observer_set",
    "module": "torch.ao.quantization.quantization_mappings",
    "fullName": "torch.ao.quantization.quantization_mappings.no_observer_set",
    "signature": "() -> Set[Any]",
    "description": "These modules cannot have observers inserted by default."
  },
  "3165": {
    "name": "type_before_parametrizations",
    "module": "torch.ao.quantization.quantization_mappings",
    "fullName": "torch.ao.quantization.quantization_mappings.type_before_parametrizations",
    "signature": "(module: torch.nn.modules.module.Module) -> type",
    "description": "Returns the module type before parametrizations were applied and if not,"
  },
  "3166": {
    "name": "apply_permutation",
    "module": "torch.ao.quantization.quantization_mappings.nnqr.modules.rnn",
    "fullName": "torch.ao.quantization.quantization_mappings.nnqr.modules.rnn.apply_permutation",
    "signature": "(tensor: torch.Tensor, permutation: torch.Tensor, dim: int = 1) -> torch.Tensor",
    "description": "No description available."
  },
  "3167": {
    "name": "get_quantize_and_dequantized_weight",
    "module": "torch.ao.quantization.quantization_mappings.nnqr.modules.rnn",
    "fullName": "torch.ao.quantization.quantization_mappings.nnqr.modules.rnn.get_quantize_and_dequantized_weight",
    "signature": "(module, wn)",
    "description": "No description available."
  },
  "3168": {
    "name": "get_quantized_weight",
    "module": "torch.ao.quantization.quantization_mappings.nnqr.modules.rnn",
    "fullName": "torch.ao.quantization.quantization_mappings.nnqr.modules.rnn.get_quantized_weight",
    "signature": "(module, wn)",
    "description": "No description available."
  },
  "3169": {
    "name": "get_weight_and_quantization_params",
    "module": "torch.ao.quantization.quantization_mappings.nnqr.modules.rnn",
    "fullName": "torch.ao.quantization.quantization_mappings.nnqr.modules.rnn.get_weight_and_quantization_params",
    "signature": "(module, wn)",
    "description": "No description available."
  },
  "3170": {
    "name": "is_valid_linear_block_sparse_pattern",
    "module": "torch.ao.quantization.quantization_mappings.ao_nn.sparse.quantized.utils",
    "fullName": "torch.ao.quantization.quantization_mappings.ao_nn.sparse.quantized.utils.is_valid_linear_block_sparse_pattern",
    "signature": "(row_block_size, col_block_size)",
    "description": "No description available."
  },
  "3171": {
    "name": "hide_packed_params_repr",
    "module": "torch.ao.quantization.quantization_mappings.ao_nn.sparse.quantized.linear",
    "fullName": "torch.ao.quantization.quantization_mappings.ao_nn.sparse.quantized.linear.hide_packed_params_repr",
    "signature": "(self, params)",
    "description": "No description available."
  },
  "3172": {
    "name": "hide_packed_params_repr",
    "module": "torch.ao.quantization.quantization_mappings.ao_nn.sparse.quantized.dynamic.linear",
    "fullName": "torch.ao.quantization.quantization_mappings.ao_nn.sparse.quantized.dynamic.linear.hide_packed_params_repr",
    "signature": "(self, params)",
    "description": "No description available."
  },
  "3173": {
    "name": "quant_type_to_str",
    "module": "torch.ao.quantization.quant_type",
    "fullName": "torch.ao.quantization.quant_type.quant_type_to_str",
    "signature": "(quant_type)",
    "description": "No description available."
  },
  "3174": {
    "name": "QConfigAny",
    "module": "torch.ao.quantization.qconfig",
    "fullName": "torch.ao.quantization.qconfig.QConfigAny",
    "signature": "(*args, **kwargs)",
    "description": "No description available."
  },
  "3175": {
    "name": "activation_is_memoryless",
    "module": "torch.ao.quantization.qconfig",
    "fullName": "torch.ao.quantization.qconfig.activation_is_memoryless",
    "signature": "(qconfig: torch.ao.quantization.qconfig.QConfig)",
    "description": "Return whether the observer for activations defined in the given QConfig is memoryless."
  },
  "3176": {
    "name": "add_module_to_qconfig_obs_ctr",
    "module": "torch.ao.quantization.qconfig",
    "fullName": "torch.ao.quantization.qconfig.add_module_to_qconfig_obs_ctr",
    "signature": "(qconfig: Optional[torch.ao.quantization.qconfig.QConfig], module: Optional[torch.nn.modules.module.Module]) -> Any",
    "description": "This is a helper function for use in quantization prepare that updates a qconfig so that"
  },
  "3177": {
    "name": "assert_valid_qconfig",
    "module": "torch.ao.quantization.qconfig",
    "fullName": "torch.ao.quantization.qconfig.assert_valid_qconfig",
    "signature": "(qconfig: Optional[torch.ao.quantization.qconfig.QConfig], mod: torch.nn.modules.module.Module) -> None",
    "description": "Verifies that this `qconfig` is valid."
  },
  "3178": {
    "name": "get_default_qat_qconfig",
    "module": "torch.ao.quantization.qconfig",
    "fullName": "torch.ao.quantization.qconfig.get_default_qat_qconfig",
    "signature": "(backend='fbgemm', version=1)",
    "description": "Returns the default QAT qconfig for the specified backend."
  },
  "3179": {
    "name": "get_default_qat_qconfig_dict",
    "module": "torch.ao.quantization.qconfig",
    "fullName": "torch.ao.quantization.qconfig.get_default_qat_qconfig_dict",
    "signature": "(backend='fbgemm', version=1)",
    "description": "No description available."
  },
  "3180": {
    "name": "get_default_qconfig",
    "module": "torch.ao.quantization.qconfig",
    "fullName": "torch.ao.quantization.qconfig.get_default_qconfig",
    "signature": "(backend='fbgemm', version=0)",
    "description": "Returns the default PTQ qconfig for the specified backend."
  },
  "3181": {
    "name": "get_default_qconfig_dict",
    "module": "torch.ao.quantization.qconfig",
    "fullName": "torch.ao.quantization.qconfig.get_default_qconfig_dict",
    "signature": "(backend='fbgemm', version=0)",
    "description": "No description available."
  },
  "3182": {
    "name": "is_reuse_input_qconfig",
    "module": "torch.ao.quantization.qconfig",
    "fullName": "torch.ao.quantization.qconfig.is_reuse_input_qconfig",
    "signature": "(qconfig: Optional[torch.ao.quantization.qconfig.QConfig])",
    "description": "No description available."
  },
  "3183": {
    "name": "namedtuple",
    "module": "torch.ao.quantization.qconfig",
    "fullName": "torch.ao.quantization.qconfig.namedtuple",
    "signature": "(typename, field_names, *, rename=False, defaults=None, module=None)",
    "description": "Returns a new subclass of tuple with named fields."
  },
  "3184": {
    "name": "qconfig_equals",
    "module": "torch.ao.quantization.qconfig",
    "fullName": "torch.ao.quantization.qconfig.qconfig_equals",
    "signature": "(q1: Optional[torch.ao.quantization.qconfig.QConfig], q2: Optional[torch.ao.quantization.qconfig.QConfig])",
    "description": "Returns `True` if `q1` equals `q2`, and `False` otherwise."
  },
  "3185": {
    "name": "abstractmethod",
    "module": "torch.ao.quantization.observer",
    "fullName": "torch.ao.quantization.observer.abstractmethod",
    "signature": "(funcobj)",
    "description": "A decorator indicating abstract methods."
  },
  "3186": {
    "name": "calculate_qmin_qmax",
    "module": "torch.ao.quantization.observer",
    "fullName": "torch.ao.quantization.observer.calculate_qmin_qmax",
    "signature": "(quant_min: int, quant_max: int, has_customized_qrange: bool, dtype: torch.dtype, reduce_range: bool) -> Tuple[int, int]",
    "description": "Calculates actual qmin and qmax based on the quantization range,"
  },
  "3187": {
    "name": "check_min_max_valid",
    "module": "torch.ao.quantization.observer",
    "fullName": "torch.ao.quantization.observer.check_min_max_valid",
    "signature": "(min_val: torch.Tensor, max_val: torch.Tensor) -> bool",
    "description": "Checks if the given minimum and maximum values are valid, meaning that"
  },
  "3188": {
    "name": "get_observer_state_dict",
    "module": "torch.ao.quantization.observer",
    "fullName": "torch.ao.quantization.observer.get_observer_state_dict",
    "signature": "(mod)",
    "description": "Returns the state dict corresponding to the observer stats."
  },
  "3189": {
    "name": "load_observer_state_dict",
    "module": "torch.ao.quantization.observer",
    "fullName": "torch.ao.quantization.observer.load_observer_state_dict",
    "signature": "(mod, obs_dict)",
    "description": "Given input model and a state_dict containing model observer stats,"
  },
  "3190": {
    "name": "Pattern",
    "module": "torch.ao.quantization.fuser_method_mappings",
    "fullName": "torch.ao.quantization.fuser_method_mappings.Pattern",
    "signature": "(*args, **kwargs)",
    "description": "No description available."
  },
  "3191": {
    "name": "fuse_conv_bn",
    "module": "torch.ao.quantization.fuser_method_mappings",
    "fullName": "torch.ao.quantization.fuser_method_mappings.fuse_conv_bn",
    "signature": "(is_qat, conv, bn)",
    "description": "Given the conv and bn modules, fuses them and returns the fused module"
  },
  "3192": {
    "name": "fuse_conv_bn_relu",
    "module": "torch.ao.quantization.fuser_method_mappings",
    "fullName": "torch.ao.quantization.fuser_method_mappings.fuse_conv_bn_relu",
    "signature": "(is_qat, conv, bn, relu)",
    "description": "Given the conv and bn modules, fuses them and returns the fused module"
  },
  "3193": {
    "name": "fuse_convtranspose_bn",
    "module": "torch.ao.quantization.fuser_method_mappings",
    "fullName": "torch.ao.quantization.fuser_method_mappings.fuse_convtranspose_bn",
    "signature": "(is_qat, convt, bn)",
    "description": "Given ConvTranspose and bn modules, fuses them and returns the fused module"
  },
  "3194": {
    "name": "fuse_linear_bn",
    "module": "torch.ao.quantization.fuser_method_mappings",
    "fullName": "torch.ao.quantization.fuser_method_mappings.fuse_linear_bn",
    "signature": "(is_qat, linear, bn)",
    "description": "Given the linear and bn modules, fuses them and returns the fused module"
  },
  "3195": {
    "name": "get_combined_dict",
    "module": "torch.ao.quantization.fuser_method_mappings",
    "fullName": "torch.ao.quantization.fuser_method_mappings.get_combined_dict",
    "signature": "(default_dict, additional_dict)",
    "description": "No description available."
  },
  "3196": {
    "name": "get_fuser_method",
    "module": "torch.ao.quantization.fuser_method_mappings",
    "fullName": "torch.ao.quantization.fuser_method_mappings.get_fuser_method",
    "signature": "(op_list, additional_fuser_method_mapping=None)",
    "description": "Get fuser method for the given list of module types,"
  },
  "3197": {
    "name": "get_fuser_method_new",
    "module": "torch.ao.quantization.fuser_method_mappings",
    "fullName": "torch.ao.quantization.fuser_method_mappings.get_fuser_method_new",
    "signature": "(op_pattern: Union[Callable, Tuple[Callable, Callable], Tuple[Callable, Tuple[Callable, Callable]], Any], fuser_method_mapping: Optional[Dict[Union[Callable, Tuple[Callable, Callable], Tuple[Callable, Tuple[Callable, Callable]], Any], Union[torch.nn.modules.container.Sequential, Callable]]] = None)",
    "description": "This will be made defult after we deparate the get_fuser_method"
  },
  "3198": {
    "name": "get_valid_patterns",
    "module": "torch.ao.quantization.fuser_method_mappings",
    "fullName": "torch.ao.quantization.fuser_method_mappings.get_valid_patterns",
    "signature": "(op_pattern)",
    "description": "Returns a list of valid patterns generated from the op_pattern,"
  },
  "3199": {
    "name": "reverse2",
    "module": "torch.ao.quantization.fuser_method_mappings",
    "fullName": "torch.ao.quantization.fuser_method_mappings.reverse2",
    "signature": "(f)",
    "description": "No description available."
  },
  "3200": {
    "name": "reverse3",
    "module": "torch.ao.quantization.fuser_method_mappings",
    "fullName": "torch.ao.quantization.fuser_method_mappings.reverse3",
    "signature": "(f)",
    "description": "No description available."
  },
  "3201": {
    "name": "reverse_sequential_wrapper2",
    "module": "torch.ao.quantization.fuser_method_mappings",
    "fullName": "torch.ao.quantization.fuser_method_mappings.reverse_sequential_wrapper2",
    "signature": "(sequential)",
    "description": "Given a sequential class for two modules, return a function that takes"
  },
  "3202": {
    "name": "sequential_wrapper2",
    "module": "torch.ao.quantization.fuser_method_mappings",
    "fullName": "torch.ao.quantization.fuser_method_mappings.sequential_wrapper2",
    "signature": "(sequential)",
    "description": "Given a sequential class for two modules, return a function that takes"
  },
  "3203": {
    "name": "abstractmethod",
    "module": "torch.ao.quantization.fake_quantize",
    "fullName": "torch.ao.quantization.fake_quantize.abstractmethod",
    "signature": "(funcobj)",
    "description": "A decorator indicating abstract methods."
  },
  "3204": {
    "name": "disable_fake_quant",
    "module": "torch.ao.quantization.fake_quantize",
    "fullName": "torch.ao.quantization.fake_quantize.disable_fake_quant",
    "signature": "(mod)",
    "description": "Disable fake quantization for this module, if applicable. Example usage::"
  },
  "3205": {
    "name": "disable_observer",
    "module": "torch.ao.quantization.fake_quantize",
    "fullName": "torch.ao.quantization.fake_quantize.disable_observer",
    "signature": "(mod)",
    "description": "Disable observation for this module, if applicable. Example usage::"
  },
  "3206": {
    "name": "enable_fake_quant",
    "module": "torch.ao.quantization.fake_quantize",
    "fullName": "torch.ao.quantization.fake_quantize.enable_fake_quant",
    "signature": "(mod)",
    "description": "Enable fake quantization for this module, if applicable. Example usage::"
  },
  "3207": {
    "name": "enable_observer",
    "module": "torch.ao.quantization.fake_quantize",
    "fullName": "torch.ao.quantization.fake_quantize.enable_observer",
    "signature": "(mod)",
    "description": "Enable observation for this module, if applicable. Example usage::"
  },
  "3208": {
    "name": "autocast_decorator",
    "module": "torch.amp.autocast_mode",
    "fullName": "torch.amp.autocast_mode.autocast_decorator",
    "signature": "(autocast_instance, func)",
    "description": "No description available."
  }
}