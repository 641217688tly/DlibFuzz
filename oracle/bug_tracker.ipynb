{
 "cells": [
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "1. fuzzer/seeds/test_seeds/zero-shot/221/264_263_243/seed_4.py\n",
    "\n",
    "Executing seed file: /mnt/e/SoftwareCourses/DeepLearning/DlibFuzz/fuzzer/seeds/test_seeds/zero-shot/221/264_263_243/seed_4.py\n",
    "PyTorch code failed: Calculated padded input size per channel: (1 x 3). Kernel size: (2 x 2). Kernel size can't be greater than actual input size\n",
    "2024-09-08 18:21:31.212703: F ./tensorflow/core/util/tensor_format.h:427] Check failed: index >= 0 && index < num_total_dims Invalid index from the dimension: 3, 0, C\n",
    "/tmp/tmpm_hzyz4w: line 3: 56558 Aborted                 python /mnt/e/SoftwareCourses/DeepLearning/DlibFuzz/oracle/driver.py\n",
    "ERROR conda.cli.main_run:execute(125): `conda run python /mnt/e/SoftwareCourses/DeepLearning/DlibFuzz/oracle/driver.py` failed. (See above for error)\n",
    "\n",
    "问题仍然出现的原因是由于在 TensorFlow 代码中引发了一个底层的崩溃错误，而这个错误并没有通过 Python 的异常捕获机制被处理。TensorFlow 有时在底层（C++部分）遇到问题时，会导致整个进程直接崩溃（Aborted），而不是抛出 Python 异常。"
   ],
   "id": "6cffb9b83da87fe6"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "2. 种子问题：\n",
    "PyTorch: 不要在使用 torch.nn.CrossEntropyLoss() 时手动计算 softmax，它会自动处理。你可以直接传入 logits，而不是 output_pt\n",
    "\n",
    "fuzzer/seeds/test_seeds/zero-shot/1001/1212_1253_1081/seed_2.py\n",
    "\n",
    "fuzzer/seeds/test_seeds/zero-shot/119/140_136_126/seed_3.py\n",
    "\n",
    "剩下的不计了，都是这个问题\n"
   ],
   "id": "6b29c9a90d6df7de"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "3. fuzzer/seeds/test_seeds/zero-shot/111/132_128_118/seed_2.py\n",
    "\n",
    "种子问题：\n",
    "为了保证三个库之间的代码功能一致，你应该在 JAX 中也进行平方运算，而不是 ReLU。将 JAX 的代码修改为与 PyTorch 和 TensorFlow 相同的操作，即平方运算。"
   ],
   "id": "df37fec290c90bdd"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "4. fuzzer/seeds/test_seeds/zero-shot/115/136_132_122/seed_0.py\n",
    "\n",
    "种子问题：\n",
    "PyTorch、TensorFlow 和 JAX 之间的损失函数计算有所不同，导致了输出结果的差异。这可能是因为每个框架中损失函数和计算方式不同所导致的\n",
    "- PyTorch\n",
    "PyTorch 中你使用了 torch.nn.functional.softmax 来对 x_pt 进行 softmax 操作，并使用了 torch.nn.functional.binary_cross_entropy 作为损失函数。\n",
    "binary_cross_entropy 期望的输出范围在 [0, 1]，通常与 sigmoid 输出匹配，但这里你使用了 softmax，这可能导致不匹配。此外，PyTorch 的 backward 操作是基于计算图的自动微分，因此损失函数计算之后，你使用了 .backward() 来执行梯度计算。\n",
    "- TensorFlow\n",
    "在 TensorFlow 中，使用了 tf.nn.softmax 来对 x_tf 进行 softmax 操作，但损失函数用的是 sigmoid_cross_entropy_with_logits。这个函数通常用于输出层未经过 sigmoid 处理的 logits，而在你的代码中你先进行了 softmax 处理。这可能是导致结果不同的原因之一。\n",
    "- JAX\n",
    "JAX 中的实现直接通过 jax.nn.softmax 对 x_jax 进行 softmax 操作，并使用手动实现的交叉熵损失函数 -jnp.mean(y_jax * jnp.log(output_jax))。这种损失函数的实现与 TensorFlow 的方式有所不同，可能导致结果的差异。"
   ],
   "id": "cc8d776132c70bca"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "5. fuzzer/seeds/test_seeds/zero-shot/13/16_15_15/seed_0.py\n",
    "\n",
    "种子问题：在 JAX 中，你使用了 jnp.max()，并手动对输入进行了 reshape，但是你的 max 操作仅指定了 (2, 3) 轴，导致它没有进行与 PyTorch 和 TensorFlow 相同的池化操作。你需要使用 JAX 提供的 jax.lax.reduce_window() 函数来正确实现 max_pool 操作。\n",
    "\n"
   ],
   "id": "430265860f457bed"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "6. 种子问题：在 JAX 中，损失函数是手动实现的交叉熵损失（-y_jax * log(output_jax)）。在这种实现中，可能存在数值精度问题，尤其在使用 log 函数时，值可能接近于0，导致不稳定性。为了确保三者之间的一致性，建议在 JAX 中使用等效的 binary_cross_entropy 实现，而不是手动计算交叉熵。\n",
    "\n",
    "fuzzer/seeds/test_seeds/zero-shot/135/161_154_145/seed_0.py\n",
    "\n",
    "fuzzer/seeds/test_seeds/zero-shot/140/167_160_152/seed_0.py\n",
    "\n",
    "fuzzer/seeds/test_seeds/zero-shot/167/200_199_184/seed_4.py\n",
    "\n",
    "剩下的不计了，都是这个问题\n",
    "\n"
   ],
   "id": "9701fa1aa893218e"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "7. fuzzer/seeds/test_seeds/zero-shot/142/170_163_154/seed_4.py\n",
    "\n",
    "种子问题：在计算二元交叉熵损失（Binary Cross Entropy Loss, BCE Loss）时，PyTorch 和 TensorFlow 提供了专用的损失函数，分别为 BCEWithLogitsLoss 和 BinaryCrossentropy。这两个函数在计算时会先对输入的 logits 进行 Sigmoid 激活，然后再计算交叉熵损失。因此，输入给这些函数的应当是模型的原始输出（logits），而无需提前进行 softmax 激活。\n",
    "\n",
    "在 JAX 中，虽然没有直接对应的 BCEWithLogitsLoss 函数，但我们可以通过组合使用 jax.nn.sigmoid 和 jax.numpy 中的交叉熵损失函数来实现类似的功能。需要注意的是，JAX 中的交叉熵损失函数通常期望输入为概率值（即经过 softmax 激活后的值）。因此，为了与 PyTorch 和 TensorFlow 的计算方式保持一致，我们需要对 JAX 的计算方法进行调整。"
   ],
   "id": "f9794f9eafecbc6b"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "",
   "id": "c08148b26dadcebd"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "8. fuzzer/seeds/test_seeds/zero-shot/150/181_175_164/seed_1.py\n",
    "\n",
    "种子问题：PyTorch 中使用的 torch.std 默认计算 无偏 标准差，也就是通过 Bessel 校正 进行调整，分母是 n-1，其中 n 是样本数量。这样做的目的是使结果成为无偏估计。为了使三者的结果一致，你可以指定 PyTorch 计算标准差时不使用 Bessel 校正，也就是让它计算有偏标准差（即分母为 n 而不是 n-1）。"
   ],
   "id": "3cc0f052603f421f"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "9. fuzzer/seeds/test_seeds/zero-shot/161/193_189_176/seed_1.py\n",
    "\n",
    "种子问题：TensorFlow 的结果与 PyTorch 不一致，可能是由于计算交叉熵损失时，TensorFlow 没有平均化损失。需要显式调用 reduce_mean 来取平均。JAX手动使用了 jax.nn.log_softmax() 来计算 softmax 的对数，然后通过手动计算交叉熵损失。问题在于你直接计算了交叉熵而没有取均值（通常交叉熵损失是平均值），导致了损失值过高。"
   ],
   "id": "7506cf857cb2cb4d"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "10. fuzzer/seeds/test_seeds/zero-shot/183/216_217_200/seed_4.py\n",
    "\n",
    "种子问题：PyTorch 中使用的 F.linear 函数是一个用于计算线性变换的函数，它的输入是 input 和 weight，并将 bias 加到结果中。然而，PyTorch 的 F.linear 函数计算时假设 weight 是已经转置的（shape 是 [out_features, in_features]）。\n",
    "因此，直接传入 w_pt 会导致与 TensorFlow 和 JAX 计算方式不一致。"
   ],
   "id": "ae2bf1a9d5cc5bed"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "11. fuzzer/seeds/test_seeds/zero-shot/19/22_22_21/seed_4.py\n",
    "\n",
    "种子问题：在 PyTorch 中，你使用了 torch.nn.functional.softmax 对 x_pt 进行 softmax 操作，并使用了 torch.nn.functional.cross_entropy 作为损失函数。cross_entropy 期望的输入是 logits，而不是 softmax 后的概率值。因此，你应该直接传入 x_pt，而不是 output_pt。"
   ],
   "id": "eca18e2da0f0f705"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "12. fuzzer/seeds/test_seeds/zero-shot/100/119_115_106/seed_1.py\n",
    "\n",
    "种子问题：出现多个种子集中在同一文件里"
   ],
   "id": "910dec81536cc316"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "\n",
    "\n",
    "13. fuzzer/seeds/test_seeds/zero-shot/103/123_119_110/seed_4.py\n",
    "\n",
    "种子问题：在 PyTorch 部分将 weight_pt 张量的形状设置为 (1, 3)，而你期望它进行的是线性变换，因此需要将 weight_pt 的形状转换为 (3, 1)，与输入 input_pt 的形状 (1, 3) 兼容"
   ],
   "id": "70d20905c248a13a"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "14. fuzzer/seeds/test_seeds/test/1/1_1_1/seed_3.py\n",
    "\n",
    "PyTorch、TensorFlow 和 JAX 在这个特定情况下的错误是一致的。所有三个框架都报错，原因是它们在执行矩阵乘法时遇到了 不兼容的形状。\n",
    "问题：他们三个库之间的直接错误类型都不一致，无法直接通过报错来判断是否一致。\n",
    "\n"
   ],
   "id": "eaaf817aba221985"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "15. fuzzer/seeds/test_seeds/zero-shot/499/604_613_536/seed_2.py\n",
    "\n",
    "种子问题：Jax code execution failed: AttributeError: module 'jax.nn' has no attribute 'max_pool'"
   ],
   "id": "1281d3d07baf0a18"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
